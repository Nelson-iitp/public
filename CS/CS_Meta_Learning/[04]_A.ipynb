{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa3dbac-4b37-43bc-a61f-dc622d7d9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nets import Qnetn, MLP_NN\n",
    "from taskmeta import SINTASK, SINTASK2, SINTASK3, SIN_xlow, SIN_xhigh\n",
    "\n",
    "\n",
    "device, dtype = 'cpu', torch.float32\n",
    "def tensor(data, rgrad=False):\n",
    "    return torch.tensor(data, device=device, dtype=dtype, requires_grad=rgrad)\n",
    "\n",
    "GRS = 12\n",
    "RNG = np.random.default_rng(GRS)\n",
    "randseed = lambda : RNG.integers(1, 10_000)\n",
    "xl, xh = SIN_xlow, SIN_xhigh\n",
    "\n",
    "HH = [1, 25, 25, 25, 1]\n",
    "lossF = lambda pp, yy: 0.5 * (pp-yy)**2\n",
    "lossM = lambda pp, yy: torch.sum(0.5 * (pp - yy) ** 2) \n",
    "\n",
    "NN = lambda : MLP_NN(HH, device, dtype, actF=nn.ReLU, seed=randseed())\n",
    "NP = lambda ext_params: MLP_NN(ext_params, device, dtype, actF=nn.ReLU, seed=randseed(), from_param=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b52a3",
   "metadata": {},
   "source": [
    "# Meta Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21409cf2-9479-421c-a117-ba3a8be76529",
   "metadata": {},
   "outputs": [],
   "source": [
    "taskerL = [\n",
    "    SINTASK2((7, 3), seed=randseed()),\n",
    "    SINTASK2((6, 2), seed=randseed()),\n",
    "    SINTASK2((5, 1), seed=randseed()),\n",
    "    SINTASK2((4, 0), seed=randseed()),\n",
    "    SINTASK2((3, -1), seed=randseed()),\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813474cd",
   "metadata": {},
   "source": [
    "# algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46091481",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_epochs =  1000\n",
    "outer_lr =      0.001\n",
    "inner_lr =      0.001\n",
    "task_batch_size =    5\n",
    "train_K =       8\n",
    "test_K =        8\n",
    "P = print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d4a61",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca371467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1: randomly initialize theta \n",
      "--------------------------\n",
      "~ N_LAYERS:[4]\n",
      "~ D_TYPE:[torch.float32]\n",
      "~ DEV:[cpu]\n",
      "--------------------------\n",
      "--> Weights[0]:: Params[25] of Shape[torch.Size([25, 1])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[ 4.0752e-02],\n",
      "        [-8.0139e-02],\n",
      "        [-2.0934e-03],\n",
      "        [ 9.1835e-02],\n",
      "        [-6.5284e-05],\n",
      "        [ 5.8189e-02],\n",
      "        [-8.6809e-02],\n",
      "        [ 2.3397e-02],\n",
      "        [ 6.8264e-02],\n",
      "        [-1.3586e-02],\n",
      "        [ 2.6145e-02],\n",
      "        [-1.7595e-02],\n",
      "        [ 3.8888e-02],\n",
      "        [-8.4653e-02],\n",
      "        [ 2.8353e-02],\n",
      "        [-5.0440e-02],\n",
      "        [-5.0463e-02],\n",
      "        [-2.3697e-02],\n",
      "        [-6.8030e-02],\n",
      "        [-3.7875e-02],\n",
      "        [-2.6412e-02],\n",
      "        [ 5.6413e-04],\n",
      "        [-8.0990e-02],\n",
      "        [-4.2174e-03],\n",
      "        [-5.4436e-02]], requires_grad=True)\n",
      "--> Bias[0]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([-0.0140, -0.0385,  0.0818, -0.0299, -0.0257,  0.0169, -0.0935,  0.0139,\n",
      "         0.0154,  0.0660, -0.0231, -0.0817,  0.0506,  0.0211,  0.0320, -0.0998,\n",
      "         0.0019, -0.0629, -0.0903,  0.0614, -0.0029, -0.0160,  0.0457, -0.0232,\n",
      "         0.0527], requires_grad=True)\n",
      "--> Weights[1]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[-0.0123, -0.0274,  0.0475,  0.0957,  0.0381,  0.0467, -0.0976,  0.0199,\n",
      "          0.0776, -0.0414,  0.0917,  0.0442,  0.0178, -0.0975, -0.0694,  0.0507,\n",
      "         -0.0838, -0.0614,  0.0403, -0.0060, -0.0326, -0.0495,  0.0380,  0.0398,\n",
      "          0.0153],\n",
      "        [ 0.0231,  0.0929,  0.0626,  0.0120, -0.0016, -0.0588,  0.0764,  0.0895,\n",
      "          0.0234, -0.0981, -0.0367,  0.0609, -0.0022, -0.0471,  0.0696,  0.0652,\n",
      "          0.0535, -0.0012, -0.0951,  0.0915, -0.0910,  0.0112, -0.0657, -0.0504,\n",
      "         -0.0894],\n",
      "        [-0.0499, -0.0772, -0.0278, -0.0131,  0.0578,  0.0207, -0.0243, -0.0200,\n",
      "         -0.0603, -0.0996, -0.0239, -0.0757,  0.0944,  0.0189,  0.0864, -0.0176,\n",
      "         -0.0106, -0.0970, -0.0638, -0.0252, -0.0084,  0.0558,  0.0365,  0.0023,\n",
      "          0.0094],\n",
      "        [-0.0785, -0.0724, -0.0760,  0.0248, -0.0270,  0.0467, -0.0059, -0.0191,\n",
      "         -0.0088, -0.0154, -0.0093, -0.0381, -0.0086, -0.0308, -0.0725, -0.0861,\n",
      "         -0.0347, -0.0528, -0.0746, -0.0393, -0.0971,  0.0711,  0.0747,  0.0340,\n",
      "          0.0207],\n",
      "        [-0.0587,  0.0524,  0.0016, -0.0291, -0.0066,  0.0773, -0.0094,  0.0577,\n",
      "         -0.0085, -0.0422,  0.0303, -0.0802, -0.0115, -0.0119,  0.0693, -0.0008,\n",
      "          0.0824, -0.0421, -0.0452, -0.0474, -0.0353, -0.0329,  0.0369, -0.0530,\n",
      "         -0.0728],\n",
      "        [-0.0752,  0.0358, -0.0234,  0.0643,  0.0993, -0.0241,  0.0384, -0.0821,\n",
      "         -0.0028, -0.0086,  0.0561, -0.0270,  0.0398, -0.0576,  0.0428, -0.0494,\n",
      "         -0.0382,  0.0373, -0.0579, -0.0621,  0.0656,  0.0312, -0.0863, -0.0960,\n",
      "          0.0126],\n",
      "        [-0.0416, -0.0473,  0.0485, -0.0920, -0.0188, -0.0218, -0.0863, -0.0929,\n",
      "          0.0162, -0.0493,  0.0482,  0.0241,  0.0122,  0.0015,  0.0955,  0.0197,\n",
      "         -0.0852, -0.0338, -0.0175,  0.0924,  0.0061,  0.0681, -0.0786, -0.0602,\n",
      "          0.0258],\n",
      "        [-0.0928, -0.0027,  0.0466,  0.0368, -0.0318, -0.0812, -0.0249, -0.0828,\n",
      "         -0.0079, -0.0750, -0.0260,  0.0919, -0.0119,  0.0387,  0.0984, -0.0228,\n",
      "          0.0337,  0.0093,  0.0644, -0.0203,  0.0018, -0.0353,  0.0561,  0.0517,\n",
      "         -0.0442],\n",
      "        [-0.0186, -0.0558, -0.0576, -0.0332,  0.0251, -0.0045, -0.0059, -0.0796,\n",
      "         -0.0802,  0.0132, -0.0885, -0.0577,  0.0832, -0.0870, -0.0699,  0.0746,\n",
      "          0.0060, -0.0093,  0.0286, -0.0527, -0.0395,  0.0206,  0.0990, -0.0401,\n",
      "         -0.0448],\n",
      "        [-0.0965, -0.0192,  0.0314,  0.0820,  0.0847,  0.0354,  0.0416, -0.0106,\n",
      "         -0.0709,  0.0508,  0.0293, -0.0851,  0.0276, -0.0469, -0.0789,  0.0879,\n",
      "         -0.0271, -0.0431,  0.0102, -0.0913, -0.0154,  0.0092, -0.0035, -0.0895,\n",
      "          0.0736],\n",
      "        [ 0.0761,  0.0482, -0.0687, -0.0107,  0.0969, -0.0459,  0.0694, -0.0284,\n",
      "          0.0551, -0.0532, -0.0546, -0.0980, -0.0865, -0.0464,  0.0297,  0.0362,\n",
      "          0.0720, -0.0126,  0.0666, -0.0365,  0.0043, -0.0058,  0.0893, -0.0483,\n",
      "          0.0309],\n",
      "        [ 0.0855,  0.0253,  0.0946,  0.0368,  0.0761, -0.0280, -0.0115, -0.0067,\n",
      "         -0.0811, -0.0942,  0.0608, -0.0936,  0.0677,  0.0790, -0.0685,  0.0945,\n",
      "         -0.0903,  0.0978,  0.0823,  0.0240,  0.0253, -0.0029,  0.0245,  0.0630,\n",
      "         -0.0213],\n",
      "        [ 0.0517, -0.0301, -0.0545, -0.0312, -0.0653,  0.0319, -0.0388,  0.0928,\n",
      "          0.0065,  0.0649, -0.0587,  0.0138, -0.0076, -0.0507, -0.0875, -0.0067,\n",
      "         -0.0282, -0.0879,  0.0777,  0.0763,  0.0073, -0.0729,  0.0358, -0.0672,\n",
      "          0.0023],\n",
      "        [-0.0955, -0.0104,  0.0610,  0.0767, -0.0443, -0.0977,  0.0534, -0.0266,\n",
      "         -0.0864,  0.0385,  0.0489,  0.0081,  0.0479, -0.0270, -0.0158,  0.0765,\n",
      "         -0.0439, -0.0302,  0.0920,  0.0226,  0.0301, -0.0970, -0.0116,  0.0841,\n",
      "          0.0413],\n",
      "        [ 0.0125,  0.0842, -0.0136,  0.0802, -0.0605, -0.0674, -0.0687, -0.0816,\n",
      "         -0.0224,  0.0978, -0.0788,  0.0884, -0.0746,  0.0248,  0.0079,  0.0217,\n",
      "         -0.0888,  0.0244,  0.0045,  0.0213, -0.0864, -0.0910,  0.0019, -0.0322,\n",
      "         -0.0673],\n",
      "        [-0.0404,  0.0825,  0.0130, -0.0727,  0.0119,  0.0953,  0.0790, -0.0834,\n",
      "         -0.0405,  0.0897,  0.0159, -0.0841,  0.0675, -0.0602, -0.0205, -0.0922,\n",
      "          0.0905, -0.0563,  0.0228,  0.0538, -0.0027, -0.0904, -0.0981,  0.0152,\n",
      "         -0.0857],\n",
      "        [-0.0006,  0.0221, -0.0298,  0.0158, -0.0704,  0.0125,  0.0325, -0.0189,\n",
      "         -0.0109, -0.0525, -0.0480,  0.0012, -0.0205, -0.0562,  0.0749, -0.0397,\n",
      "          0.0888,  0.0415, -0.0021, -0.0996,  0.0257, -0.0713, -0.0266, -0.0618,\n",
      "         -0.0789],\n",
      "        [ 0.0856,  0.0144,  0.0072,  0.0233, -0.0077, -0.0746,  0.0910,  0.0735,\n",
      "         -0.0572, -0.0267,  0.0173, -0.0090,  0.0763,  0.0695,  0.0796,  0.0298,\n",
      "         -0.0049,  0.0733,  0.0661,  0.0007,  0.0225,  0.0860,  0.0337,  0.0536,\n",
      "         -0.0840],\n",
      "        [ 0.0751, -0.0787, -0.0641, -0.0318,  0.0824, -0.0239, -0.0779,  0.0372,\n",
      "         -0.0591,  0.0942, -0.0320,  0.0691, -0.0714, -0.0347, -0.0029, -0.0579,\n",
      "         -0.0982,  0.0567, -0.0774,  0.0399,  0.0320, -0.0595, -0.0104, -0.0905,\n",
      "         -0.0960],\n",
      "        [-0.0637,  0.0545,  0.0685, -0.0475, -0.0845,  0.0458,  0.0219, -0.0743,\n",
      "         -0.0552, -0.0717,  0.0014, -0.0851,  0.0920,  0.0137,  0.0281, -0.0476,\n",
      "          0.0982,  0.0219,  0.0426, -0.0117, -0.0418,  0.0348, -0.0458, -0.0626,\n",
      "         -0.0879],\n",
      "        [ 0.0584,  0.0674,  0.0896,  0.0306, -0.0503, -0.0043,  0.0873, -0.0209,\n",
      "          0.0626, -0.0471,  0.0804,  0.0096, -0.0232, -0.0077,  0.0582, -0.0434,\n",
      "          0.0428, -0.0923, -0.0425,  0.0090, -0.0735, -0.0822,  0.0192,  0.0063,\n",
      "          0.0979],\n",
      "        [-0.0607, -0.0901, -0.0250,  0.0964,  0.0320, -0.0440, -0.0271,  0.0893,\n",
      "          0.0332, -0.0544, -0.0242,  0.0918, -0.0303,  0.0546, -0.0044, -0.0411,\n",
      "         -0.0942,  0.0005, -0.0377, -0.0455, -0.0509,  0.0596, -0.0245, -0.0095,\n",
      "          0.0779],\n",
      "        [ 0.0268,  0.0575, -0.0801, -0.0581, -0.0138, -0.0790, -0.0025, -0.0307,\n",
      "          0.0316, -0.0734, -0.0147, -0.0462,  0.0283,  0.0414,  0.0040, -0.0522,\n",
      "          0.0620, -0.0325,  0.0271,  0.0223,  0.0034, -0.0107,  0.0968,  0.0352,\n",
      "         -0.0635],\n",
      "        [-0.0293, -0.0638,  0.0712, -0.0385, -0.0818,  0.0429,  0.0140, -0.0987,\n",
      "         -0.0353,  0.0447,  0.0294,  0.0932,  0.0869, -0.0112,  0.0054, -0.0610,\n",
      "          0.0636, -0.0306, -0.0568, -0.0762, -0.0112, -0.0654,  0.0655, -0.0837,\n",
      "         -0.0319],\n",
      "        [ 0.0693,  0.0907,  0.0694,  0.0165,  0.0969,  0.0580,  0.0631,  0.0328,\n",
      "          0.0348,  0.0239, -0.0959,  0.0969,  0.0149,  0.0200, -0.0490,  0.0040,\n",
      "         -0.0182, -0.0776,  0.0491,  0.0653,  0.0366,  0.0369,  0.0166, -0.0226,\n",
      "          0.0303]], requires_grad=True)\n",
      "--> Bias[1]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([ 0.0111,  0.0632,  0.0549, -0.0222,  0.0431,  0.0459, -0.0642,  0.0135,\n",
      "         0.0277, -0.0242,  0.0372, -0.0035,  0.0296, -0.0718, -0.0024, -0.0611,\n",
      "        -0.0775, -0.0568, -0.0751, -0.0140,  0.0996,  0.0182, -0.0334, -0.0367,\n",
      "         0.0218], requires_grad=True)\n",
      "--> Weights[2]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[-7.9172e-02,  8.7749e-02,  9.0667e-03, -6.4810e-02, -4.4727e-02,\n",
      "          4.6463e-02, -6.0198e-02, -5.4794e-02, -3.2060e-02,  8.0462e-03,\n",
      "          5.1433e-02,  4.5928e-02, -4.2349e-02,  7.2138e-02,  6.0003e-02,\n",
      "         -4.5602e-02,  5.6945e-02,  3.9833e-02, -6.1837e-02,  7.1879e-02,\n",
      "          9.5367e-02, -3.7005e-02, -6.6941e-02,  8.4476e-02,  9.1080e-02],\n",
      "        [ 6.0103e-02, -9.7601e-02, -7.5997e-02,  8.5033e-02,  4.5226e-02,\n",
      "         -3.2453e-02,  9.8743e-02,  4.9070e-02, -6.5963e-02, -4.5399e-02,\n",
      "          6.4180e-02, -5.5633e-02, -9.7472e-02, -4.2940e-03,  1.7629e-02,\n",
      "         -4.1968e-02,  3.4459e-02,  6.4871e-02, -6.5458e-02,  1.9959e-02,\n",
      "          2.6821e-02,  9.5116e-02, -3.7854e-02, -6.5891e-02,  5.3360e-02],\n",
      "        [-7.4726e-02, -7.3484e-02, -7.2044e-02, -7.1722e-02,  7.0408e-02,\n",
      "         -3.2759e-02,  4.5070e-02, -9.9326e-02, -1.5752e-02, -5.7176e-02,\n",
      "          9.8639e-02, -2.3909e-02, -2.8338e-02, -9.8931e-02, -4.4261e-02,\n",
      "         -1.4025e-02,  7.4345e-02, -2.9533e-02, -8.5300e-02,  1.9670e-02,\n",
      "         -2.8276e-02,  7.5684e-02,  3.5287e-02, -2.5301e-02,  5.4369e-02],\n",
      "        [-7.7096e-02,  8.8804e-02, -4.2495e-02, -2.6019e-02,  6.6222e-02,\n",
      "          1.6411e-02, -6.6194e-02,  6.6191e-04,  9.3643e-02, -4.9710e-02,\n",
      "         -5.1958e-02, -8.4390e-02,  8.2695e-02,  1.5619e-02, -2.8781e-02,\n",
      "         -2.7515e-02, -2.3940e-02,  2.7778e-02,  4.3062e-02,  3.6314e-02,\n",
      "          9.2494e-02,  6.5525e-02, -9.0430e-02,  5.2199e-02, -1.6778e-02],\n",
      "        [-6.8242e-02, -6.5754e-02,  8.0772e-02, -3.7393e-02,  6.2527e-02,\n",
      "          5.5018e-02,  7.8856e-02,  5.8267e-02, -6.8497e-02, -7.0781e-02,\n",
      "         -7.0417e-02, -7.8330e-02,  9.5694e-02, -6.8368e-02, -6.7409e-02,\n",
      "          7.4455e-02,  8.6789e-02,  2.9291e-02, -3.7516e-02, -3.1690e-02,\n",
      "         -7.7685e-02, -9.3918e-02, -6.4807e-02,  9.4706e-02, -7.9461e-03],\n",
      "        [-1.4323e-02, -4.0668e-02,  8.1656e-02, -6.9650e-02, -7.8484e-02,\n",
      "          3.0058e-02, -6.4106e-02, -8.9816e-02, -8.6586e-02,  4.5957e-02,\n",
      "          3.7535e-02,  1.7940e-02,  3.0453e-02, -1.7215e-02,  5.6329e-02,\n",
      "         -2.7745e-02, -9.8958e-02,  1.1096e-02, -9.8328e-02, -4.3539e-02,\n",
      "         -2.1317e-02, -2.2515e-03,  8.0601e-02,  5.2731e-02, -6.4546e-02],\n",
      "        [ 5.0391e-02,  6.4638e-02,  4.6301e-02, -2.1344e-02,  8.8304e-02,\n",
      "         -4.3354e-02, -5.3089e-02,  9.5958e-02,  2.2395e-02, -3.6873e-02,\n",
      "          5.1058e-02,  4.1734e-02,  4.0237e-02, -7.8331e-02,  8.0558e-02,\n",
      "         -3.9619e-02, -7.0831e-02,  3.3533e-03,  5.2318e-02,  7.9742e-03,\n",
      "         -6.5552e-02,  9.3048e-02,  2.6302e-02,  8.2166e-02, -3.6143e-02],\n",
      "        [-7.6958e-02,  3.8605e-02, -9.0593e-02, -9.3693e-02,  2.5614e-02,\n",
      "          1.1963e-02,  5.9821e-02, -5.7479e-02,  9.9673e-02,  1.2197e-02,\n",
      "         -6.4529e-03, -2.6528e-02, -5.9029e-02, -7.4216e-02,  6.9367e-02,\n",
      "         -5.0097e-02, -6.7504e-02,  1.4231e-02,  6.0504e-02,  5.0405e-02,\n",
      "          6.4331e-03,  3.2962e-02,  2.2410e-02, -7.1368e-02,  5.2193e-02],\n",
      "        [ 3.6138e-02,  4.9633e-02,  6.9147e-02, -4.3630e-02, -6.4744e-02,\n",
      "         -6.1725e-02,  4.5644e-02,  6.6289e-02,  9.9620e-02, -3.8704e-02,\n",
      "          8.0587e-02,  2.4565e-02,  1.3379e-02, -5.3424e-02,  8.6179e-02,\n",
      "          3.0671e-02, -9.9263e-02,  4.3128e-02,  1.3109e-02,  9.8784e-02,\n",
      "         -9.4025e-02, -3.1013e-02,  4.0334e-03, -1.1025e-02, -8.3821e-02],\n",
      "        [-5.7148e-02,  2.2877e-03,  7.3879e-02, -5.3651e-02,  3.8579e-03,\n",
      "          9.1965e-02,  7.1286e-02, -1.2319e-02, -9.7754e-02,  9.7139e-02,\n",
      "         -2.3939e-02, -2.2865e-02,  8.2991e-02,  1.7684e-02,  1.3923e-02,\n",
      "         -1.5160e-02,  1.6471e-02,  2.7929e-02,  4.4688e-02,  7.9370e-02,\n",
      "         -4.2275e-02,  8.9432e-03, -4.6016e-02,  4.4590e-02,  9.7080e-02],\n",
      "        [-8.6128e-02, -7.9485e-03,  5.6818e-02, -9.8405e-02,  5.8554e-02,\n",
      "         -5.6708e-02,  7.2041e-02,  7.9303e-02, -3.9590e-02,  9.7104e-02,\n",
      "         -4.1731e-02, -3.2189e-03,  1.3536e-02,  9.2793e-02,  3.9544e-02,\n",
      "         -9.8775e-02, -3.1000e-02, -5.0249e-02,  6.6974e-02,  1.2025e-02,\n",
      "         -3.5789e-02,  8.1474e-02, -7.0036e-02, -6.2586e-02, -2.3334e-02],\n",
      "        [-7.6238e-02, -7.5717e-02, -7.5179e-03,  9.8299e-02,  7.4386e-02,\n",
      "         -4.2711e-02,  4.7045e-02, -9.2262e-02, -1.6142e-02, -4.1139e-02,\n",
      "          1.7397e-02,  5.6110e-02,  3.3996e-02, -7.1122e-02,  8.9194e-02,\n",
      "         -4.8352e-03, -8.8916e-02, -2.7892e-02, -2.2227e-02, -9.0219e-02,\n",
      "         -7.0836e-02,  1.0709e-02,  1.3661e-02,  9.4297e-02,  2.6642e-02],\n",
      "        [-5.9014e-02, -7.5498e-02,  9.2721e-02,  9.2681e-02,  4.4434e-02,\n",
      "          5.0540e-04, -7.3464e-03,  2.1271e-02,  1.9748e-02,  9.8224e-02,\n",
      "         -7.8529e-02, -2.0077e-02, -1.3997e-02, -2.2338e-02,  8.0343e-02,\n",
      "          9.7849e-02, -3.7248e-02,  9.5686e-02,  1.1844e-02,  8.3270e-02,\n",
      "          5.0802e-02,  2.0686e-02,  1.7910e-02,  9.8282e-02,  5.5477e-02],\n",
      "        [-1.6753e-02, -8.5215e-02, -3.4303e-02,  5.9785e-02, -9.9288e-03,\n",
      "          8.0527e-02,  8.9872e-02, -9.9245e-02,  3.4679e-02,  1.6452e-02,\n",
      "         -2.8671e-02, -4.8747e-02,  8.7314e-02, -6.4422e-03,  9.6371e-02,\n",
      "         -8.0203e-02,  6.6978e-02, -9.4448e-02,  7.3856e-02, -7.5439e-02,\n",
      "         -4.7904e-02,  3.0706e-02,  8.5862e-02, -2.2353e-02, -1.2004e-02],\n",
      "        [-6.8482e-02, -4.5301e-02,  2.7558e-02, -3.9202e-02,  6.4171e-02,\n",
      "         -4.7520e-02, -8.2360e-02, -5.5919e-03,  1.8676e-02,  5.4259e-02,\n",
      "          9.6333e-02,  3.2363e-02,  1.0819e-02,  2.8999e-02,  9.3198e-02,\n",
      "         -3.1425e-02,  1.5465e-02,  9.2779e-02,  8.1858e-02, -4.5335e-02,\n",
      "         -8.6383e-02, -2.4954e-02,  8.6906e-02, -2.4840e-02, -6.7834e-02],\n",
      "        [-4.4500e-02,  2.8906e-02,  4.3108e-02, -9.6328e-02, -2.9922e-02,\n",
      "         -1.4185e-02,  2.8945e-02, -2.7085e-02,  9.5182e-02, -7.0803e-02,\n",
      "         -2.2913e-03, -3.6068e-02,  5.4670e-02,  1.1741e-02,  1.0845e-02,\n",
      "         -2.4324e-02, -3.5358e-02,  2.0073e-02,  2.1093e-03, -7.3530e-02,\n",
      "          6.8938e-02,  6.1012e-03, -3.2689e-03, -4.8533e-02, -5.5072e-02],\n",
      "        [-6.4878e-02,  5.9257e-03,  4.3907e-02, -6.7502e-03, -1.2343e-02,\n",
      "          5.2126e-02, -6.0060e-02,  5.9863e-02,  8.3986e-02, -7.5026e-02,\n",
      "          7.1105e-02,  8.1719e-02, -3.0238e-02, -7.0321e-02, -4.9150e-04,\n",
      "         -2.2673e-02, -8.4233e-02,  4.5640e-02,  4.5422e-02,  2.0106e-02,\n",
      "          2.8011e-02,  6.5828e-02,  6.3617e-03, -2.0583e-02,  3.7973e-02],\n",
      "        [-9.9421e-03,  5.9724e-02,  6.2446e-02,  2.8715e-03,  9.5814e-02,\n",
      "          9.1686e-02,  2.6845e-02, -5.9054e-02, -4.0250e-02, -2.1940e-02,\n",
      "         -2.2133e-02, -8.9482e-02, -6.9146e-02, -8.0801e-02, -4.6771e-02,\n",
      "         -9.3295e-02, -7.8400e-03,  4.4618e-05,  5.0432e-02,  7.5492e-02,\n",
      "          7.4885e-02,  1.8328e-02, -6.6055e-02,  3.6523e-02,  6.7848e-02],\n",
      "        [-7.2230e-02,  3.9185e-02, -5.6678e-02,  2.8256e-02, -7.3477e-02,\n",
      "         -9.3491e-02,  6.0357e-03, -7.8121e-02, -9.6374e-02,  5.4355e-02,\n",
      "          4.1801e-02, -5.4153e-02,  3.4998e-02, -7.3518e-02,  6.6875e-02,\n",
      "          3.8884e-02, -8.0056e-02, -6.3230e-02,  2.8875e-02,  9.4527e-02,\n",
      "          4.3511e-02, -5.4066e-02, -8.6415e-02, -9.3919e-02,  6.9295e-02],\n",
      "        [-4.9851e-02,  4.5282e-04, -6.3029e-02, -8.6966e-02,  7.4817e-02,\n",
      "          4.4455e-02,  9.9550e-02,  2.7335e-02,  3.2303e-02,  5.4283e-02,\n",
      "         -4.7407e-02,  3.9955e-02, -7.9927e-02, -5.7541e-02,  3.0504e-03,\n",
      "         -1.5411e-02,  7.4795e-02, -7.5065e-02,  4.4044e-02, -2.6505e-02,\n",
      "         -2.7130e-02, -7.0500e-02,  4.0650e-02,  3.5287e-02, -1.7736e-02],\n",
      "        [ 1.1748e-02,  7.2026e-02,  4.1183e-02,  8.5479e-02,  2.8064e-02,\n",
      "          9.0205e-02,  8.2236e-02,  3.6847e-02, -7.0930e-02, -2.1307e-02,\n",
      "         -3.7372e-02,  5.8305e-02, -8.5428e-02,  3.6808e-02,  4.0276e-02,\n",
      "         -6.9572e-02, -1.1313e-02,  8.1611e-02, -3.0025e-02, -7.9494e-02,\n",
      "          5.4711e-02, -1.4866e-02,  9.1410e-02,  9.4365e-02,  9.0002e-02],\n",
      "        [-4.1252e-02,  2.5765e-02, -7.8125e-02, -2.5532e-02, -6.7875e-02,\n",
      "          3.7509e-03, -8.6158e-02, -4.1003e-02,  7.1787e-02, -7.7817e-02,\n",
      "          1.0214e-02,  3.4763e-02, -4.1320e-02,  7.5485e-02, -5.8231e-02,\n",
      "         -9.2544e-02, -6.2946e-02,  4.7687e-02, -8.3815e-02,  2.8539e-02,\n",
      "          4.7149e-02,  3.6314e-02, -5.5286e-02, -9.3893e-02,  6.6175e-02],\n",
      "        [-6.3941e-02,  8.9660e-02, -1.7853e-02,  3.1441e-02,  5.5111e-02,\n",
      "          2.1542e-02, -3.3162e-02, -3.6676e-03,  8.1653e-02, -6.2133e-02,\n",
      "          3.2533e-02,  2.7914e-03, -8.4295e-02, -5.3086e-02, -5.0286e-02,\n",
      "          6.8468e-02, -3.5059e-03,  2.3294e-02, -9.7234e-02,  6.4298e-02,\n",
      "          9.7333e-02, -1.8761e-02,  2.1191e-02,  2.3274e-03,  6.9014e-02],\n",
      "        [-7.2950e-02,  1.3149e-02,  5.7416e-02, -3.1880e-03, -7.6926e-02,\n",
      "         -5.6713e-02, -6.4454e-02,  7.8573e-02, -8.4530e-02, -1.8895e-02,\n",
      "         -1.0064e-02,  9.8342e-02, -2.1965e-02,  3.5210e-02, -3.4309e-02,\n",
      "          2.6205e-02, -3.0095e-02,  7.6376e-02,  2.5883e-02,  7.7924e-02,\n",
      "          5.6788e-02,  4.9300e-02, -6.9081e-02,  1.4470e-02,  1.1058e-02],\n",
      "        [-4.0792e-03, -5.4019e-02,  9.2376e-02,  2.9688e-02, -7.5440e-02,\n",
      "          6.2900e-02,  9.5127e-02,  7.5433e-02,  5.8458e-02, -5.2849e-03,\n",
      "         -1.2438e-02,  2.8946e-02,  3.6772e-02,  7.5348e-02, -1.9422e-02,\n",
      "          4.3440e-02, -2.4700e-02,  3.9229e-02, -1.9314e-02, -9.7970e-02,\n",
      "         -1.2166e-02, -4.3912e-02,  7.9272e-03,  1.5720e-02,  9.9998e-02]],\n",
      "       requires_grad=True)\n",
      "--> Bias[2]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([ 0.0238, -0.0477, -0.0579, -0.0425,  0.0478,  0.0099, -0.0330, -0.0716,\n",
      "         0.0293,  0.0704,  0.0853,  0.0034,  0.0530,  0.0371, -0.0048,  0.0202,\n",
      "        -0.0107,  0.0998, -0.0042, -0.0979,  0.0668, -0.0265,  0.0098,  0.0513,\n",
      "         0.0831], requires_grad=True)\n",
      "--> Weights[3]:: Params[25] of Shape[torch.Size([1, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[ 0.0761,  0.0980,  0.0201,  0.0777, -0.0018, -0.0498,  0.0567,  0.0706,\n",
      "          0.0181,  0.0627, -0.0525,  0.0492, -0.0021, -0.0371,  0.0456,  0.0060,\n",
      "         -0.0741, -0.0153,  0.0560, -0.0555, -0.0916,  0.0947,  0.0702,  0.0880,\n",
      "         -0.0317]], requires_grad=True)\n",
      "--> Bias[3]:: Params[1] of Shape[torch.Size([1])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([-0.0026], requires_grad=True)\n",
      "--------------------------\n",
      "PARAMS:\t 1,376\n",
      "--------------------------\n",
      "# 2: while not done  1000\n",
      "# 3: outer_epoch:[0] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 192.74098205566406\n",
      "\t Loss: 73.52549743652344\n",
      "\t Loss: 48.08657455444336\n",
      "\t Loss: 32.63756561279297\n",
      "\t Loss: 17.174846649169922\n",
      "Outer loss: 309.3296813964844\n",
      "# 3: outer_epoch:[1] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 115.40272521972656\n",
      "\t Loss: 107.19507598876953\n",
      "\t Loss: 39.2481689453125\n",
      "\t Loss: 27.53609275817871\n",
      "\t Loss: 42.14846420288086\n",
      "Outer loss: 260.3733215332031\n",
      "# 3: outer_epoch:[2] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 83.77241516113281\n",
      "\t Loss: 66.81306457519531\n",
      "\t Loss: 54.39924240112305\n",
      "\t Loss: 13.686625480651855\n",
      "\t Loss: 25.63909912109375\n",
      "Outer loss: 249.74659729003906\n",
      "# 3: outer_epoch:[3] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 78.38531494140625\n",
      "\t Loss: 135.93807983398438\n",
      "\t Loss: 34.27515411376953\n",
      "\t Loss: 39.417083740234375\n",
      "\t Loss: 21.435346603393555\n",
      "Outer loss: 288.54327392578125\n",
      "# 3: outer_epoch:[4] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 84.99494934082031\n",
      "\t Loss: 104.19109344482422\n",
      "\t Loss: 58.139583587646484\n",
      "\t Loss: 35.81033706665039\n",
      "\t Loss: 26.45869255065918\n",
      "Outer loss: 264.9264221191406\n",
      "# 3: outer_epoch:[5] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 165.05130004882812\n",
      "\t Loss: 100.1084213256836\n",
      "\t Loss: 36.907806396484375\n",
      "\t Loss: 25.98401641845703\n",
      "\t Loss: 14.399572372436523\n",
      "Outer loss: 314.9559326171875\n",
      "# 3: outer_epoch:[6] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 68.42456817626953\n",
      "\t Loss: 111.82916259765625\n",
      "\t Loss: 47.81352615356445\n",
      "\t Loss: 26.982187271118164\n",
      "\t Loss: 24.089399337768555\n",
      "Outer loss: 290.0570373535156\n",
      "# 3: outer_epoch:[7] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 104.57946014404297\n",
      "\t Loss: 79.83787536621094\n",
      "\t Loss: 45.985355377197266\n",
      "\t Loss: 24.019681930541992\n",
      "\t Loss: 24.835111618041992\n",
      "Outer loss: 478.971923828125\n",
      "# 3: outer_epoch:[8] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 157.43682861328125\n",
      "\t Loss: 57.58302688598633\n",
      "\t Loss: 39.010498046875\n",
      "\t Loss: 28.199562072753906\n",
      "\t Loss: 29.02987289428711\n",
      "Outer loss: 339.6752624511719\n",
      "# 3: outer_epoch:[9] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.74486541748047\n",
      "\t Loss: 94.18167114257812\n",
      "\t Loss: 49.021427154541016\n",
      "\t Loss: 32.322723388671875\n",
      "\t Loss: 29.07609748840332\n",
      "Outer loss: 305.4184875488281\n",
      "# 3: outer_epoch:[10] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.700815200805664\n",
      "\t Loss: 60.04131317138672\n",
      "\t Loss: 70.10954284667969\n",
      "\t Loss: 36.70731735229492\n",
      "\t Loss: 19.682506561279297\n",
      "Outer loss: 296.116943359375\n",
      "# 3: outer_epoch:[11] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 92.74539947509766\n",
      "\t Loss: 50.373714447021484\n",
      "\t Loss: 34.88229751586914\n",
      "\t Loss: 17.41044044494629\n",
      "\t Loss: 44.44261169433594\n",
      "Outer loss: 339.96990966796875\n",
      "# 3: outer_epoch:[12] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 80.6605224609375\n",
      "\t Loss: 79.95835876464844\n",
      "\t Loss: 50.221763610839844\n",
      "\t Loss: 20.336997985839844\n",
      "\t Loss: 33.61948013305664\n",
      "Outer loss: 283.538330078125\n",
      "# 3: outer_epoch:[13] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 142.74530029296875\n",
      "\t Loss: 103.47349548339844\n",
      "\t Loss: 44.458412170410156\n",
      "\t Loss: 38.90055847167969\n",
      "\t Loss: 19.13393783569336\n",
      "Outer loss: 355.89764404296875\n",
      "# 3: outer_epoch:[14] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 66.20079803466797\n",
      "\t Loss: 82.87788391113281\n",
      "\t Loss: 61.051734924316406\n",
      "\t Loss: 49.0997314453125\n",
      "\t Loss: 35.792598724365234\n",
      "Outer loss: 288.7828063964844\n",
      "# 3: outer_epoch:[15] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 117.91370391845703\n",
      "\t Loss: 105.67485046386719\n",
      "\t Loss: 44.43513107299805\n",
      "\t Loss: 41.00201416015625\n",
      "\t Loss: 38.28141403198242\n",
      "Outer loss: 356.106201171875\n",
      "# 3: outer_epoch:[16] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 129.31837463378906\n",
      "\t Loss: 92.3992919921875\n",
      "\t Loss: 52.46517562866211\n",
      "\t Loss: 31.842002868652344\n",
      "\t Loss: 30.54974365234375\n",
      "Outer loss: 253.67169189453125\n",
      "# 3: outer_epoch:[17] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 55.7139778137207\n",
      "\t Loss: 59.3115348815918\n",
      "\t Loss: 71.27694702148438\n",
      "\t Loss: 42.83863830566406\n",
      "\t Loss: 29.43359375\n",
      "Outer loss: 246.20004272460938\n",
      "# 3: outer_epoch:[18] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 138.64285278320312\n",
      "\t Loss: 77.44761657714844\n",
      "\t Loss: 61.36101531982422\n",
      "\t Loss: 29.439096450805664\n",
      "\t Loss: 22.014732360839844\n",
      "Outer loss: 336.41033935546875\n",
      "# 3: outer_epoch:[19] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 94.09446716308594\n",
      "\t Loss: 108.44236755371094\n",
      "\t Loss: 40.935096740722656\n",
      "\t Loss: 40.561073303222656\n",
      "\t Loss: 46.41664505004883\n",
      "Outer loss: 359.394775390625\n",
      "# 3: outer_epoch:[20] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 154.7779083251953\n",
      "\t Loss: 41.92633819580078\n",
      "\t Loss: 45.69328308105469\n",
      "\t Loss: 24.474079132080078\n",
      "\t Loss: 27.29119873046875\n",
      "Outer loss: 269.71856689453125\n",
      "# 3: outer_epoch:[21] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 145.6210174560547\n",
      "\t Loss: 23.257186889648438\n",
      "\t Loss: 36.85512161254883\n",
      "\t Loss: 28.7100830078125\n",
      "\t Loss: 25.10304069519043\n",
      "Outer loss: 311.79644775390625\n",
      "# 3: outer_epoch:[22] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 84.73939514160156\n",
      "\t Loss: 102.82684326171875\n",
      "\t Loss: 33.0583610534668\n",
      "\t Loss: 29.137998580932617\n",
      "\t Loss: 20.060192108154297\n",
      "Outer loss: 289.9998779296875\n",
      "# 3: outer_epoch:[23] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 123.4295883178711\n",
      "\t Loss: 50.20998001098633\n",
      "\t Loss: 52.54951477050781\n",
      "\t Loss: 36.406028747558594\n",
      "\t Loss: 20.66753387451172\n",
      "Outer loss: 307.8790588378906\n",
      "# 3: outer_epoch:[24] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 147.4766387939453\n",
      "\t Loss: 102.77030944824219\n",
      "\t Loss: 54.496055603027344\n",
      "\t Loss: 20.205434799194336\n",
      "\t Loss: 25.66258430480957\n",
      "Outer loss: 267.68304443359375\n",
      "# 3: outer_epoch:[25] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 117.97032165527344\n",
      "\t Loss: 54.11946105957031\n",
      "\t Loss: 72.96705627441406\n",
      "\t Loss: 29.857484817504883\n",
      "\t Loss: 53.89969253540039\n",
      "Outer loss: 265.8953857421875\n",
      "# 3: outer_epoch:[26] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 206.78704833984375\n",
      "\t Loss: 39.52255630493164\n",
      "\t Loss: 62.87360382080078\n",
      "\t Loss: 38.41779327392578\n",
      "\t Loss: 31.84724998474121\n",
      "Outer loss: 293.430908203125\n",
      "# 3: outer_epoch:[27] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 183.56912231445312\n",
      "\t Loss: 75.27478790283203\n",
      "\t Loss: 59.94588088989258\n",
      "\t Loss: 43.428768157958984\n",
      "\t Loss: 11.377228736877441\n",
      "Outer loss: 357.5037536621094\n",
      "# 3: outer_epoch:[28] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 127.60064697265625\n",
      "\t Loss: 50.99115753173828\n",
      "\t Loss: 65.29454803466797\n",
      "\t Loss: 42.30067443847656\n",
      "\t Loss: 4.256929874420166\n",
      "Outer loss: 250.5542755126953\n",
      "# 3: outer_epoch:[29] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 191.2639617919922\n",
      "\t Loss: 43.60272216796875\n",
      "\t Loss: 21.348037719726562\n",
      "\t Loss: 35.494300842285156\n",
      "\t Loss: 22.6711483001709\n",
      "Outer loss: 403.0207214355469\n",
      "# 3: outer_epoch:[30] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.697021484375\n",
      "\t Loss: 111.03398895263672\n",
      "\t Loss: 67.86531066894531\n",
      "\t Loss: 35.54049301147461\n",
      "\t Loss: 34.85787582397461\n",
      "Outer loss: 304.7256774902344\n",
      "# 3: outer_epoch:[31] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 131.58721923828125\n",
      "\t Loss: 95.66776275634766\n",
      "\t Loss: 41.38844680786133\n",
      "\t Loss: 26.529033660888672\n",
      "\t Loss: 16.568147659301758\n",
      "Outer loss: 278.1031494140625\n",
      "# 3: outer_epoch:[32] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 61.68805694580078\n",
      "\t Loss: 59.46821594238281\n",
      "\t Loss: 58.58987808227539\n",
      "\t Loss: 37.68317794799805\n",
      "\t Loss: 24.924190521240234\n",
      "Outer loss: 328.90484619140625\n",
      "# 3: outer_epoch:[33] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 80.84194946289062\n",
      "\t Loss: 92.58538055419922\n",
      "\t Loss: 31.313377380371094\n",
      "\t Loss: 14.746990203857422\n",
      "\t Loss: 32.41386795043945\n",
      "Outer loss: 358.12158203125\n",
      "# 3: outer_epoch:[34] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 112.04428100585938\n",
      "\t Loss: 97.34803009033203\n",
      "\t Loss: 45.47674560546875\n",
      "\t Loss: 20.66547393798828\n",
      "\t Loss: 34.187416076660156\n",
      "Outer loss: 298.5083312988281\n",
      "# 3: outer_epoch:[35] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 116.46234130859375\n",
      "\t Loss: 76.35073852539062\n",
      "\t Loss: 35.816871643066406\n",
      "\t Loss: 33.280372619628906\n",
      "\t Loss: 27.972375869750977\n",
      "Outer loss: 257.7586975097656\n",
      "# 3: outer_epoch:[36] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 82.17962646484375\n",
      "\t Loss: 68.47006225585938\n",
      "\t Loss: 42.630130767822266\n",
      "\t Loss: 21.446847915649414\n",
      "\t Loss: 17.69487190246582\n",
      "Outer loss: 267.23138427734375\n",
      "# 3: outer_epoch:[37] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 80.06599426269531\n",
      "\t Loss: 112.31320190429688\n",
      "\t Loss: 42.63011169433594\n",
      "\t Loss: 33.73476028442383\n",
      "\t Loss: 54.04950714111328\n",
      "Outer loss: 288.91448974609375\n",
      "# 3: outer_epoch:[38] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 113.49142456054688\n",
      "\t Loss: 53.05006408691406\n",
      "\t Loss: 35.42856979370117\n",
      "\t Loss: 23.7590274810791\n",
      "\t Loss: 14.920662879943848\n",
      "Outer loss: 305.2459716796875\n",
      "# 3: outer_epoch:[39] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 142.2816925048828\n",
      "\t Loss: 82.22573852539062\n",
      "\t Loss: 63.00960159301758\n",
      "\t Loss: 28.358747482299805\n",
      "\t Loss: 34.83987808227539\n",
      "Outer loss: 227.20123291015625\n",
      "# 3: outer_epoch:[40] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 110.29371643066406\n",
      "\t Loss: 64.08525085449219\n",
      "\t Loss: 51.30681228637695\n",
      "\t Loss: 44.083431243896484\n",
      "\t Loss: 12.153656959533691\n",
      "Outer loss: 330.44403076171875\n",
      "# 3: outer_epoch:[41] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 188.8328857421875\n",
      "\t Loss: 68.1505355834961\n",
      "\t Loss: 22.03852653503418\n",
      "\t Loss: 51.92985153198242\n",
      "\t Loss: 32.836029052734375\n",
      "Outer loss: 249.626953125\n",
      "# 3: outer_epoch:[42] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 138.49899291992188\n",
      "\t Loss: 51.02420425415039\n",
      "\t Loss: 69.93267059326172\n",
      "\t Loss: 37.96721649169922\n",
      "\t Loss: 40.44416427612305\n",
      "Outer loss: 231.0942840576172\n",
      "# 3: outer_epoch:[43] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 95.91693115234375\n",
      "\t Loss: 23.069164276123047\n",
      "\t Loss: 52.641300201416016\n",
      "\t Loss: 34.48759841918945\n",
      "\t Loss: 15.184721946716309\n",
      "Outer loss: 265.1999206542969\n",
      "# 3: outer_epoch:[44] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 153.51878356933594\n",
      "\t Loss: 103.13026428222656\n",
      "\t Loss: 46.70698928833008\n",
      "\t Loss: 34.49435043334961\n",
      "\t Loss: 12.691380500793457\n",
      "Outer loss: 262.8270263671875\n",
      "# 3: outer_epoch:[45] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 74.87858581542969\n",
      "\t Loss: 93.2993392944336\n",
      "\t Loss: 64.33187103271484\n",
      "\t Loss: 41.50598907470703\n",
      "\t Loss: 48.87826919555664\n",
      "Outer loss: 334.057373046875\n",
      "# 3: outer_epoch:[46] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 61.559959411621094\n",
      "\t Loss: 60.93043899536133\n",
      "\t Loss: 46.33007049560547\n",
      "\t Loss: 31.48488998413086\n",
      "\t Loss: 35.627662658691406\n",
      "Outer loss: 278.4263000488281\n",
      "# 3: outer_epoch:[47] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 122.44395446777344\n",
      "\t Loss: 84.9865493774414\n",
      "\t Loss: 69.02183532714844\n",
      "\t Loss: 43.39043045043945\n",
      "\t Loss: 17.265045166015625\n",
      "Outer loss: 290.4973449707031\n",
      "# 3: outer_epoch:[48] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 109.84732055664062\n",
      "\t Loss: 29.222028732299805\n",
      "\t Loss: 77.01020050048828\n",
      "\t Loss: 14.685550689697266\n",
      "\t Loss: 12.212543487548828\n",
      "Outer loss: 266.5010070800781\n",
      "# 3: outer_epoch:[49] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 99.8022689819336\n",
      "\t Loss: 100.68112182617188\n",
      "\t Loss: 55.98831558227539\n",
      "\t Loss: 20.422042846679688\n",
      "\t Loss: 21.421009063720703\n",
      "Outer loss: 402.16070556640625\n",
      "# 3: outer_epoch:[50] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 136.88385009765625\n",
      "\t Loss: 75.8128433227539\n",
      "\t Loss: 66.0052719116211\n",
      "\t Loss: 57.04376983642578\n",
      "\t Loss: 25.931278228759766\n",
      "Outer loss: 356.0159606933594\n",
      "# 3: outer_epoch:[51] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 143.38880920410156\n",
      "\t Loss: 70.89160919189453\n",
      "\t Loss: 34.043174743652344\n",
      "\t Loss: 23.61742401123047\n",
      "\t Loss: 32.78148651123047\n",
      "Outer loss: 325.4276123046875\n",
      "# 3: outer_epoch:[52] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 137.12701416015625\n",
      "\t Loss: 69.65389251708984\n",
      "\t Loss: 53.193729400634766\n",
      "\t Loss: 39.56170654296875\n",
      "\t Loss: 24.549549102783203\n",
      "Outer loss: 283.35528564453125\n",
      "# 3: outer_epoch:[53] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 76.72428894042969\n",
      "\t Loss: 37.143428802490234\n",
      "\t Loss: 44.62008285522461\n",
      "\t Loss: 39.08784866333008\n",
      "\t Loss: 33.031497955322266\n",
      "Outer loss: 341.7850341796875\n",
      "# 3: outer_epoch:[54] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 74.89215087890625\n",
      "\t Loss: 41.69023513793945\n",
      "\t Loss: 44.14481735229492\n",
      "\t Loss: 40.96007537841797\n",
      "\t Loss: 58.245941162109375\n",
      "Outer loss: 300.3426208496094\n",
      "# 3: outer_epoch:[55] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 104.24089050292969\n",
      "\t Loss: 75.99827575683594\n",
      "\t Loss: 52.02545928955078\n",
      "\t Loss: 36.14981460571289\n",
      "\t Loss: 49.677406311035156\n",
      "Outer loss: 409.62652587890625\n",
      "# 3: outer_epoch:[56] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 74.8112564086914\n",
      "\t Loss: 70.77782440185547\n",
      "\t Loss: 30.654621124267578\n",
      "\t Loss: 52.16270446777344\n",
      "\t Loss: 13.268649101257324\n",
      "Outer loss: 264.9140625\n",
      "# 3: outer_epoch:[57] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 97.5794448852539\n",
      "\t Loss: 60.151268005371094\n",
      "\t Loss: 46.901100158691406\n",
      "\t Loss: 20.315345764160156\n",
      "\t Loss: 24.919198989868164\n",
      "Outer loss: 242.8313751220703\n",
      "# 3: outer_epoch:[58] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 107.06059265136719\n",
      "\t Loss: 86.41567993164062\n",
      "\t Loss: 51.657588958740234\n",
      "\t Loss: 41.113914489746094\n",
      "\t Loss: 30.27740478515625\n",
      "Outer loss: 380.55670166015625\n",
      "# 3: outer_epoch:[59] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 90.51371002197266\n",
      "\t Loss: 59.83098602294922\n",
      "\t Loss: 56.7613525390625\n",
      "\t Loss: 29.8775691986084\n",
      "\t Loss: 18.21807098388672\n",
      "Outer loss: 346.4227294921875\n",
      "# 3: outer_epoch:[60] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 120.64704895019531\n",
      "\t Loss: 82.70219421386719\n",
      "\t Loss: 31.681018829345703\n",
      "\t Loss: 32.78153610229492\n",
      "\t Loss: 40.879390716552734\n",
      "Outer loss: 300.2055358886719\n",
      "# 3: outer_epoch:[61] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 165.4451904296875\n",
      "\t Loss: 57.54185485839844\n",
      "\t Loss: 58.08399963378906\n",
      "\t Loss: 26.54555892944336\n",
      "\t Loss: 48.1170768737793\n",
      "Outer loss: 320.3330993652344\n",
      "# 3: outer_epoch:[62] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.919403076171875\n",
      "\t Loss: 47.182350158691406\n",
      "\t Loss: 44.781978607177734\n",
      "\t Loss: 39.394195556640625\n",
      "\t Loss: 37.355987548828125\n",
      "Outer loss: 297.58892822265625\n",
      "# 3: outer_epoch:[63] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 109.06985473632812\n",
      "\t Loss: 61.11174774169922\n",
      "\t Loss: 48.249839782714844\n",
      "\t Loss: 15.4698486328125\n",
      "\t Loss: 46.97897720336914\n",
      "Outer loss: 293.33404541015625\n",
      "# 3: outer_epoch:[64] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 198.32302856445312\n",
      "\t Loss: 88.56449127197266\n",
      "\t Loss: 35.836891174316406\n",
      "\t Loss: 36.370086669921875\n",
      "\t Loss: 23.301292419433594\n",
      "Outer loss: 309.9796142578125\n",
      "# 3: outer_epoch:[65] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 202.32228088378906\n",
      "\t Loss: 72.92108917236328\n",
      "\t Loss: 49.54827117919922\n",
      "\t Loss: 70.55326843261719\n",
      "\t Loss: 59.833274841308594\n",
      "Outer loss: 352.4346008300781\n",
      "# 3: outer_epoch:[66] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 72.06605529785156\n",
      "\t Loss: 68.97261810302734\n",
      "\t Loss: 62.87622833251953\n",
      "\t Loss: 41.29435348510742\n",
      "\t Loss: 43.994178771972656\n",
      "Outer loss: 321.9459533691406\n",
      "# 3: outer_epoch:[67] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 105.51388549804688\n",
      "\t Loss: 58.865257263183594\n",
      "\t Loss: 53.284427642822266\n",
      "\t Loss: 36.34568405151367\n",
      "\t Loss: 30.691299438476562\n",
      "Outer loss: 258.5754089355469\n",
      "# 3: outer_epoch:[68] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 99.41934204101562\n",
      "\t Loss: 66.1341552734375\n",
      "\t Loss: 43.0472526550293\n",
      "\t Loss: 56.51272201538086\n",
      "\t Loss: 21.956233978271484\n",
      "Outer loss: 370.42340087890625\n",
      "# 3: outer_epoch:[69] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 115.7137222290039\n",
      "\t Loss: 74.49557495117188\n",
      "\t Loss: 67.95479583740234\n",
      "\t Loss: 27.162578582763672\n",
      "\t Loss: 32.50404739379883\n",
      "Outer loss: 300.9967346191406\n",
      "# 3: outer_epoch:[70] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 115.4991455078125\n",
      "\t Loss: 67.27228546142578\n",
      "\t Loss: 39.93370819091797\n",
      "\t Loss: 37.16701126098633\n",
      "\t Loss: 72.74712371826172\n",
      "Outer loss: 229.45953369140625\n",
      "# 3: outer_epoch:[71] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.92340087890625\n",
      "\t Loss: 103.80689239501953\n",
      "\t Loss: 45.59013366699219\n",
      "\t Loss: 36.78523635864258\n",
      "\t Loss: 35.58091735839844\n",
      "Outer loss: 282.8665771484375\n",
      "# 3: outer_epoch:[72] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 159.53102111816406\n",
      "\t Loss: 67.51499938964844\n",
      "\t Loss: 60.637203216552734\n",
      "\t Loss: 31.57219696044922\n",
      "\t Loss: 62.295345306396484\n",
      "Outer loss: 273.2928771972656\n",
      "# 3: outer_epoch:[73] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 86.93743896484375\n",
      "\t Loss: 90.35456848144531\n",
      "\t Loss: 82.97193908691406\n",
      "\t Loss: 40.20232009887695\n",
      "\t Loss: 15.242706298828125\n",
      "Outer loss: 337.2451477050781\n",
      "# 3: outer_epoch:[74] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 79.13677215576172\n",
      "\t Loss: 86.7234115600586\n",
      "\t Loss: 45.286712646484375\n",
      "\t Loss: 33.003265380859375\n",
      "\t Loss: 29.308090209960938\n",
      "Outer loss: 357.3087158203125\n",
      "# 3: outer_epoch:[75] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 122.81282043457031\n",
      "\t Loss: 61.93246078491211\n",
      "\t Loss: 48.286685943603516\n",
      "\t Loss: 29.534290313720703\n",
      "\t Loss: 38.603118896484375\n",
      "Outer loss: 315.50152587890625\n",
      "# 3: outer_epoch:[76] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 119.422607421875\n",
      "\t Loss: 109.24565887451172\n",
      "\t Loss: 62.70005798339844\n",
      "\t Loss: 35.86577606201172\n",
      "\t Loss: 43.61985397338867\n",
      "Outer loss: 274.5084533691406\n",
      "# 3: outer_epoch:[77] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 95.74154663085938\n",
      "\t Loss: 83.44242095947266\n",
      "\t Loss: 27.484987258911133\n",
      "\t Loss: 53.4691276550293\n",
      "\t Loss: 13.59073543548584\n",
      "Outer loss: 267.57293701171875\n",
      "# 3: outer_epoch:[78] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 117.36253356933594\n",
      "\t Loss: 70.96725463867188\n",
      "\t Loss: 32.533023834228516\n",
      "\t Loss: 25.783327102661133\n",
      "\t Loss: 28.172456741333008\n",
      "Outer loss: 318.78155517578125\n",
      "# 3: outer_epoch:[79] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 117.16539764404297\n",
      "\t Loss: 83.95567321777344\n",
      "\t Loss: 31.22769546508789\n",
      "\t Loss: 30.25881576538086\n",
      "\t Loss: 23.836454391479492\n",
      "Outer loss: 290.3916015625\n",
      "# 3: outer_epoch:[80] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 126.66600036621094\n",
      "\t Loss: 68.70574951171875\n",
      "\t Loss: 45.643795013427734\n",
      "\t Loss: 30.404199600219727\n",
      "\t Loss: 35.292659759521484\n",
      "Outer loss: 266.8635559082031\n",
      "# 3: outer_epoch:[81] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 104.92597198486328\n",
      "\t Loss: 68.84536743164062\n",
      "\t Loss: 36.85750198364258\n",
      "\t Loss: 41.627227783203125\n",
      "\t Loss: 40.24596405029297\n",
      "Outer loss: 311.62054443359375\n",
      "# 3: outer_epoch:[82] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 166.05322265625\n",
      "\t Loss: 80.4689712524414\n",
      "\t Loss: 44.5932502746582\n",
      "\t Loss: 24.586097717285156\n",
      "\t Loss: 44.6679573059082\n",
      "Outer loss: 236.45864868164062\n",
      "# 3: outer_epoch:[83] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 85.53531646728516\n",
      "\t Loss: 97.62305450439453\n",
      "\t Loss: 14.927695274353027\n",
      "\t Loss: 39.406837463378906\n",
      "\t Loss: 46.892433166503906\n",
      "Outer loss: 251.64199829101562\n",
      "# 3: outer_epoch:[84] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 114.20452880859375\n",
      "\t Loss: 70.1425552368164\n",
      "\t Loss: 61.58210754394531\n",
      "\t Loss: 47.95738983154297\n",
      "\t Loss: 58.73477554321289\n",
      "Outer loss: 344.4559631347656\n",
      "# 3: outer_epoch:[85] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 87.16065979003906\n",
      "\t Loss: 113.26164245605469\n",
      "\t Loss: 25.223342895507812\n",
      "\t Loss: 40.73263168334961\n",
      "\t Loss: 10.988622665405273\n",
      "Outer loss: 288.3258056640625\n",
      "# 3: outer_epoch:[86] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 70.31433868408203\n",
      "\t Loss: 80.72557830810547\n",
      "\t Loss: 19.159841537475586\n",
      "\t Loss: 47.20765686035156\n",
      "\t Loss: 54.55058670043945\n",
      "Outer loss: 301.3968505859375\n",
      "# 3: outer_epoch:[87] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 58.027198791503906\n",
      "\t Loss: 103.80342864990234\n",
      "\t Loss: 38.69355010986328\n",
      "\t Loss: 41.02384567260742\n",
      "\t Loss: 30.361963272094727\n",
      "Outer loss: 249.99148559570312\n",
      "# 3: outer_epoch:[88] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 160.837646484375\n",
      "\t Loss: 61.82789611816406\n",
      "\t Loss: 60.33549499511719\n",
      "\t Loss: 16.769662857055664\n",
      "\t Loss: 34.755706787109375\n",
      "Outer loss: 261.4577331542969\n",
      "# 3: outer_epoch:[89] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 98.13427734375\n",
      "\t Loss: 84.46989440917969\n",
      "\t Loss: 47.72663497924805\n",
      "\t Loss: 27.86299705505371\n",
      "\t Loss: 34.042537689208984\n",
      "Outer loss: 282.3856201171875\n",
      "# 3: outer_epoch:[90] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.94541931152344\n",
      "\t Loss: 63.449764251708984\n",
      "\t Loss: 50.12261962890625\n",
      "\t Loss: 25.977439880371094\n",
      "\t Loss: 53.79573440551758\n",
      "Outer loss: 220.78152465820312\n",
      "# 3: outer_epoch:[91] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.939857482910156\n",
      "\t Loss: 98.17082214355469\n",
      "\t Loss: 51.16796112060547\n",
      "\t Loss: 33.255088806152344\n",
      "\t Loss: 44.95109176635742\n",
      "Outer loss: 239.32275390625\n",
      "# 3: outer_epoch:[92] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 112.25273132324219\n",
      "\t Loss: 78.67514038085938\n",
      "\t Loss: 38.62405014038086\n",
      "\t Loss: 32.15693283081055\n",
      "\t Loss: 30.28532600402832\n",
      "Outer loss: 330.6014099121094\n",
      "# 3: outer_epoch:[93] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 96.00920104980469\n",
      "\t Loss: 84.29693603515625\n",
      "\t Loss: 43.15093231201172\n",
      "\t Loss: 24.86029815673828\n",
      "\t Loss: 48.417381286621094\n",
      "Outer loss: 373.7783508300781\n",
      "# 3: outer_epoch:[94] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 124.11962890625\n",
      "\t Loss: 122.33775329589844\n",
      "\t Loss: 43.429569244384766\n",
      "\t Loss: 10.873527526855469\n",
      "\t Loss: 39.17015075683594\n",
      "Outer loss: 274.6180114746094\n",
      "# 3: outer_epoch:[95] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 109.46607971191406\n",
      "\t Loss: 69.2646484375\n",
      "\t Loss: 42.210365295410156\n",
      "\t Loss: 21.352460861206055\n",
      "\t Loss: 42.11113357543945\n",
      "Outer loss: 221.07115173339844\n",
      "# 3: outer_epoch:[96] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 99.50633239746094\n",
      "\t Loss: 57.953025817871094\n",
      "\t Loss: 52.0192985534668\n",
      "\t Loss: 7.663919448852539\n",
      "\t Loss: 26.54523468017578\n",
      "Outer loss: 258.60467529296875\n",
      "# 3: outer_epoch:[97] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 65.37071990966797\n",
      "\t Loss: 69.96403503417969\n",
      "\t Loss: 33.5843505859375\n",
      "\t Loss: 39.809139251708984\n",
      "\t Loss: 27.941478729248047\n",
      "Outer loss: 232.27389526367188\n",
      "# 3: outer_epoch:[98] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.474336624145508\n",
      "\t Loss: 84.63494110107422\n",
      "\t Loss: 64.55150604248047\n",
      "\t Loss: 33.06084442138672\n",
      "\t Loss: 23.60761833190918\n",
      "Outer loss: 206.23019409179688\n",
      "# 3: outer_epoch:[99] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 98.47539520263672\n",
      "\t Loss: 33.56840896606445\n",
      "\t Loss: 42.527870178222656\n",
      "\t Loss: 41.400146484375\n",
      "\t Loss: 37.36104965209961\n",
      "Outer loss: 239.97499084472656\n",
      "# 3: outer_epoch:[100] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 128.83279418945312\n",
      "\t Loss: 90.11956024169922\n",
      "\t Loss: 40.394691467285156\n",
      "\t Loss: 32.167205810546875\n",
      "\t Loss: 26.802047729492188\n",
      "Outer loss: 216.701171875\n",
      "# 3: outer_epoch:[101] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.84215545654297\n",
      "\t Loss: 55.397857666015625\n",
      "\t Loss: 37.743186950683594\n",
      "\t Loss: 22.73003578186035\n",
      "\t Loss: 43.442684173583984\n",
      "Outer loss: 233.01255798339844\n",
      "# 3: outer_epoch:[102] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 128.57022094726562\n",
      "\t Loss: 35.920928955078125\n",
      "\t Loss: 44.72492599487305\n",
      "\t Loss: 9.926848411560059\n",
      "\t Loss: 43.47344207763672\n",
      "Outer loss: 229.93386840820312\n",
      "# 3: outer_epoch:[103] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.876747131347656\n",
      "\t Loss: 87.37474060058594\n",
      "\t Loss: 30.57980728149414\n",
      "\t Loss: 8.3063383102417\n",
      "\t Loss: 23.6447696685791\n",
      "Outer loss: 289.3546142578125\n",
      "# 3: outer_epoch:[104] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.9160270690918\n",
      "\t Loss: 52.77542495727539\n",
      "\t Loss: 39.644771575927734\n",
      "\t Loss: 30.275524139404297\n",
      "\t Loss: 30.81772804260254\n",
      "Outer loss: 156.8328399658203\n",
      "# 3: outer_epoch:[105] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.282623291015625\n",
      "\t Loss: 55.08720016479492\n",
      "\t Loss: 11.688278198242188\n",
      "\t Loss: 29.020296096801758\n",
      "\t Loss: 32.562782287597656\n",
      "Outer loss: 159.64210510253906\n",
      "# 3: outer_epoch:[106] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 93.74283599853516\n",
      "\t Loss: 39.023433685302734\n",
      "\t Loss: 19.35127067565918\n",
      "\t Loss: 25.119888305664062\n",
      "\t Loss: 25.443157196044922\n",
      "Outer loss: 196.3704376220703\n",
      "# 3: outer_epoch:[107] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 127.05062103271484\n",
      "\t Loss: 39.5317497253418\n",
      "\t Loss: 22.07044792175293\n",
      "\t Loss: 7.540253162384033\n",
      "\t Loss: 26.826549530029297\n",
      "Outer loss: 166.86927795410156\n",
      "# 3: outer_epoch:[108] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 103.59300994873047\n",
      "\t Loss: 29.969297409057617\n",
      "\t Loss: 19.81802749633789\n",
      "\t Loss: 14.994159698486328\n",
      "\t Loss: 10.719819068908691\n",
      "Outer loss: 113.57441711425781\n",
      "# 3: outer_epoch:[109] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 59.68467712402344\n",
      "\t Loss: 34.809959411621094\n",
      "\t Loss: 14.922842025756836\n",
      "\t Loss: 17.11681365966797\n",
      "\t Loss: 21.48553466796875\n",
      "Outer loss: 153.51644897460938\n",
      "# 3: outer_epoch:[110] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 70.50164794921875\n",
      "\t Loss: 35.86903381347656\n",
      "\t Loss: 26.69890594482422\n",
      "\t Loss: 7.324882984161377\n",
      "\t Loss: 6.454852104187012\n",
      "Outer loss: 149.85411071777344\n",
      "# 3: outer_epoch:[111] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.62004470825195\n",
      "\t Loss: 49.030433654785156\n",
      "\t Loss: 12.089832305908203\n",
      "\t Loss: 6.0375518798828125\n",
      "\t Loss: 11.030827522277832\n",
      "Outer loss: 139.62527465820312\n",
      "# 3: outer_epoch:[112] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.584720611572266\n",
      "\t Loss: 34.50506591796875\n",
      "\t Loss: 25.227081298828125\n",
      "\t Loss: 14.302030563354492\n",
      "\t Loss: 33.33173751831055\n",
      "Outer loss: 112.4505386352539\n",
      "# 3: outer_epoch:[113] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 59.02617263793945\n",
      "\t Loss: 41.65934371948242\n",
      "\t Loss: 12.178138732910156\n",
      "\t Loss: 9.610895156860352\n",
      "\t Loss: 20.351959228515625\n",
      "Outer loss: 89.94019317626953\n",
      "# 3: outer_epoch:[114] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.900390625\n",
      "\t Loss: 46.35934829711914\n",
      "\t Loss: 15.535107612609863\n",
      "\t Loss: 7.8973612785339355\n",
      "\t Loss: 16.81105613708496\n",
      "Outer loss: 152.9916534423828\n",
      "# 3: outer_epoch:[115] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 97.08934020996094\n",
      "\t Loss: 19.163000106811523\n",
      "\t Loss: 16.51348114013672\n",
      "\t Loss: 12.677325248718262\n",
      "\t Loss: 24.509187698364258\n",
      "Outer loss: 109.4324722290039\n",
      "# 3: outer_epoch:[116] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 101.27326202392578\n",
      "\t Loss: 16.799358367919922\n",
      "\t Loss: 17.920310974121094\n",
      "\t Loss: 6.779140472412109\n",
      "\t Loss: 9.354076385498047\n",
      "Outer loss: 107.5003890991211\n",
      "# 3: outer_epoch:[117] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.39902114868164\n",
      "\t Loss: 27.381332397460938\n",
      "\t Loss: 12.954031944274902\n",
      "\t Loss: 19.300573348999023\n",
      "\t Loss: 22.569250106811523\n",
      "Outer loss: 100.6630859375\n",
      "# 3: outer_epoch:[118] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.92472839355469\n",
      "\t Loss: 52.55096435546875\n",
      "\t Loss: 13.490457534790039\n",
      "\t Loss: 4.676937103271484\n",
      "\t Loss: 14.614880561828613\n",
      "Outer loss: 82.89628601074219\n",
      "# 3: outer_epoch:[119] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 75.24315643310547\n",
      "\t Loss: 16.889339447021484\n",
      "\t Loss: 11.384176254272461\n",
      "\t Loss: 10.891999244689941\n",
      "\t Loss: 10.504069328308105\n",
      "Outer loss: 173.43910217285156\n",
      "# 3: outer_epoch:[120] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.36692810058594\n",
      "\t Loss: 30.701953887939453\n",
      "\t Loss: 24.846637725830078\n",
      "\t Loss: 16.301210403442383\n",
      "\t Loss: 37.73040771484375\n",
      "Outer loss: 71.06231689453125\n",
      "# 3: outer_epoch:[121] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.72910690307617\n",
      "\t Loss: 18.72506332397461\n",
      "\t Loss: 17.328588485717773\n",
      "\t Loss: 26.475746154785156\n",
      "\t Loss: 52.23228454589844\n",
      "Outer loss: 114.15122985839844\n",
      "# 3: outer_epoch:[122] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.01154327392578\n",
      "\t Loss: 35.892635345458984\n",
      "\t Loss: 14.409852981567383\n",
      "\t Loss: 21.38134765625\n",
      "\t Loss: 13.899139404296875\n",
      "Outer loss: 95.40008544921875\n",
      "# 3: outer_epoch:[123] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.59653091430664\n",
      "\t Loss: 25.257692337036133\n",
      "\t Loss: 13.496186256408691\n",
      "\t Loss: 15.192947387695312\n",
      "\t Loss: 20.324838638305664\n",
      "Outer loss: 97.62197875976562\n",
      "# 3: outer_epoch:[124] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 61.54685592651367\n",
      "\t Loss: 22.69611930847168\n",
      "\t Loss: 15.540687561035156\n",
      "\t Loss: 6.478314399719238\n",
      "\t Loss: 33.887386322021484\n",
      "Outer loss: 112.25272369384766\n",
      "# 3: outer_epoch:[125] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.18364715576172\n",
      "\t Loss: 23.5019474029541\n",
      "\t Loss: 10.226710319519043\n",
      "\t Loss: 11.546966552734375\n",
      "\t Loss: 22.13861656188965\n",
      "Outer loss: 96.1428451538086\n",
      "# 3: outer_epoch:[126] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.6287841796875\n",
      "\t Loss: 23.50896644592285\n",
      "\t Loss: 14.133352279663086\n",
      "\t Loss: 2.075256824493408\n",
      "\t Loss: 17.382278442382812\n",
      "Outer loss: 130.2322998046875\n",
      "# 3: outer_epoch:[127] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.80371856689453\n",
      "\t Loss: 15.418646812438965\n",
      "\t Loss: 12.356802940368652\n",
      "\t Loss: 9.320497512817383\n",
      "\t Loss: 20.218168258666992\n",
      "Outer loss: 84.49668884277344\n",
      "# 3: outer_epoch:[128] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 62.99037551879883\n",
      "\t Loss: 19.754432678222656\n",
      "\t Loss: 14.307019233703613\n",
      "\t Loss: 3.7678701877593994\n",
      "\t Loss: 18.114778518676758\n",
      "Outer loss: 69.80426025390625\n",
      "# 3: outer_epoch:[129] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.833980560302734\n",
      "\t Loss: 25.195663452148438\n",
      "\t Loss: 10.787867546081543\n",
      "\t Loss: 17.683650970458984\n",
      "\t Loss: 16.849205017089844\n",
      "Outer loss: 113.99164581298828\n",
      "# 3: outer_epoch:[130] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.961463928222656\n",
      "\t Loss: 14.618149757385254\n",
      "\t Loss: 18.679901123046875\n",
      "\t Loss: 16.69092559814453\n",
      "\t Loss: 39.937931060791016\n",
      "Outer loss: 101.24501037597656\n",
      "# 3: outer_epoch:[131] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.10336685180664\n",
      "\t Loss: 17.89808464050293\n",
      "\t Loss: 12.45235824584961\n",
      "\t Loss: 10.596183776855469\n",
      "\t Loss: 27.365985870361328\n",
      "Outer loss: 88.94731140136719\n",
      "# 3: outer_epoch:[132] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.8243350982666\n",
      "\t Loss: 12.060721397399902\n",
      "\t Loss: 8.922670364379883\n",
      "\t Loss: 29.78588104248047\n",
      "\t Loss: 32.587703704833984\n",
      "Outer loss: 77.24796295166016\n",
      "# 3: outer_epoch:[133] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.650718688964844\n",
      "\t Loss: 33.135982513427734\n",
      "\t Loss: 8.410246849060059\n",
      "\t Loss: 27.542360305786133\n",
      "\t Loss: 29.341522216796875\n",
      "Outer loss: 114.61825561523438\n",
      "# 3: outer_epoch:[134] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 58.6181526184082\n",
      "\t Loss: 35.26692199707031\n",
      "\t Loss: 10.008718490600586\n",
      "\t Loss: 13.311860084533691\n",
      "\t Loss: 21.000930786132812\n",
      "Outer loss: 125.94147491455078\n",
      "# 3: outer_epoch:[135] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.97311782836914\n",
      "\t Loss: 13.072543144226074\n",
      "\t Loss: 5.9544477462768555\n",
      "\t Loss: 17.224123001098633\n",
      "\t Loss: 34.082122802734375\n",
      "Outer loss: 107.49029541015625\n",
      "# 3: outer_epoch:[136] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.9197998046875\n",
      "\t Loss: 27.434844970703125\n",
      "\t Loss: 11.063182830810547\n",
      "\t Loss: 18.420948028564453\n",
      "\t Loss: 21.532020568847656\n",
      "Outer loss: 104.88656616210938\n",
      "# 3: outer_epoch:[137] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.78462219238281\n",
      "\t Loss: 25.702884674072266\n",
      "\t Loss: 10.289759635925293\n",
      "\t Loss: 8.780367851257324\n",
      "\t Loss: 17.552513122558594\n",
      "Outer loss: 92.30357360839844\n",
      "# 3: outer_epoch:[138] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.66728973388672\n",
      "\t Loss: 23.727441787719727\n",
      "\t Loss: 12.35468578338623\n",
      "\t Loss: 7.614843368530273\n",
      "\t Loss: 27.831321716308594\n",
      "Outer loss: 109.44794464111328\n",
      "# 3: outer_epoch:[139] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.947105407714844\n",
      "\t Loss: 21.720136642456055\n",
      "\t Loss: 7.0480852127075195\n",
      "\t Loss: 19.23430824279785\n",
      "\t Loss: 18.025711059570312\n",
      "Outer loss: 76.21593475341797\n",
      "# 3: outer_epoch:[140] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.98773193359375\n",
      "\t Loss: 25.446868896484375\n",
      "\t Loss: 6.0203399658203125\n",
      "\t Loss: 9.63625717163086\n",
      "\t Loss: 16.745561599731445\n",
      "Outer loss: 107.07491302490234\n",
      "# 3: outer_epoch:[141] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.5505313873291\n",
      "\t Loss: 25.458467483520508\n",
      "\t Loss: 17.421104431152344\n",
      "\t Loss: 14.617511749267578\n",
      "\t Loss: 35.872154235839844\n",
      "Outer loss: 84.94417572021484\n",
      "# 3: outer_epoch:[142] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.791595458984375\n",
      "\t Loss: 15.924558639526367\n",
      "\t Loss: 3.2883033752441406\n",
      "\t Loss: 10.704049110412598\n",
      "\t Loss: 30.83831024169922\n",
      "Outer loss: 116.37107849121094\n",
      "# 3: outer_epoch:[143] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.266658782958984\n",
      "\t Loss: 9.990225791931152\n",
      "\t Loss: 11.282212257385254\n",
      "\t Loss: 24.062414169311523\n",
      "\t Loss: 49.50450897216797\n",
      "Outer loss: 99.63501739501953\n",
      "# 3: outer_epoch:[144] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.922075271606445\n",
      "\t Loss: 26.27753448486328\n",
      "\t Loss: 22.37590980529785\n",
      "\t Loss: 9.62445068359375\n",
      "\t Loss: 21.76570701599121\n",
      "Outer loss: 90.81134796142578\n",
      "# 3: outer_epoch:[145] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 56.71145248413086\n",
      "\t Loss: 15.71890926361084\n",
      "\t Loss: 11.5130033493042\n",
      "\t Loss: 10.96762752532959\n",
      "\t Loss: 37.67112350463867\n",
      "Outer loss: 95.40821075439453\n",
      "# 3: outer_epoch:[146] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.774559020996094\n",
      "\t Loss: 28.560237884521484\n",
      "\t Loss: 22.589344024658203\n",
      "\t Loss: 2.604079008102417\n",
      "\t Loss: 10.615400314331055\n",
      "Outer loss: 99.14554595947266\n",
      "# 3: outer_epoch:[147] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 61.14332962036133\n",
      "\t Loss: 7.581791400909424\n",
      "\t Loss: 5.947630405426025\n",
      "\t Loss: 15.523301124572754\n",
      "\t Loss: 7.4176554679870605\n",
      "Outer loss: 118.0693359375\n",
      "# 3: outer_epoch:[148] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.56794548034668\n",
      "\t Loss: 16.62286949157715\n",
      "\t Loss: 15.084511756896973\n",
      "\t Loss: 16.05731964111328\n",
      "\t Loss: 27.492265701293945\n",
      "Outer loss: 89.16921997070312\n",
      "# 3: outer_epoch:[149] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 94.07487487792969\n",
      "\t Loss: 36.33906555175781\n",
      "\t Loss: 7.426534175872803\n",
      "\t Loss: 17.68178939819336\n",
      "\t Loss: 29.75477409362793\n",
      "Outer loss: 87.67549133300781\n",
      "# 3: outer_epoch:[150] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.549470901489258\n",
      "\t Loss: 22.224374771118164\n",
      "\t Loss: 2.966484546661377\n",
      "\t Loss: 29.123634338378906\n",
      "\t Loss: 12.288825988769531\n",
      "Outer loss: 83.02903747558594\n",
      "# 3: outer_epoch:[151] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.31997299194336\n",
      "\t Loss: 14.270373344421387\n",
      "\t Loss: 12.295328140258789\n",
      "\t Loss: 7.484525203704834\n",
      "\t Loss: 35.30579376220703\n",
      "Outer loss: 83.76718139648438\n",
      "# 3: outer_epoch:[152] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.33426284790039\n",
      "\t Loss: 21.42035675048828\n",
      "\t Loss: 14.433945655822754\n",
      "\t Loss: 11.873868942260742\n",
      "\t Loss: 12.469606399536133\n",
      "Outer loss: 88.71599578857422\n",
      "# 3: outer_epoch:[153] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.77982711791992\n",
      "\t Loss: 39.10481262207031\n",
      "\t Loss: 22.43505096435547\n",
      "\t Loss: 8.39852523803711\n",
      "\t Loss: 4.379848003387451\n",
      "Outer loss: 96.84757232666016\n",
      "# 3: outer_epoch:[154] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.928783416748047\n",
      "\t Loss: 19.241138458251953\n",
      "\t Loss: 5.649768829345703\n",
      "\t Loss: 3.451874017715454\n",
      "\t Loss: 14.190505027770996\n",
      "Outer loss: 112.23876190185547\n",
      "# 3: outer_epoch:[155] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.650405883789062\n",
      "\t Loss: 7.315057277679443\n",
      "\t Loss: 23.30190658569336\n",
      "\t Loss: 39.15092468261719\n",
      "\t Loss: 40.50148391723633\n",
      "Outer loss: 81.42019653320312\n",
      "# 3: outer_epoch:[156] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.01054000854492\n",
      "\t Loss: 15.811838150024414\n",
      "\t Loss: 9.933120727539062\n",
      "\t Loss: 36.773311614990234\n",
      "\t Loss: 43.895233154296875\n",
      "Outer loss: 87.58673095703125\n",
      "# 3: outer_epoch:[157] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.01118469238281\n",
      "\t Loss: 10.21612548828125\n",
      "\t Loss: 9.83639144897461\n",
      "\t Loss: 11.546151161193848\n",
      "\t Loss: 37.24456024169922\n",
      "Outer loss: 65.21757507324219\n",
      "# 3: outer_epoch:[158] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.30222702026367\n",
      "\t Loss: 28.172332763671875\n",
      "\t Loss: 20.277694702148438\n",
      "\t Loss: 5.349109172821045\n",
      "\t Loss: 29.95013427734375\n",
      "Outer loss: 55.846622467041016\n",
      "# 3: outer_epoch:[159] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.5039176940918\n",
      "\t Loss: 16.508594512939453\n",
      "\t Loss: 9.452685356140137\n",
      "\t Loss: 9.98599910736084\n",
      "\t Loss: 22.655550003051758\n",
      "Outer loss: 112.35238647460938\n",
      "# 3: outer_epoch:[160] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.47464370727539\n",
      "\t Loss: 12.257719039916992\n",
      "\t Loss: 15.987339973449707\n",
      "\t Loss: 12.288236618041992\n",
      "\t Loss: 19.003028869628906\n",
      "Outer loss: 71.63595581054688\n",
      "# 3: outer_epoch:[161] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.2407112121582\n",
      "\t Loss: 7.281368732452393\n",
      "\t Loss: 6.349609375\n",
      "\t Loss: 17.550151824951172\n",
      "\t Loss: 29.847728729248047\n",
      "Outer loss: 79.05883026123047\n",
      "# 3: outer_epoch:[162] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.835811614990234\n",
      "\t Loss: 10.383757591247559\n",
      "\t Loss: 3.8290812969207764\n",
      "\t Loss: 12.736083984375\n",
      "\t Loss: 46.17934036254883\n",
      "Outer loss: 91.93366241455078\n",
      "# 3: outer_epoch:[163] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.07081985473633\n",
      "\t Loss: 16.5526065826416\n",
      "\t Loss: 11.05903434753418\n",
      "\t Loss: 20.339311599731445\n",
      "\t Loss: 57.45138168334961\n",
      "Outer loss: 68.45350646972656\n",
      "# 3: outer_epoch:[164] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.82109832763672\n",
      "\t Loss: 14.185497283935547\n",
      "\t Loss: 1.3149471282958984\n",
      "\t Loss: 7.035146236419678\n",
      "\t Loss: 24.179176330566406\n",
      "Outer loss: 70.94357299804688\n",
      "# 3: outer_epoch:[165] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.367183685302734\n",
      "\t Loss: 11.050562858581543\n",
      "\t Loss: 16.34784698486328\n",
      "\t Loss: 9.889806747436523\n",
      "\t Loss: 23.731264114379883\n",
      "Outer loss: 78.39488220214844\n",
      "# 3: outer_epoch:[166] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.15912628173828\n",
      "\t Loss: 24.213054656982422\n",
      "\t Loss: 9.449087142944336\n",
      "\t Loss: 15.201539039611816\n",
      "\t Loss: 29.608678817749023\n",
      "Outer loss: 78.73124694824219\n",
      "# 3: outer_epoch:[167] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.563777923583984\n",
      "\t Loss: 10.855744361877441\n",
      "\t Loss: 20.20256233215332\n",
      "\t Loss: 10.69332504272461\n",
      "\t Loss: 15.36661434173584\n",
      "Outer loss: 108.0242919921875\n",
      "# 3: outer_epoch:[168] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.65974044799805\n",
      "\t Loss: 14.476424217224121\n",
      "\t Loss: 13.12136459350586\n",
      "\t Loss: 5.500524044036865\n",
      "\t Loss: 29.13954734802246\n",
      "Outer loss: 90.08882904052734\n",
      "# 3: outer_epoch:[169] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.100988388061523\n",
      "\t Loss: 16.088333129882812\n",
      "\t Loss: 9.507787704467773\n",
      "\t Loss: 15.544981956481934\n",
      "\t Loss: 16.038850784301758\n",
      "Outer loss: 111.53894805908203\n",
      "# 3: outer_epoch:[170] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.181833267211914\n",
      "\t Loss: 30.848783493041992\n",
      "\t Loss: 22.98988914489746\n",
      "\t Loss: 22.3660945892334\n",
      "\t Loss: 31.148136138916016\n",
      "Outer loss: 69.12571716308594\n",
      "# 3: outer_epoch:[171] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 76.47750854492188\n",
      "\t Loss: 18.195608139038086\n",
      "\t Loss: 15.467409133911133\n",
      "\t Loss: 6.136704444885254\n",
      "\t Loss: 41.93634796142578\n",
      "Outer loss: 109.12287902832031\n",
      "# 3: outer_epoch:[172] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 53.24987030029297\n",
      "\t Loss: 14.330170631408691\n",
      "\t Loss: 8.05134105682373\n",
      "\t Loss: 13.357240676879883\n",
      "\t Loss: 18.63432502746582\n",
      "Outer loss: 77.2009048461914\n",
      "# 3: outer_epoch:[173] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.24187469482422\n",
      "\t Loss: 19.076387405395508\n",
      "\t Loss: 4.503036975860596\n",
      "\t Loss: 11.55069351196289\n",
      "\t Loss: 12.566017150878906\n",
      "Outer loss: 85.49862670898438\n",
      "# 3: outer_epoch:[174] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.970909118652344\n",
      "\t Loss: 18.240625381469727\n",
      "\t Loss: 1.3806254863739014\n",
      "\t Loss: 10.076263427734375\n",
      "\t Loss: 22.474504470825195\n",
      "Outer loss: 112.00497436523438\n",
      "# 3: outer_epoch:[175] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.63019561767578\n",
      "\t Loss: 17.415925979614258\n",
      "\t Loss: 15.444990158081055\n",
      "\t Loss: 10.606596946716309\n",
      "\t Loss: 34.054325103759766\n",
      "Outer loss: 74.54913330078125\n",
      "# 3: outer_epoch:[176] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.68181610107422\n",
      "\t Loss: 24.162220001220703\n",
      "\t Loss: 9.092912673950195\n",
      "\t Loss: 13.694408416748047\n",
      "\t Loss: 34.001625061035156\n",
      "Outer loss: 120.39985656738281\n",
      "# 3: outer_epoch:[177] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 72.48087310791016\n",
      "\t Loss: 10.161243438720703\n",
      "\t Loss: 8.153833389282227\n",
      "\t Loss: 16.373767852783203\n",
      "\t Loss: 49.6143798828125\n",
      "Outer loss: 68.87040710449219\n",
      "# 3: outer_epoch:[178] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.172962188720703\n",
      "\t Loss: 10.98910903930664\n",
      "\t Loss: 12.088743209838867\n",
      "\t Loss: 32.24897003173828\n",
      "\t Loss: 39.187965393066406\n",
      "Outer loss: 86.0904541015625\n",
      "# 3: outer_epoch:[179] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.51768493652344\n",
      "\t Loss: 23.546354293823242\n",
      "\t Loss: 5.03892183303833\n",
      "\t Loss: 9.697563171386719\n",
      "\t Loss: 38.08757781982422\n",
      "Outer loss: 110.38092803955078\n",
      "# 3: outer_epoch:[180] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.34370422363281\n",
      "\t Loss: 21.537803649902344\n",
      "\t Loss: 4.181427955627441\n",
      "\t Loss: 15.805957794189453\n",
      "\t Loss: 55.63662338256836\n",
      "Outer loss: 93.1124267578125\n",
      "# 3: outer_epoch:[181] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.04826736450195\n",
      "\t Loss: 7.204399108886719\n",
      "\t Loss: 12.21601390838623\n",
      "\t Loss: 10.761983871459961\n",
      "\t Loss: 35.50452423095703\n",
      "Outer loss: 99.75354766845703\n",
      "# 3: outer_epoch:[182] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.900081634521484\n",
      "\t Loss: 32.85847473144531\n",
      "\t Loss: 11.677699089050293\n",
      "\t Loss: 11.41366958618164\n",
      "\t Loss: 18.53068733215332\n",
      "Outer loss: 86.93793487548828\n",
      "# 3: outer_epoch:[183] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 99.86993408203125\n",
      "\t Loss: 25.716899871826172\n",
      "\t Loss: 5.5052995681762695\n",
      "\t Loss: 4.996447563171387\n",
      "\t Loss: 27.935955047607422\n",
      "Outer loss: 113.96635437011719\n",
      "# 3: outer_epoch:[184] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.53841018676758\n",
      "\t Loss: 17.805082321166992\n",
      "\t Loss: 6.446576118469238\n",
      "\t Loss: 3.7839837074279785\n",
      "\t Loss: 29.11971664428711\n",
      "Outer loss: 106.46626281738281\n",
      "# 3: outer_epoch:[185] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.943321228027344\n",
      "\t Loss: 20.27393341064453\n",
      "\t Loss: 6.491107940673828\n",
      "\t Loss: 9.942404747009277\n",
      "\t Loss: 26.377653121948242\n",
      "Outer loss: 84.25131225585938\n",
      "# 3: outer_epoch:[186] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 52.31150817871094\n",
      "\t Loss: 16.735017776489258\n",
      "\t Loss: 9.111421585083008\n",
      "\t Loss: 16.789583206176758\n",
      "\t Loss: 35.51438522338867\n",
      "Outer loss: 73.27159118652344\n",
      "# 3: outer_epoch:[187] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.45709991455078\n",
      "\t Loss: 12.688855171203613\n",
      "\t Loss: 13.437028884887695\n",
      "\t Loss: 21.103593826293945\n",
      "\t Loss: 39.81482696533203\n",
      "Outer loss: 80.95327758789062\n",
      "# 3: outer_epoch:[188] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 56.0731315612793\n",
      "\t Loss: 21.35467529296875\n",
      "\t Loss: 4.068270683288574\n",
      "\t Loss: 16.342308044433594\n",
      "\t Loss: 24.087669372558594\n",
      "Outer loss: 104.80335235595703\n",
      "# 3: outer_epoch:[189] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.394548416137695\n",
      "\t Loss: 19.416255950927734\n",
      "\t Loss: 8.73324203491211\n",
      "\t Loss: 21.055343627929688\n",
      "\t Loss: 34.52042007446289\n",
      "Outer loss: 77.81523895263672\n",
      "# 3: outer_epoch:[190] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.77725601196289\n",
      "\t Loss: 15.319799423217773\n",
      "\t Loss: 2.8080825805664062\n",
      "\t Loss: 9.681988716125488\n",
      "\t Loss: 29.767276763916016\n",
      "Outer loss: 83.42552185058594\n",
      "# 3: outer_epoch:[191] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.329620361328125\n",
      "\t Loss: 6.568201541900635\n",
      "\t Loss: 12.532194137573242\n",
      "\t Loss: 23.63309097290039\n",
      "\t Loss: 42.137271881103516\n",
      "Outer loss: 83.61376953125\n",
      "# 3: outer_epoch:[192] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.52729034423828\n",
      "\t Loss: 17.80282974243164\n",
      "\t Loss: 7.710503578186035\n",
      "\t Loss: 12.648313522338867\n",
      "\t Loss: 19.97911834716797\n",
      "Outer loss: 96.396728515625\n",
      "# 3: outer_epoch:[193] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.762460708618164\n",
      "\t Loss: 37.6518440246582\n",
      "\t Loss: 7.085024833679199\n",
      "\t Loss: 12.299489974975586\n",
      "\t Loss: 18.065895080566406\n",
      "Outer loss: 113.86387634277344\n",
      "# 3: outer_epoch:[194] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.472740173339844\n",
      "\t Loss: 24.58681869506836\n",
      "\t Loss: 5.786740303039551\n",
      "\t Loss: 7.1573638916015625\n",
      "\t Loss: 32.099098205566406\n",
      "Outer loss: 80.28536987304688\n",
      "# 3: outer_epoch:[195] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.62053680419922\n",
      "\t Loss: 11.514460563659668\n",
      "\t Loss: 6.309698104858398\n",
      "\t Loss: 11.27682113647461\n",
      "\t Loss: 25.5665283203125\n",
      "Outer loss: 67.45719146728516\n",
      "# 3: outer_epoch:[196] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.20484161376953\n",
      "\t Loss: 19.693431854248047\n",
      "\t Loss: 12.177873611450195\n",
      "\t Loss: 7.736692428588867\n",
      "\t Loss: 31.414623260498047\n",
      "Outer loss: 88.72573852539062\n",
      "# 3: outer_epoch:[197] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 55.63852310180664\n",
      "\t Loss: 40.60602569580078\n",
      "\t Loss: 7.197263717651367\n",
      "\t Loss: 6.463491916656494\n",
      "\t Loss: 32.86210632324219\n",
      "Outer loss: 73.91802215576172\n",
      "# 3: outer_epoch:[198] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.035669326782227\n",
      "\t Loss: 21.559289932250977\n",
      "\t Loss: 6.630922317504883\n",
      "\t Loss: 6.838438987731934\n",
      "\t Loss: 18.56322479248047\n",
      "Outer loss: 67.24515533447266\n",
      "# 3: outer_epoch:[199] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.27338409423828\n",
      "\t Loss: 10.858699798583984\n",
      "\t Loss: 10.07485294342041\n",
      "\t Loss: 8.194343566894531\n",
      "\t Loss: 21.503942489624023\n",
      "Outer loss: 62.005104064941406\n",
      "# 3: outer_epoch:[200] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.72779846191406\n",
      "\t Loss: 13.855552673339844\n",
      "\t Loss: 3.1041953563690186\n",
      "\t Loss: 10.456315040588379\n",
      "\t Loss: 25.30445671081543\n",
      "Outer loss: 58.310157775878906\n",
      "# 3: outer_epoch:[201] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.70561408996582\n",
      "\t Loss: 11.714115142822266\n",
      "\t Loss: 6.273919105529785\n",
      "\t Loss: 21.52899169921875\n",
      "\t Loss: 38.844322204589844\n",
      "Outer loss: 73.72618865966797\n",
      "# 3: outer_epoch:[202] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.305124282836914\n",
      "\t Loss: 17.59391975402832\n",
      "\t Loss: 12.13351821899414\n",
      "\t Loss: 30.28098487854004\n",
      "\t Loss: 32.09941864013672\n",
      "Outer loss: 111.22956848144531\n",
      "# 3: outer_epoch:[203] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 65.73725128173828\n",
      "\t Loss: 19.890592575073242\n",
      "\t Loss: 8.27026653289795\n",
      "\t Loss: 11.11852741241455\n",
      "\t Loss: 48.967342376708984\n",
      "Outer loss: 56.20637130737305\n",
      "# 3: outer_epoch:[204] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.86326217651367\n",
      "\t Loss: 13.564983367919922\n",
      "\t Loss: 5.723603248596191\n",
      "\t Loss: 12.972987174987793\n",
      "\t Loss: 28.99193000793457\n",
      "Outer loss: 76.00154876708984\n",
      "# 3: outer_epoch:[205] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.983059883117676\n",
      "\t Loss: 7.823393821716309\n",
      "\t Loss: 7.959709167480469\n",
      "\t Loss: 12.413426399230957\n",
      "\t Loss: 46.48036193847656\n",
      "Outer loss: 97.43389892578125\n",
      "# 3: outer_epoch:[206] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.983671188354492\n",
      "\t Loss: 15.172353744506836\n",
      "\t Loss: 13.688079833984375\n",
      "\t Loss: 26.38721466064453\n",
      "\t Loss: 44.353790283203125\n",
      "Outer loss: 65.11460876464844\n",
      "# 3: outer_epoch:[207] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.73065948486328\n",
      "\t Loss: 20.29692840576172\n",
      "\t Loss: 8.963641166687012\n",
      "\t Loss: 8.358192443847656\n",
      "\t Loss: 13.915384292602539\n",
      "Outer loss: 83.83845520019531\n",
      "# 3: outer_epoch:[208] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.235572814941406\n",
      "\t Loss: 13.710838317871094\n",
      "\t Loss: 8.19531536102295\n",
      "\t Loss: 4.966826915740967\n",
      "\t Loss: 30.516040802001953\n",
      "Outer loss: 101.30154418945312\n",
      "# 3: outer_epoch:[209] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.876787185668945\n",
      "\t Loss: 10.188163757324219\n",
      "\t Loss: 15.818733215332031\n",
      "\t Loss: 7.3880228996276855\n",
      "\t Loss: 24.198259353637695\n",
      "Outer loss: 83.2328109741211\n",
      "# 3: outer_epoch:[210] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.74361801147461\n",
      "\t Loss: 12.089877128601074\n",
      "\t Loss: 8.862122535705566\n",
      "\t Loss: 29.74045181274414\n",
      "\t Loss: 48.163475036621094\n",
      "Outer loss: 86.67117309570312\n",
      "# 3: outer_epoch:[211] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 56.9586181640625\n",
      "\t Loss: 20.8013973236084\n",
      "\t Loss: 14.452573776245117\n",
      "\t Loss: 11.240234375\n",
      "\t Loss: 24.65275764465332\n",
      "Outer loss: 69.19212341308594\n",
      "# 3: outer_epoch:[212] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.479766845703125\n",
      "\t Loss: 24.373016357421875\n",
      "\t Loss: 14.397943496704102\n",
      "\t Loss: 17.424428939819336\n",
      "\t Loss: 22.360605239868164\n",
      "Outer loss: 112.58979797363281\n",
      "# 3: outer_epoch:[213] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.597999572753906\n",
      "\t Loss: 14.782364845275879\n",
      "\t Loss: 14.510683059692383\n",
      "\t Loss: 20.235401153564453\n",
      "\t Loss: 45.795684814453125\n",
      "Outer loss: 56.642982482910156\n",
      "# 3: outer_epoch:[214] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.53626823425293\n",
      "\t Loss: 20.281417846679688\n",
      "\t Loss: 12.148587226867676\n",
      "\t Loss: 13.237159729003906\n",
      "\t Loss: 31.5577392578125\n",
      "Outer loss: 67.05184936523438\n",
      "# 3: outer_epoch:[215] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.503713607788086\n",
      "\t Loss: 6.785552978515625\n",
      "\t Loss: 10.584845542907715\n",
      "\t Loss: 22.925804138183594\n",
      "\t Loss: 28.67058753967285\n",
      "Outer loss: 65.18682861328125\n",
      "# 3: outer_epoch:[216] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.57998275756836\n",
      "\t Loss: 16.359283447265625\n",
      "\t Loss: 12.607810974121094\n",
      "\t Loss: 18.827350616455078\n",
      "\t Loss: 33.15054702758789\n",
      "Outer loss: 108.83053588867188\n",
      "# 3: outer_epoch:[217] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 240.81304931640625\n",
      "\t Loss: 141.71585083007812\n",
      "\t Loss: 89.00064849853516\n",
      "\t Loss: 14.932955741882324\n",
      "\t Loss: 18.78220558166504\n",
      "Outer loss: 225.72718811035156\n",
      "# 3: outer_epoch:[218] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 76.05879974365234\n",
      "\t Loss: 132.68084716796875\n",
      "\t Loss: 39.90803527832031\n",
      "\t Loss: 29.26622772216797\n",
      "\t Loss: 20.73163604736328\n",
      "Outer loss: 399.40594482421875\n",
      "# 3: outer_epoch:[219] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 66.44517517089844\n",
      "\t Loss: 71.96541595458984\n",
      "\t Loss: 20.772645950317383\n",
      "\t Loss: 8.592830657958984\n",
      "\t Loss: 16.550086975097656\n",
      "Outer loss: 93.07269287109375\n",
      "# 3: outer_epoch:[220] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.74480056762695\n",
      "\t Loss: 11.681668281555176\n",
      "\t Loss: 10.8936185836792\n",
      "\t Loss: 13.106743812561035\n",
      "\t Loss: 34.90605163574219\n",
      "Outer loss: 70.87238311767578\n",
      "# 3: outer_epoch:[221] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.91719055175781\n",
      "\t Loss: 12.975530624389648\n",
      "\t Loss: 11.388870239257812\n",
      "\t Loss: 12.892305374145508\n",
      "\t Loss: 10.555879592895508\n",
      "Outer loss: 92.27212524414062\n",
      "# 3: outer_epoch:[222] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.591197967529297\n",
      "\t Loss: 13.248405456542969\n",
      "\t Loss: 15.199117660522461\n",
      "\t Loss: 11.415021896362305\n",
      "\t Loss: 42.073097229003906\n",
      "Outer loss: 65.33631134033203\n",
      "# 3: outer_epoch:[223] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.29958724975586\n",
      "\t Loss: 10.166292190551758\n",
      "\t Loss: 5.951018810272217\n",
      "\t Loss: 17.275222778320312\n",
      "\t Loss: 28.519306182861328\n",
      "Outer loss: 87.89816284179688\n",
      "# 3: outer_epoch:[224] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.968732833862305\n",
      "\t Loss: 23.252819061279297\n",
      "\t Loss: 17.659988403320312\n",
      "\t Loss: 29.635112762451172\n",
      "\t Loss: 65.44386291503906\n",
      "Outer loss: 48.835601806640625\n",
      "# 3: outer_epoch:[225] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.130495071411133\n",
      "\t Loss: 7.700267314910889\n",
      "\t Loss: 22.065279006958008\n",
      "\t Loss: 25.271753311157227\n",
      "\t Loss: 53.49615478515625\n",
      "Outer loss: 84.11288452148438\n",
      "# 3: outer_epoch:[226] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.35285949707031\n",
      "\t Loss: 9.210631370544434\n",
      "\t Loss: 9.080994606018066\n",
      "\t Loss: 16.393531799316406\n",
      "\t Loss: 51.16753387451172\n",
      "Outer loss: 64.9766845703125\n",
      "# 3: outer_epoch:[227] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.819395065307617\n",
      "\t Loss: 12.684370994567871\n",
      "\t Loss: 5.307248115539551\n",
      "\t Loss: 16.64944076538086\n",
      "\t Loss: 32.63921356201172\n",
      "Outer loss: 56.89950180053711\n",
      "# 3: outer_epoch:[228] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.7133731842041\n",
      "\t Loss: 22.926733016967773\n",
      "\t Loss: 5.976515769958496\n",
      "\t Loss: 27.160110473632812\n",
      "\t Loss: 33.214210510253906\n",
      "Outer loss: 69.24078369140625\n",
      "# 3: outer_epoch:[229] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.51910400390625\n",
      "\t Loss: 15.471431732177734\n",
      "\t Loss: 40.366722106933594\n",
      "\t Loss: 33.50088119506836\n",
      "\t Loss: 83.29971313476562\n",
      "Outer loss: 87.5801010131836\n",
      "# 3: outer_epoch:[230] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.969329833984375\n",
      "\t Loss: 11.624129295349121\n",
      "\t Loss: 14.085031509399414\n",
      "\t Loss: 25.53757095336914\n",
      "\t Loss: 7.68968391418457\n",
      "Outer loss: 66.58047485351562\n",
      "# 3: outer_epoch:[231] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.682750701904297\n",
      "\t Loss: 21.628000259399414\n",
      "\t Loss: 6.091739654541016\n",
      "\t Loss: 6.891138076782227\n",
      "\t Loss: 18.624858856201172\n",
      "Outer loss: 91.42398834228516\n",
      "# 3: outer_epoch:[232] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.79513168334961\n",
      "\t Loss: 7.7062201499938965\n",
      "\t Loss: 5.80996561050415\n",
      "\t Loss: 14.521678924560547\n",
      "\t Loss: 30.83046531677246\n",
      "Outer loss: 87.54281616210938\n",
      "# 3: outer_epoch:[233] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.093210220336914\n",
      "\t Loss: 26.320350646972656\n",
      "\t Loss: 5.697342872619629\n",
      "\t Loss: 15.120229721069336\n",
      "\t Loss: 36.29347610473633\n",
      "Outer loss: 89.24896240234375\n",
      "# 3: outer_epoch:[234] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.09257698059082\n",
      "\t Loss: 21.211463928222656\n",
      "\t Loss: 12.026022911071777\n",
      "\t Loss: 16.343555450439453\n",
      "\t Loss: 28.952051162719727\n",
      "Outer loss: 83.03473663330078\n",
      "# 3: outer_epoch:[235] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.49940299987793\n",
      "\t Loss: 9.828084945678711\n",
      "\t Loss: 4.6057515144348145\n",
      "\t Loss: 12.165135383605957\n",
      "\t Loss: 54.42545700073242\n",
      "Outer loss: 76.78968811035156\n",
      "# 3: outer_epoch:[236] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.153339385986328\n",
      "\t Loss: 8.584487915039062\n",
      "\t Loss: 5.772547721862793\n",
      "\t Loss: 13.58004379272461\n",
      "\t Loss: 20.846939086914062\n",
      "Outer loss: 67.58822631835938\n",
      "# 3: outer_epoch:[237] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.175758361816406\n",
      "\t Loss: 11.587996482849121\n",
      "\t Loss: 12.049843788146973\n",
      "\t Loss: 20.40291404724121\n",
      "\t Loss: 40.77037048339844\n",
      "Outer loss: 49.16307067871094\n",
      "# 3: outer_epoch:[238] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.673503875732422\n",
      "\t Loss: 12.604008674621582\n",
      "\t Loss: 8.568571090698242\n",
      "\t Loss: 7.014278411865234\n",
      "\t Loss: 26.615314483642578\n",
      "Outer loss: 78.62142181396484\n",
      "# 3: outer_epoch:[239] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.755716323852539\n",
      "\t Loss: 10.083893775939941\n",
      "\t Loss: 6.374931335449219\n",
      "\t Loss: 22.30187225341797\n",
      "\t Loss: 48.8759651184082\n",
      "Outer loss: 93.02023315429688\n",
      "# 3: outer_epoch:[240] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.967071533203125\n",
      "\t Loss: 21.096450805664062\n",
      "\t Loss: 15.999646186828613\n",
      "\t Loss: 9.778701782226562\n",
      "\t Loss: 39.02803039550781\n",
      "Outer loss: 56.521427154541016\n",
      "# 3: outer_epoch:[241] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.752685546875\n",
      "\t Loss: 8.965882301330566\n",
      "\t Loss: 4.5064191818237305\n",
      "\t Loss: 13.677167892456055\n",
      "\t Loss: 53.679595947265625\n",
      "Outer loss: 79.41732788085938\n",
      "# 3: outer_epoch:[242] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.297117233276367\n",
      "\t Loss: 9.823116302490234\n",
      "\t Loss: 8.146875381469727\n",
      "\t Loss: 20.752653121948242\n",
      "\t Loss: 42.941802978515625\n",
      "Outer loss: 73.90874481201172\n",
      "# 3: outer_epoch:[243] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.095458984375\n",
      "\t Loss: 8.998934745788574\n",
      "\t Loss: 22.16069221496582\n",
      "\t Loss: 34.28205490112305\n",
      "\t Loss: 53.561180114746094\n",
      "Outer loss: 60.22092056274414\n",
      "# 3: outer_epoch:[244] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.189414978027344\n",
      "\t Loss: 8.871984481811523\n",
      "\t Loss: 2.2544355392456055\n",
      "\t Loss: 9.029882431030273\n",
      "\t Loss: 40.05014419555664\n",
      "Outer loss: 72.15670013427734\n",
      "# 3: outer_epoch:[245] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.21837615966797\n",
      "\t Loss: 12.126066207885742\n",
      "\t Loss: 6.407442569732666\n",
      "\t Loss: 5.7798051834106445\n",
      "\t Loss: 28.520727157592773\n",
      "Outer loss: 66.40645599365234\n",
      "# 3: outer_epoch:[246] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.059682846069336\n",
      "\t Loss: 8.204242706298828\n",
      "\t Loss: 6.305721759796143\n",
      "\t Loss: 18.08201789855957\n",
      "\t Loss: 24.33650779724121\n",
      "Outer loss: 64.42091369628906\n",
      "# 3: outer_epoch:[247] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.221378326416016\n",
      "\t Loss: 11.095946311950684\n",
      "\t Loss: 8.989532470703125\n",
      "\t Loss: 13.853673934936523\n",
      "\t Loss: 28.19322395324707\n",
      "Outer loss: 58.7097053527832\n",
      "# 3: outer_epoch:[248] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.215911865234375\n",
      "\t Loss: 14.942880630493164\n",
      "\t Loss: 1.9512029886245728\n",
      "\t Loss: 16.072586059570312\n",
      "\t Loss: 38.06367874145508\n",
      "Outer loss: 45.75594711303711\n",
      "# 3: outer_epoch:[249] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.88972854614258\n",
      "\t Loss: 14.571557998657227\n",
      "\t Loss: 5.4078874588012695\n",
      "\t Loss: 9.192763328552246\n",
      "\t Loss: 14.297615051269531\n",
      "Outer loss: 72.87702941894531\n",
      "# 3: outer_epoch:[250] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.473474502563477\n",
      "\t Loss: 7.0267133712768555\n",
      "\t Loss: 11.72262191772461\n",
      "\t Loss: 22.550006866455078\n",
      "\t Loss: 49.05191421508789\n",
      "Outer loss: 79.73884582519531\n",
      "# 3: outer_epoch:[251] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 109.34629821777344\n",
      "\t Loss: 120.97496032714844\n",
      "\t Loss: 94.94786071777344\n",
      "\t Loss: 143.23297119140625\n",
      "\t Loss: 279.4749450683594\n",
      "Outer loss: 71.1502685546875\n",
      "# 3: outer_epoch:[252] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 113.7074966430664\n",
      "\t Loss: 39.271663665771484\n",
      "\t Loss: 41.9483642578125\n",
      "\t Loss: 68.48856353759766\n",
      "\t Loss: 203.9393310546875\n",
      "Outer loss: 175.08621215820312\n",
      "# 3: outer_epoch:[253] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 81.45794677734375\n",
      "\t Loss: 18.971763610839844\n",
      "\t Loss: 13.647259712219238\n",
      "\t Loss: 6.011257171630859\n",
      "\t Loss: 24.521530151367188\n",
      "Outer loss: 110.15179443359375\n",
      "# 3: outer_epoch:[254] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.120893478393555\n",
      "\t Loss: 22.343671798706055\n",
      "\t Loss: 10.93032169342041\n",
      "\t Loss: 11.916023254394531\n",
      "\t Loss: 26.80811309814453\n",
      "Outer loss: 100.449462890625\n",
      "# 3: outer_epoch:[255] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.89527130126953\n",
      "\t Loss: 16.2431697845459\n",
      "\t Loss: 11.802209854125977\n",
      "\t Loss: 16.34920883178711\n",
      "\t Loss: 36.51692581176758\n",
      "Outer loss: 55.86286926269531\n",
      "# 3: outer_epoch:[256] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.78940773010254\n",
      "\t Loss: 4.508467674255371\n",
      "\t Loss: 3.642998695373535\n",
      "\t Loss: 4.889000415802002\n",
      "\t Loss: 25.901700973510742\n",
      "Outer loss: 86.9206771850586\n",
      "# 3: outer_epoch:[257] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.74407196044922\n",
      "\t Loss: 19.363426208496094\n",
      "\t Loss: 7.89791202545166\n",
      "\t Loss: 5.522777080535889\n",
      "\t Loss: 29.887588500976562\n",
      "Outer loss: 54.35702133178711\n",
      "# 3: outer_epoch:[258] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.944236755371094\n",
      "\t Loss: 13.252503395080566\n",
      "\t Loss: 13.218936920166016\n",
      "\t Loss: 8.40689468383789\n",
      "\t Loss: 26.217994689941406\n",
      "Outer loss: 88.73080444335938\n",
      "# 3: outer_epoch:[259] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.13410186767578\n",
      "\t Loss: 20.04833221435547\n",
      "\t Loss: 15.412484169006348\n",
      "\t Loss: 38.1170768737793\n",
      "\t Loss: 64.40327453613281\n",
      "Outer loss: 67.14569091796875\n",
      "# 3: outer_epoch:[260] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.4588565826416\n",
      "\t Loss: 11.078435897827148\n",
      "\t Loss: 12.093055725097656\n",
      "\t Loss: 22.96788787841797\n",
      "\t Loss: 40.440547943115234\n",
      "Outer loss: 78.36317443847656\n",
      "# 3: outer_epoch:[261] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.039227485656738\n",
      "\t Loss: 10.743816375732422\n",
      "\t Loss: 9.449230194091797\n",
      "\t Loss: 21.199298858642578\n",
      "\t Loss: 35.76813888549805\n",
      "Outer loss: 82.57340240478516\n",
      "# 3: outer_epoch:[262] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 77.57527923583984\n",
      "\t Loss: 22.867652893066406\n",
      "\t Loss: 15.781899452209473\n",
      "\t Loss: 18.180891036987305\n",
      "\t Loss: 27.857769012451172\n",
      "Outer loss: 87.09894561767578\n",
      "# 3: outer_epoch:[263] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.862504959106445\n",
      "\t Loss: 15.62167739868164\n",
      "\t Loss: 10.453832626342773\n",
      "\t Loss: 13.004701614379883\n",
      "\t Loss: 31.2388916015625\n",
      "Outer loss: 48.34780502319336\n",
      "# 3: outer_epoch:[264] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.6220588684082\n",
      "\t Loss: 22.996543884277344\n",
      "\t Loss: 6.856335639953613\n",
      "\t Loss: 9.948657035827637\n",
      "\t Loss: 18.023391723632812\n",
      "Outer loss: 97.50889587402344\n",
      "# 3: outer_epoch:[265] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.614415168762207\n",
      "\t Loss: 12.901206016540527\n",
      "\t Loss: 23.082944869995117\n",
      "\t Loss: 43.92717361450195\n",
      "\t Loss: 59.97949981689453\n",
      "Outer loss: 56.483367919921875\n",
      "# 3: outer_epoch:[266] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.821792602539062\n",
      "\t Loss: 6.6035661697387695\n",
      "\t Loss: 7.529995918273926\n",
      "\t Loss: 36.48349380493164\n",
      "\t Loss: 49.208457946777344\n",
      "Outer loss: 58.45035171508789\n",
      "# 3: outer_epoch:[267] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.593354225158691\n",
      "\t Loss: 5.5059428215026855\n",
      "\t Loss: 10.419081687927246\n",
      "\t Loss: 22.395099639892578\n",
      "\t Loss: 45.24637222290039\n",
      "Outer loss: 61.27904510498047\n",
      "# 3: outer_epoch:[268] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.390735626220703\n",
      "\t Loss: 9.047379493713379\n",
      "\t Loss: 8.180292129516602\n",
      "\t Loss: 18.352558135986328\n",
      "\t Loss: 33.728633880615234\n",
      "Outer loss: 62.82767868041992\n",
      "# 3: outer_epoch:[269] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.967430114746094\n",
      "\t Loss: 9.680474281311035\n",
      "\t Loss: 13.056609153747559\n",
      "\t Loss: 10.351667404174805\n",
      "\t Loss: 21.525209426879883\n",
      "Outer loss: 50.974063873291016\n",
      "# 3: outer_epoch:[270] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.898162841796875\n",
      "\t Loss: 13.05748462677002\n",
      "\t Loss: 8.91978645324707\n",
      "\t Loss: 8.230335235595703\n",
      "\t Loss: 14.92774486541748\n",
      "Outer loss: 47.97088623046875\n",
      "# 3: outer_epoch:[271] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.72170066833496\n",
      "\t Loss: 23.346290588378906\n",
      "\t Loss: 2.808558225631714\n",
      "\t Loss: 14.954967498779297\n",
      "\t Loss: 27.166318893432617\n",
      "Outer loss: 43.6338005065918\n",
      "# 3: outer_epoch:[272] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.546077728271484\n",
      "\t Loss: 7.559807777404785\n",
      "\t Loss: 2.327470302581787\n",
      "\t Loss: 9.193668365478516\n",
      "\t Loss: 11.785710334777832\n",
      "Outer loss: 61.514766693115234\n",
      "# 3: outer_epoch:[273] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.83871841430664\n",
      "\t Loss: 23.581172943115234\n",
      "\t Loss: 8.982169151306152\n",
      "\t Loss: 13.453137397766113\n",
      "\t Loss: 35.323081970214844\n",
      "Outer loss: 37.88726043701172\n",
      "# 3: outer_epoch:[274] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.2784309387207\n",
      "\t Loss: 14.763242721557617\n",
      "\t Loss: 13.814074516296387\n",
      "\t Loss: 10.48961353302002\n",
      "\t Loss: 25.32732391357422\n",
      "Outer loss: 59.49219512939453\n",
      "# 3: outer_epoch:[275] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 58.51828384399414\n",
      "\t Loss: 10.377971649169922\n",
      "\t Loss: 9.626137733459473\n",
      "\t Loss: 11.407177925109863\n",
      "\t Loss: 10.80317497253418\n",
      "Outer loss: 116.15418243408203\n",
      "# 3: outer_epoch:[276] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 118.66112518310547\n",
      "\t Loss: 99.41204833984375\n",
      "\t Loss: 90.93391418457031\n",
      "\t Loss: 240.48941040039062\n",
      "\t Loss: 155.7791748046875\n",
      "Outer loss: 152.90887451171875\n",
      "# 3: outer_epoch:[277] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 72.64778900146484\n",
      "\t Loss: 19.56277084350586\n",
      "\t Loss: 7.35476016998291\n",
      "\t Loss: 10.388496398925781\n",
      "\t Loss: 24.019739151000977\n",
      "Outer loss: 80.60257720947266\n",
      "# 3: outer_epoch:[278] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.53671646118164\n",
      "\t Loss: 14.282221794128418\n",
      "\t Loss: 7.068103790283203\n",
      "\t Loss: 14.496001243591309\n",
      "\t Loss: 13.682117462158203\n",
      "Outer loss: 49.00532150268555\n",
      "# 3: outer_epoch:[279] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.267743110656738\n",
      "\t Loss: 17.879913330078125\n",
      "\t Loss: 7.80185604095459\n",
      "\t Loss: 8.586976051330566\n",
      "\t Loss: 43.43913269042969\n",
      "Outer loss: 59.024436950683594\n",
      "# 3: outer_epoch:[280] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.74725341796875\n",
      "\t Loss: 12.967491149902344\n",
      "\t Loss: 4.126734733581543\n",
      "\t Loss: 14.014314651489258\n",
      "\t Loss: 22.37662696838379\n",
      "Outer loss: 74.19699096679688\n",
      "# 3: outer_epoch:[281] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.54745101928711\n",
      "\t Loss: 15.649296760559082\n",
      "\t Loss: 10.806679725646973\n",
      "\t Loss: 9.483114242553711\n",
      "\t Loss: 9.920552253723145\n",
      "Outer loss: 54.07360076904297\n",
      "# 3: outer_epoch:[282] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.1202507019043\n",
      "\t Loss: 7.412867069244385\n",
      "\t Loss: 0.680841863155365\n",
      "\t Loss: 6.937873363494873\n",
      "\t Loss: 28.536861419677734\n",
      "Outer loss: 37.4749755859375\n",
      "# 3: outer_epoch:[283] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.729934692382812\n",
      "\t Loss: 12.099851608276367\n",
      "\t Loss: 4.978230953216553\n",
      "\t Loss: 5.656031131744385\n",
      "\t Loss: 24.76189422607422\n",
      "Outer loss: 75.95672607421875\n",
      "# 3: outer_epoch:[284] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.708248138427734\n",
      "\t Loss: 9.598177909851074\n",
      "\t Loss: 6.582942008972168\n",
      "\t Loss: 11.014862060546875\n",
      "\t Loss: 23.70978546142578\n",
      "Outer loss: 54.34091567993164\n",
      "# 3: outer_epoch:[285] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.016891479492188\n",
      "\t Loss: 3.8080227375030518\n",
      "\t Loss: 12.442720413208008\n",
      "\t Loss: 16.772886276245117\n",
      "\t Loss: 33.15263366699219\n",
      "Outer loss: 41.25772476196289\n",
      "# 3: outer_epoch:[286] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.067875862121582\n",
      "\t Loss: 12.268850326538086\n",
      "\t Loss: 4.0080671310424805\n",
      "\t Loss: 14.73538875579834\n",
      "\t Loss: 27.985671997070312\n",
      "Outer loss: 69.1407241821289\n",
      "# 3: outer_epoch:[287] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.517160415649414\n",
      "\t Loss: 8.268556594848633\n",
      "\t Loss: 12.058761596679688\n",
      "\t Loss: 29.175317764282227\n",
      "\t Loss: 35.665985107421875\n",
      "Outer loss: 44.117271423339844\n",
      "# 3: outer_epoch:[288] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.73549461364746\n",
      "\t Loss: 2.7989754676818848\n",
      "\t Loss: 7.190150260925293\n",
      "\t Loss: 14.897666931152344\n",
      "\t Loss: 81.86837768554688\n",
      "Outer loss: 51.14329147338867\n",
      "# 3: outer_epoch:[289] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.366283416748047\n",
      "\t Loss: 3.0845203399658203\n",
      "\t Loss: 6.611654758453369\n",
      "\t Loss: 15.143298149108887\n",
      "\t Loss: 53.05021286010742\n",
      "Outer loss: 45.3559455871582\n",
      "# 3: outer_epoch:[290] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.050054550170898\n",
      "\t Loss: 4.811349391937256\n",
      "\t Loss: 12.912240982055664\n",
      "\t Loss: 27.98714256286621\n",
      "\t Loss: 47.29899215698242\n",
      "Outer loss: 61.8043327331543\n",
      "# 3: outer_epoch:[291] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.533628463745117\n",
      "\t Loss: 4.183950424194336\n",
      "\t Loss: 9.677201271057129\n",
      "\t Loss: 14.893027305603027\n",
      "\t Loss: 74.09215545654297\n",
      "Outer loss: 54.98454666137695\n",
      "# 3: outer_epoch:[292] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.554393768310547\n",
      "\t Loss: 3.696288585662842\n",
      "\t Loss: 5.538486480712891\n",
      "\t Loss: 9.433243751525879\n",
      "\t Loss: 45.62983703613281\n",
      "Outer loss: 58.24283218383789\n",
      "# 3: outer_epoch:[293] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.83816146850586\n",
      "\t Loss: 24.330713272094727\n",
      "\t Loss: 23.822811126708984\n",
      "\t Loss: 48.71963882446289\n",
      "\t Loss: 94.51753234863281\n",
      "Outer loss: 84.0714340209961\n",
      "# 3: outer_epoch:[294] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 69.30901336669922\n",
      "\t Loss: 37.78818130493164\n",
      "\t Loss: 11.129898071289062\n",
      "\t Loss: 5.724148750305176\n",
      "\t Loss: 12.160415649414062\n",
      "Outer loss: 54.1633186340332\n",
      "# 3: outer_epoch:[295] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 64.882080078125\n",
      "\t Loss: 17.380130767822266\n",
      "\t Loss: 4.9125871658325195\n",
      "\t Loss: 3.213933229446411\n",
      "\t Loss: 20.157102584838867\n",
      "Outer loss: 37.167076110839844\n",
      "# 3: outer_epoch:[296] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.1873664855957\n",
      "\t Loss: 14.951290130615234\n",
      "\t Loss: 5.422013759613037\n",
      "\t Loss: 4.323591232299805\n",
      "\t Loss: 35.78034210205078\n",
      "Outer loss: 48.329689025878906\n",
      "# 3: outer_epoch:[297] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.47425079345703\n",
      "\t Loss: 11.220657348632812\n",
      "\t Loss: 5.036978244781494\n",
      "\t Loss: 11.781137466430664\n",
      "\t Loss: 31.21150016784668\n",
      "Outer loss: 49.01426696777344\n",
      "# 3: outer_epoch:[298] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.42632293701172\n",
      "\t Loss: 13.369884490966797\n",
      "\t Loss: 4.054588794708252\n",
      "\t Loss: 7.208062171936035\n",
      "\t Loss: 26.69086456298828\n",
      "Outer loss: 40.86597442626953\n",
      "# 3: outer_epoch:[299] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.695510864257812\n",
      "\t Loss: 6.730876445770264\n",
      "\t Loss: 7.553694248199463\n",
      "\t Loss: 11.347797393798828\n",
      "\t Loss: 22.087276458740234\n",
      "Outer loss: 52.7265739440918\n",
      "# 3: outer_epoch:[300] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 54.84651565551758\n",
      "\t Loss: 18.814374923706055\n",
      "\t Loss: 4.417314529418945\n",
      "\t Loss: 5.212374210357666\n",
      "\t Loss: 10.125808715820312\n",
      "Outer loss: 28.67068862915039\n",
      "# 3: outer_epoch:[301] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.74620819091797\n",
      "\t Loss: 28.21306800842285\n",
      "\t Loss: 5.8629350662231445\n",
      "\t Loss: 8.571168899536133\n",
      "\t Loss: 25.727859497070312\n",
      "Outer loss: 49.021366119384766\n",
      "# 3: outer_epoch:[302] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.105432510375977\n",
      "\t Loss: 6.108010292053223\n",
      "\t Loss: 22.57472038269043\n",
      "\t Loss: 20.26370620727539\n",
      "\t Loss: 55.95804977416992\n",
      "Outer loss: 42.40461730957031\n",
      "# 3: outer_epoch:[303] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.63671875\n",
      "\t Loss: 2.5680222511291504\n",
      "\t Loss: 3.686800718307495\n",
      "\t Loss: 17.103967666625977\n",
      "\t Loss: 35.26498031616211\n",
      "Outer loss: 49.20653533935547\n",
      "# 3: outer_epoch:[304] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.886491775512695\n",
      "\t Loss: 5.25847864151001\n",
      "\t Loss: 2.0615603923797607\n",
      "\t Loss: 14.638010025024414\n",
      "\t Loss: 37.56228256225586\n",
      "Outer loss: 48.3497428894043\n",
      "# 3: outer_epoch:[305] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.303529739379883\n",
      "\t Loss: 17.084474563598633\n",
      "\t Loss: 11.212992668151855\n",
      "\t Loss: 15.132364273071289\n",
      "\t Loss: 38.73133850097656\n",
      "Outer loss: 38.12738037109375\n",
      "# 3: outer_epoch:[306] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.0715274810791\n",
      "\t Loss: 17.37301254272461\n",
      "\t Loss: 2.313947916030884\n",
      "\t Loss: 15.985042572021484\n",
      "\t Loss: 43.745906829833984\n",
      "Outer loss: 64.09786224365234\n",
      "# 3: outer_epoch:[307] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.69804573059082\n",
      "\t Loss: 6.878242492675781\n",
      "\t Loss: 4.480679512023926\n",
      "\t Loss: 10.45122241973877\n",
      "\t Loss: 63.15583419799805\n",
      "Outer loss: 45.60384750366211\n",
      "# 3: outer_epoch:[308] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.69481086730957\n",
      "\t Loss: 4.619290351867676\n",
      "\t Loss: 9.861892700195312\n",
      "\t Loss: 15.016237258911133\n",
      "\t Loss: 23.270368576049805\n",
      "Outer loss: 54.695396423339844\n",
      "# 3: outer_epoch:[309] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.69059753417969\n",
      "\t Loss: 17.41596794128418\n",
      "\t Loss: 1.442152976989746\n",
      "\t Loss: 9.587695121765137\n",
      "\t Loss: 41.064064025878906\n",
      "Outer loss: 19.512310028076172\n",
      "# 3: outer_epoch:[310] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.261873245239258\n",
      "\t Loss: 4.400906085968018\n",
      "\t Loss: 1.5077261924743652\n",
      "\t Loss: 10.2979736328125\n",
      "\t Loss: 23.17835235595703\n",
      "Outer loss: 38.85025405883789\n",
      "# 3: outer_epoch:[311] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.825233459472656\n",
      "\t Loss: 13.169539451599121\n",
      "\t Loss: 2.533755302429199\n",
      "\t Loss: 13.715676307678223\n",
      "\t Loss: 22.253812789916992\n",
      "Outer loss: 57.25625991821289\n",
      "# 3: outer_epoch:[312] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.53647804260254\n",
      "\t Loss: 7.941895008087158\n",
      "\t Loss: 5.164144039154053\n",
      "\t Loss: 11.298700332641602\n",
      "\t Loss: 65.80643463134766\n",
      "Outer loss: 30.116539001464844\n",
      "# 3: outer_epoch:[313] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.664453506469727\n",
      "\t Loss: 2.292407989501953\n",
      "\t Loss: 4.922572612762451\n",
      "\t Loss: 13.191755294799805\n",
      "\t Loss: 34.40748596191406\n",
      "Outer loss: 54.34265899658203\n",
      "# 3: outer_epoch:[314] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.60265350341797\n",
      "\t Loss: 25.613828659057617\n",
      "\t Loss: 49.52614974975586\n",
      "\t Loss: 42.83733367919922\n",
      "\t Loss: 111.31658172607422\n",
      "Outer loss: 98.01066589355469\n",
      "# 3: outer_epoch:[315] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 59.47620391845703\n",
      "\t Loss: 73.01728820800781\n",
      "\t Loss: 50.3868293762207\n",
      "\t Loss: 21.85882568359375\n",
      "\t Loss: 6.103854179382324\n",
      "Outer loss: 278.3973693847656\n",
      "# 3: outer_epoch:[316] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.29738235473633\n",
      "\t Loss: 40.05418395996094\n",
      "\t Loss: 20.41213607788086\n",
      "\t Loss: 6.581644058227539\n",
      "\t Loss: 16.588293075561523\n",
      "Outer loss: 62.11812973022461\n",
      "# 3: outer_epoch:[317] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.03425598144531\n",
      "\t Loss: 8.923259735107422\n",
      "\t Loss: 6.018678665161133\n",
      "\t Loss: 18.480493545532227\n",
      "\t Loss: 55.43173599243164\n",
      "Outer loss: 58.75404357910156\n",
      "# 3: outer_epoch:[318] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.12816047668457\n",
      "\t Loss: 5.556274890899658\n",
      "\t Loss: 10.148061752319336\n",
      "\t Loss: 29.112346649169922\n",
      "\t Loss: 64.56739807128906\n",
      "Outer loss: 48.522735595703125\n",
      "# 3: outer_epoch:[319] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.81929588317871\n",
      "\t Loss: 2.803501844406128\n",
      "\t Loss: 3.6861743927001953\n",
      "\t Loss: 25.777624130249023\n",
      "\t Loss: 39.06352615356445\n",
      "Outer loss: 48.042232513427734\n",
      "# 3: outer_epoch:[320] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.823290824890137\n",
      "\t Loss: 11.986319541931152\n",
      "\t Loss: 6.899636268615723\n",
      "\t Loss: 15.107645034790039\n",
      "\t Loss: 30.95553207397461\n",
      "Outer loss: 32.18476104736328\n",
      "# 3: outer_epoch:[321] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.76206398010254\n",
      "\t Loss: 4.505776405334473\n",
      "\t Loss: 14.062926292419434\n",
      "\t Loss: 20.243488311767578\n",
      "\t Loss: 36.082340240478516\n",
      "Outer loss: 43.41530227661133\n",
      "# 3: outer_epoch:[322] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.56685447692871\n",
      "\t Loss: 14.369507789611816\n",
      "\t Loss: 2.345803737640381\n",
      "\t Loss: 18.763858795166016\n",
      "\t Loss: 24.788103103637695\n",
      "Outer loss: 46.547061920166016\n",
      "# 3: outer_epoch:[323] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.30843162536621\n",
      "\t Loss: 9.447240829467773\n",
      "\t Loss: 4.805570602416992\n",
      "\t Loss: 9.596487045288086\n",
      "\t Loss: 39.18735122680664\n",
      "Outer loss: 24.88865089416504\n",
      "# 3: outer_epoch:[324] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.289575576782227\n",
      "\t Loss: 15.634777069091797\n",
      "\t Loss: 3.0648858547210693\n",
      "\t Loss: 11.829227447509766\n",
      "\t Loss: 37.6746826171875\n",
      "Outer loss: 44.47141647338867\n",
      "# 3: outer_epoch:[325] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.441696166992188\n",
      "\t Loss: 4.705131530761719\n",
      "\t Loss: 4.237070560455322\n",
      "\t Loss: 15.032587051391602\n",
      "\t Loss: 30.731740951538086\n",
      "Outer loss: 27.39772605895996\n",
      "# 3: outer_epoch:[326] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.341638565063477\n",
      "\t Loss: 4.089690685272217\n",
      "\t Loss: 10.52953815460205\n",
      "\t Loss: 18.857635498046875\n",
      "\t Loss: 31.444034576416016\n",
      "Outer loss: 41.457759857177734\n",
      "# 3: outer_epoch:[327] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.593772888183594\n",
      "\t Loss: 3.165443181991577\n",
      "\t Loss: 9.007685661315918\n",
      "\t Loss: 30.778249740600586\n",
      "\t Loss: 48.19281768798828\n",
      "Outer loss: 37.437740325927734\n",
      "# 3: outer_epoch:[328] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.88707160949707\n",
      "\t Loss: 6.105899810791016\n",
      "\t Loss: 10.920620918273926\n",
      "\t Loss: 19.886882781982422\n",
      "\t Loss: 34.8333740234375\n",
      "Outer loss: 56.50994873046875\n",
      "# 3: outer_epoch:[329] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.714998245239258\n",
      "\t Loss: 4.838864803314209\n",
      "\t Loss: 6.195601463317871\n",
      "\t Loss: 7.109542369842529\n",
      "\t Loss: 28.75804328918457\n",
      "Outer loss: 69.45317077636719\n",
      "# 3: outer_epoch:[330] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 111.96747589111328\n",
      "\t Loss: 164.26756286621094\n",
      "\t Loss: 144.32936096191406\n",
      "\t Loss: 206.14051818847656\n",
      "\t Loss: 330.1700439453125\n",
      "Outer loss: 100.1021728515625\n",
      "# 3: outer_epoch:[331] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.617042541503906\n",
      "\t Loss: 22.779510498046875\n",
      "\t Loss: 13.797667503356934\n",
      "\t Loss: 12.771763801574707\n",
      "\t Loss: 46.51215362548828\n",
      "Outer loss: 75.21485900878906\n",
      "# 3: outer_epoch:[332] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.39177703857422\n",
      "\t Loss: 15.394407272338867\n",
      "\t Loss: 1.9894787073135376\n",
      "\t Loss: 7.418155670166016\n",
      "\t Loss: 25.826866149902344\n",
      "Outer loss: 55.44779968261719\n",
      "# 3: outer_epoch:[333] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.228696823120117\n",
      "\t Loss: 8.04971694946289\n",
      "\t Loss: 5.261079788208008\n",
      "\t Loss: 10.456009864807129\n",
      "\t Loss: 25.771541595458984\n",
      "Outer loss: 54.01334762573242\n",
      "# 3: outer_epoch:[334] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.5188102722168\n",
      "\t Loss: 9.913922309875488\n",
      "\t Loss: 7.671468734741211\n",
      "\t Loss: 4.0203657150268555\n",
      "\t Loss: 21.483192443847656\n",
      "Outer loss: 40.6799201965332\n",
      "# 3: outer_epoch:[335] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.4688835144043\n",
      "\t Loss: 14.955809593200684\n",
      "\t Loss: 1.8373770713806152\n",
      "\t Loss: 6.638567924499512\n",
      "\t Loss: 19.598535537719727\n",
      "Outer loss: 51.6217155456543\n",
      "# 3: outer_epoch:[336] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.264705657958984\n",
      "\t Loss: 17.245498657226562\n",
      "\t Loss: 9.748273849487305\n",
      "\t Loss: 3.550741672515869\n",
      "\t Loss: 20.39984130859375\n",
      "Outer loss: 68.644287109375\n",
      "# 3: outer_epoch:[337] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.31694793701172\n",
      "\t Loss: 12.160399436950684\n",
      "\t Loss: 4.445650577545166\n",
      "\t Loss: 10.122566223144531\n",
      "\t Loss: 26.933849334716797\n",
      "Outer loss: 47.02473831176758\n",
      "# 3: outer_epoch:[338] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 54.7205924987793\n",
      "\t Loss: 4.030465126037598\n",
      "\t Loss: 2.04950213432312\n",
      "\t Loss: 4.967113971710205\n",
      "\t Loss: 21.44866943359375\n",
      "Outer loss: 40.13420104980469\n",
      "# 3: outer_epoch:[339] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.647998809814453\n",
      "\t Loss: 18.11231803894043\n",
      "\t Loss: 2.9997096061706543\n",
      "\t Loss: 5.08610725402832\n",
      "\t Loss: 12.05986213684082\n",
      "Outer loss: 43.86273956298828\n",
      "# 3: outer_epoch:[340] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.67875289916992\n",
      "\t Loss: 12.400371551513672\n",
      "\t Loss: 3.342716693878174\n",
      "\t Loss: 4.735533237457275\n",
      "\t Loss: 18.201404571533203\n",
      "Outer loss: 49.425262451171875\n",
      "# 3: outer_epoch:[341] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.52434158325195\n",
      "\t Loss: 13.835433959960938\n",
      "\t Loss: 1.5010086297988892\n",
      "\t Loss: 5.926684379577637\n",
      "\t Loss: 26.054431915283203\n",
      "Outer loss: 38.1013298034668\n",
      "# 3: outer_epoch:[342] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.912118911743164\n",
      "\t Loss: 13.258060455322266\n",
      "\t Loss: 2.342097043991089\n",
      "\t Loss: 8.889448165893555\n",
      "\t Loss: 29.062416076660156\n",
      "Outer loss: 38.93109130859375\n",
      "# 3: outer_epoch:[343] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.606563568115234\n",
      "\t Loss: 9.331376075744629\n",
      "\t Loss: 2.196162223815918\n",
      "\t Loss: 3.731346368789673\n",
      "\t Loss: 8.867555618286133\n",
      "Outer loss: 40.699462890625\n",
      "# 3: outer_epoch:[344] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.321304321289062\n",
      "\t Loss: 16.257001876831055\n",
      "\t Loss: 1.77672278881073\n",
      "\t Loss: 9.849150657653809\n",
      "\t Loss: 31.733274459838867\n",
      "Outer loss: 35.47126388549805\n",
      "# 3: outer_epoch:[345] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.559722900390625\n",
      "\t Loss: 4.42437219619751\n",
      "\t Loss: 4.628311634063721\n",
      "\t Loss: 18.843454360961914\n",
      "\t Loss: 41.8504524230957\n",
      "Outer loss: 36.35896682739258\n",
      "# 3: outer_epoch:[346] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.964231491088867\n",
      "\t Loss: 6.182553291320801\n",
      "\t Loss: 0.7567422986030579\n",
      "\t Loss: 13.929304122924805\n",
      "\t Loss: 17.248680114746094\n",
      "Outer loss: 44.16770935058594\n",
      "# 3: outer_epoch:[347] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.538095474243164\n",
      "\t Loss: 12.47087287902832\n",
      "\t Loss: 3.147012710571289\n",
      "\t Loss: 18.132722854614258\n",
      "\t Loss: 54.01042938232422\n",
      "Outer loss: 37.29289245605469\n",
      "# 3: outer_epoch:[348] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.791137218475342\n",
      "\t Loss: 7.256721019744873\n",
      "\t Loss: 15.08530330657959\n",
      "\t Loss: 35.218353271484375\n",
      "\t Loss: 65.26463317871094\n",
      "Outer loss: 34.11662292480469\n",
      "# 3: outer_epoch:[349] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.386083602905273\n",
      "\t Loss: 8.66892147064209\n",
      "\t Loss: 12.173030853271484\n",
      "\t Loss: 33.62588882446289\n",
      "\t Loss: 73.24186706542969\n",
      "Outer loss: 51.55096435546875\n",
      "# 3: outer_epoch:[350] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.318706512451172\n",
      "\t Loss: 1.8505135774612427\n",
      "\t Loss: 4.0267229080200195\n",
      "\t Loss: 8.426076889038086\n",
      "\t Loss: 11.576465606689453\n",
      "Outer loss: 38.79814529418945\n",
      "# 3: outer_epoch:[351] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.217975616455078\n",
      "\t Loss: 1.994096279144287\n",
      "\t Loss: 6.327746868133545\n",
      "\t Loss: 26.816417694091797\n",
      "\t Loss: 46.47276306152344\n",
      "Outer loss: 25.961374282836914\n",
      "# 3: outer_epoch:[352] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.3435258865356445\n",
      "\t Loss: 2.2711052894592285\n",
      "\t Loss: 3.2610111236572266\n",
      "\t Loss: 19.22653579711914\n",
      "\t Loss: 46.417091369628906\n",
      "Outer loss: 49.55677032470703\n",
      "# 3: outer_epoch:[353] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.651862144470215\n",
      "\t Loss: 4.664176940917969\n",
      "\t Loss: 4.574995040893555\n",
      "\t Loss: 16.350847244262695\n",
      "\t Loss: 47.72071838378906\n",
      "Outer loss: 34.84199905395508\n",
      "# 3: outer_epoch:[354] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.299695014953613\n",
      "\t Loss: 13.41657543182373\n",
      "\t Loss: 5.614052772521973\n",
      "\t Loss: 13.62424373626709\n",
      "\t Loss: 44.88467025756836\n",
      "Outer loss: 44.61244201660156\n",
      "# 3: outer_epoch:[355] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.800430297851562\n",
      "\t Loss: 11.378968238830566\n",
      "\t Loss: 3.19450044631958\n",
      "\t Loss: 8.84032154083252\n",
      "\t Loss: 18.707443237304688\n",
      "Outer loss: 27.89875030517578\n",
      "# 3: outer_epoch:[356] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.501693725585938\n",
      "\t Loss: 10.045174598693848\n",
      "\t Loss: 1.2459170818328857\n",
      "\t Loss: 7.224869251251221\n",
      "\t Loss: 9.475173950195312\n",
      "Outer loss: 62.84639358520508\n",
      "# 3: outer_epoch:[357] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.44837188720703\n",
      "\t Loss: 15.071626663208008\n",
      "\t Loss: 13.57058334350586\n",
      "\t Loss: 25.394229888916016\n",
      "\t Loss: 56.097537994384766\n",
      "Outer loss: 38.33253479003906\n",
      "# 3: outer_epoch:[358] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.345327377319336\n",
      "\t Loss: 12.829863548278809\n",
      "\t Loss: 1.737370491027832\n",
      "\t Loss: 7.991988182067871\n",
      "\t Loss: 27.046092987060547\n",
      "Outer loss: 30.921743392944336\n",
      "# 3: outer_epoch:[359] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.334346771240234\n",
      "\t Loss: 12.61599063873291\n",
      "\t Loss: 3.9502859115600586\n",
      "\t Loss: 2.330721855163574\n",
      "\t Loss: 16.537002563476562\n",
      "Outer loss: 51.25534439086914\n",
      "# 3: outer_epoch:[360] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.901718139648438\n",
      "\t Loss: 8.817367553710938\n",
      "\t Loss: 1.8611180782318115\n",
      "\t Loss: 6.507101058959961\n",
      "\t Loss: 35.9746208190918\n",
      "Outer loss: 57.46909713745117\n",
      "# 3: outer_epoch:[361] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.41571617126465\n",
      "\t Loss: 16.53944969177246\n",
      "\t Loss: 22.470991134643555\n",
      "\t Loss: 24.031862258911133\n",
      "\t Loss: 67.00286102294922\n",
      "Outer loss: 50.96849060058594\n",
      "# 3: outer_epoch:[362] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.122138023376465\n",
      "\t Loss: 1.7827222347259521\n",
      "\t Loss: 2.190404176712036\n",
      "\t Loss: 21.57362937927246\n",
      "\t Loss: 40.2943229675293\n",
      "Outer loss: 24.056642532348633\n",
      "# 3: outer_epoch:[363] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.63627815246582\n",
      "\t Loss: 2.4271163940429688\n",
      "\t Loss: 7.702019691467285\n",
      "\t Loss: 25.607698440551758\n",
      "\t Loss: 64.02581024169922\n",
      "Outer loss: 30.64813804626465\n",
      "# 3: outer_epoch:[364] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.97527027130127\n",
      "\t Loss: 6.084867477416992\n",
      "\t Loss: 4.128738880157471\n",
      "\t Loss: 14.451371192932129\n",
      "\t Loss: 22.265432357788086\n",
      "Outer loss: 35.44430923461914\n",
      "# 3: outer_epoch:[365] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.473583221435547\n",
      "\t Loss: 4.346060752868652\n",
      "\t Loss: 3.174131155014038\n",
      "\t Loss: 9.285502433776855\n",
      "\t Loss: 21.833433151245117\n",
      "Outer loss: 33.90787887573242\n",
      "# 3: outer_epoch:[366] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.150639533996582\n",
      "\t Loss: 10.863264083862305\n",
      "\t Loss: 22.04058837890625\n",
      "\t Loss: 60.625301361083984\n",
      "\t Loss: 76.58663940429688\n",
      "Outer loss: 47.621673583984375\n",
      "# 3: outer_epoch:[367] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.690773010253906\n",
      "\t Loss: 9.043425559997559\n",
      "\t Loss: 3.317812919616699\n",
      "\t Loss: 6.944864749908447\n",
      "\t Loss: 25.918928146362305\n",
      "Outer loss: 26.23841094970703\n",
      "# 3: outer_epoch:[368] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.164676666259766\n",
      "\t Loss: 6.683101177215576\n",
      "\t Loss: 2.2667412757873535\n",
      "\t Loss: 8.083873748779297\n",
      "\t Loss: 33.93415832519531\n",
      "Outer loss: 18.033889770507812\n",
      "# 3: outer_epoch:[369] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.289527893066406\n",
      "\t Loss: 20.885639190673828\n",
      "\t Loss: 14.082618713378906\n",
      "\t Loss: 29.60702896118164\n",
      "\t Loss: 64.9892807006836\n",
      "Outer loss: 37.31647491455078\n",
      "# 3: outer_epoch:[370] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.917040824890137\n",
      "\t Loss: 4.466858386993408\n",
      "\t Loss: 8.317825317382812\n",
      "\t Loss: 42.67689895629883\n",
      "\t Loss: 62.973533630371094\n",
      "Outer loss: 29.712448120117188\n",
      "# 3: outer_epoch:[371] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.650456428527832\n",
      "\t Loss: 5.3445258140563965\n",
      "\t Loss: 3.9219889640808105\n",
      "\t Loss: 19.55825424194336\n",
      "\t Loss: 56.25229263305664\n",
      "Outer loss: 10.199825286865234\n",
      "# 3: outer_epoch:[372] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.061697006225586\n",
      "\t Loss: 0.846269428730011\n",
      "\t Loss: 5.55174446105957\n",
      "\t Loss: 5.678009510040283\n",
      "\t Loss: 60.177398681640625\n",
      "Outer loss: 27.1994571685791\n",
      "# 3: outer_epoch:[373] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.716960906982422\n",
      "\t Loss: 6.158435821533203\n",
      "\t Loss: 22.662485122680664\n",
      "\t Loss: 42.50554656982422\n",
      "\t Loss: 30.855138778686523\n",
      "Outer loss: 55.41623306274414\n",
      "# 3: outer_epoch:[374] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.31394958496094\n",
      "\t Loss: 19.80095100402832\n",
      "\t Loss: 4.3182525634765625\n",
      "\t Loss: 2.3851070404052734\n",
      "\t Loss: 13.820356369018555\n",
      "Outer loss: 31.49875259399414\n",
      "# 3: outer_epoch:[375] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.427817344665527\n",
      "\t Loss: 12.458736419677734\n",
      "\t Loss: 0.24531953036785126\n",
      "\t Loss: 5.11366081237793\n",
      "\t Loss: 23.08163070678711\n",
      "Outer loss: 24.779563903808594\n",
      "# 3: outer_epoch:[376] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.029792785644531\n",
      "\t Loss: 4.672445774078369\n",
      "\t Loss: 5.534294128417969\n",
      "\t Loss: 13.201998710632324\n",
      "\t Loss: 25.763057708740234\n",
      "Outer loss: 38.124263763427734\n",
      "# 3: outer_epoch:[377] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.74105453491211\n",
      "\t Loss: 6.6265082359313965\n",
      "\t Loss: 13.903947830200195\n",
      "\t Loss: 26.9677677154541\n",
      "\t Loss: 48.59174346923828\n",
      "Outer loss: 22.78434181213379\n",
      "# 3: outer_epoch:[378] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.8392603397369385\n",
      "\t Loss: 1.7920809984207153\n",
      "\t Loss: 12.686506271362305\n",
      "\t Loss: 33.20985794067383\n",
      "\t Loss: 33.00978469848633\n",
      "Outer loss: 43.6367073059082\n",
      "# 3: outer_epoch:[379] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.42371940612793\n",
      "\t Loss: 2.606860637664795\n",
      "\t Loss: 5.341747283935547\n",
      "\t Loss: 26.203935623168945\n",
      "\t Loss: 58.74098587036133\n",
      "Outer loss: 28.185739517211914\n",
      "# 3: outer_epoch:[380] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.526994228363037\n",
      "\t Loss: 5.425008773803711\n",
      "\t Loss: 5.418574810028076\n",
      "\t Loss: 27.162128448486328\n",
      "\t Loss: 69.73408508300781\n",
      "Outer loss: 19.773815155029297\n",
      "# 3: outer_epoch:[381] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.835670471191406\n",
      "\t Loss: 4.121403217315674\n",
      "\t Loss: 1.9090319871902466\n",
      "\t Loss: 12.969888687133789\n",
      "\t Loss: 32.20341110229492\n",
      "Outer loss: 30.134044647216797\n",
      "# 3: outer_epoch:[382] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.29909324645996\n",
      "\t Loss: 2.7157187461853027\n",
      "\t Loss: 2.578695058822632\n",
      "\t Loss: 14.763269424438477\n",
      "\t Loss: 38.006568908691406\n",
      "Outer loss: 22.31284523010254\n",
      "# 3: outer_epoch:[383] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.722597122192383\n",
      "\t Loss: 2.0124309062957764\n",
      "\t Loss: 2.47235369682312\n",
      "\t Loss: 21.122478485107422\n",
      "\t Loss: 58.52716064453125\n",
      "Outer loss: 18.374576568603516\n",
      "# 3: outer_epoch:[384] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.763625144958496\n",
      "\t Loss: 3.9277758598327637\n",
      "\t Loss: 1.6793255805969238\n",
      "\t Loss: 10.647567749023438\n",
      "\t Loss: 50.67945098876953\n",
      "Outer loss: 14.347171783447266\n",
      "# 3: outer_epoch:[385] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.622049331665039\n",
      "\t Loss: 3.2933804988861084\n",
      "\t Loss: 5.327154159545898\n",
      "\t Loss: 16.295644760131836\n",
      "\t Loss: 40.372230529785156\n",
      "Outer loss: 26.9678955078125\n",
      "# 3: outer_epoch:[386] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.821527481079102\n",
      "\t Loss: 9.094326972961426\n",
      "\t Loss: 13.102383613586426\n",
      "\t Loss: 36.70979309082031\n",
      "\t Loss: 72.09523010253906\n",
      "Outer loss: 19.691621780395508\n",
      "# 3: outer_epoch:[387] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.306407928466797\n",
      "\t Loss: 4.237096309661865\n",
      "\t Loss: 1.1567211151123047\n",
      "\t Loss: 13.52457046508789\n",
      "\t Loss: 40.46467208862305\n",
      "Outer loss: 30.498409271240234\n",
      "# 3: outer_epoch:[388] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.676774978637695\n",
      "\t Loss: 4.21352481842041\n",
      "\t Loss: 4.061925888061523\n",
      "\t Loss: 6.2113847732543945\n",
      "\t Loss: 33.640132904052734\n",
      "Outer loss: 24.812095642089844\n",
      "# 3: outer_epoch:[389] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.881649017333984\n",
      "\t Loss: 15.79716968536377\n",
      "\t Loss: 2.085225820541382\n",
      "\t Loss: 14.909049034118652\n",
      "\t Loss: 35.761329650878906\n",
      "Outer loss: 19.756772994995117\n",
      "# 3: outer_epoch:[390] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.001964569091797\n",
      "\t Loss: 4.824098110198975\n",
      "\t Loss: 1.4463450908660889\n",
      "\t Loss: 5.94009256362915\n",
      "\t Loss: 19.517465591430664\n",
      "Outer loss: 29.109336853027344\n",
      "# 3: outer_epoch:[391] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.77340316772461\n",
      "\t Loss: 12.103854179382324\n",
      "\t Loss: 4.346505641937256\n",
      "\t Loss: 11.10221004486084\n",
      "\t Loss: 53.87583923339844\n",
      "Outer loss: 21.799930572509766\n",
      "# 3: outer_epoch:[392] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.491487503051758\n",
      "\t Loss: 5.727662086486816\n",
      "\t Loss: 1.6051479578018188\n",
      "\t Loss: 3.9072823524475098\n",
      "\t Loss: 48.85211181640625\n",
      "Outer loss: 17.833633422851562\n",
      "# 3: outer_epoch:[393] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.40635108947754\n",
      "\t Loss: 6.428036212921143\n",
      "\t Loss: 1.383159875869751\n",
      "\t Loss: 13.61518383026123\n",
      "\t Loss: 16.043075561523438\n",
      "Outer loss: 21.912019729614258\n",
      "# 3: outer_epoch:[394] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.240264892578125\n",
      "\t Loss: 3.0387845039367676\n",
      "\t Loss: 1.1274198293685913\n",
      "\t Loss: 13.526847839355469\n",
      "\t Loss: 62.3697395324707\n",
      "Outer loss: 28.627784729003906\n",
      "# 3: outer_epoch:[395] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.39002990722656\n",
      "\t Loss: 7.386566162109375\n",
      "\t Loss: 3.789577007293701\n",
      "\t Loss: 9.748826026916504\n",
      "\t Loss: 23.390756607055664\n",
      "Outer loss: 18.57227897644043\n",
      "# 3: outer_epoch:[396] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.132972717285156\n",
      "\t Loss: 14.888473510742188\n",
      "\t Loss: 1.4009861946105957\n",
      "\t Loss: 3.0012922286987305\n",
      "\t Loss: 30.21065330505371\n",
      "Outer loss: 27.79825210571289\n",
      "# 3: outer_epoch:[397] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.595552444458008\n",
      "\t Loss: 5.158389091491699\n",
      "\t Loss: 1.7468469142913818\n",
      "\t Loss: 7.567197799682617\n",
      "\t Loss: 29.713727951049805\n",
      "Outer loss: 46.02263641357422\n",
      "# 3: outer_epoch:[398] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.281909942626953\n",
      "\t Loss: 8.7786865234375\n",
      "\t Loss: 18.27593994140625\n",
      "\t Loss: 26.310705184936523\n",
      "\t Loss: 73.3265609741211\n",
      "Outer loss: 16.264495849609375\n",
      "# 3: outer_epoch:[399] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.27662467956543\n",
      "\t Loss: 4.764071941375732\n",
      "\t Loss: 3.65373158454895\n",
      "\t Loss: 27.610698699951172\n",
      "\t Loss: 43.460567474365234\n",
      "Outer loss: 21.161096572875977\n",
      "# 3: outer_epoch:[400] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.264766693115234\n",
      "\t Loss: 2.9037744998931885\n",
      "\t Loss: 5.004931926727295\n",
      "\t Loss: 20.230173110961914\n",
      "\t Loss: 19.048051834106445\n",
      "Outer loss: 34.15397262573242\n",
      "# 3: outer_epoch:[401] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.69343376159668\n",
      "\t Loss: 9.655141830444336\n",
      "\t Loss: 1.3407062292099\n",
      "\t Loss: 8.099026679992676\n",
      "\t Loss: 41.05555725097656\n",
      "Outer loss: 22.902273178100586\n",
      "# 3: outer_epoch:[402] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.864311218261719\n",
      "\t Loss: 0.5125280618667603\n",
      "\t Loss: 6.036722183227539\n",
      "\t Loss: 12.64128303527832\n",
      "\t Loss: 51.91499710083008\n",
      "Outer loss: 17.339441299438477\n",
      "# 3: outer_epoch:[403] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.734717845916748\n",
      "\t Loss: 2.1571240425109863\n",
      "\t Loss: 10.55778980255127\n",
      "\t Loss: 17.92841911315918\n",
      "\t Loss: 30.35767364501953\n",
      "Outer loss: 30.73734474182129\n",
      "# 3: outer_epoch:[404] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.577545166015625\n",
      "\t Loss: 7.3325066566467285\n",
      "\t Loss: 2.3747339248657227\n",
      "\t Loss: 17.5128116607666\n",
      "\t Loss: 42.78013610839844\n",
      "Outer loss: 15.240734100341797\n",
      "# 3: outer_epoch:[405] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.5304594039917\n",
      "\t Loss: 2.382222890853882\n",
      "\t Loss: 5.142906188964844\n",
      "\t Loss: 16.35626220703125\n",
      "\t Loss: 56.9208984375\n",
      "Outer loss: 15.499536514282227\n",
      "# 3: outer_epoch:[406] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.731792449951172\n",
      "\t Loss: 4.165770053863525\n",
      "\t Loss: 1.6681268215179443\n",
      "\t Loss: 11.505215644836426\n",
      "\t Loss: 63.277915954589844\n",
      "Outer loss: 20.272354125976562\n",
      "# 3: outer_epoch:[407] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.477901458740234\n",
      "\t Loss: 5.207484722137451\n",
      "\t Loss: 1.4548320770263672\n",
      "\t Loss: 25.375720977783203\n",
      "\t Loss: 29.233617782592773\n",
      "Outer loss: 18.018259048461914\n",
      "# 3: outer_epoch:[408] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.216312408447266\n",
      "\t Loss: 6.039031982421875\n",
      "\t Loss: 1.330487608909607\n",
      "\t Loss: 7.961857795715332\n",
      "\t Loss: 24.515544891357422\n",
      "Outer loss: 26.20087432861328\n",
      "# 3: outer_epoch:[409] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.984477996826172\n",
      "\t Loss: 13.24255084991455\n",
      "\t Loss: 1.3269747495651245\n",
      "\t Loss: 4.743921756744385\n",
      "\t Loss: 14.769968032836914\n",
      "Outer loss: 27.277687072753906\n",
      "# 3: outer_epoch:[410] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.65365982055664\n",
      "\t Loss: 7.673897743225098\n",
      "\t Loss: 2.4570322036743164\n",
      "\t Loss: 3.238485813140869\n",
      "\t Loss: 21.003215789794922\n",
      "Outer loss: 14.832854270935059\n",
      "# 3: outer_epoch:[411] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.191604614257812\n",
      "\t Loss: 9.728403091430664\n",
      "\t Loss: 0.6638012528419495\n",
      "\t Loss: 7.006535530090332\n",
      "\t Loss: 20.592395782470703\n",
      "Outer loss: 25.032161712646484\n",
      "# 3: outer_epoch:[412] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.28813934326172\n",
      "\t Loss: 7.269285202026367\n",
      "\t Loss: 1.2243821620941162\n",
      "\t Loss: 13.84270191192627\n",
      "\t Loss: 33.74616622924805\n",
      "Outer loss: 21.572668075561523\n",
      "# 3: outer_epoch:[413] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.12196922302246\n",
      "\t Loss: 3.353557586669922\n",
      "\t Loss: 7.344361782073975\n",
      "\t Loss: 15.838275909423828\n",
      "\t Loss: 36.318817138671875\n",
      "Outer loss: 27.320783615112305\n",
      "# 3: outer_epoch:[414] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.47594451904297\n",
      "\t Loss: 3.0705313682556152\n",
      "\t Loss: 2.5602073669433594\n",
      "\t Loss: 6.4511613845825195\n",
      "\t Loss: 26.772830963134766\n",
      "Outer loss: 29.94183921813965\n",
      "# 3: outer_epoch:[415] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.169757843017578\n",
      "\t Loss: 4.890663146972656\n",
      "\t Loss: 1.0231404304504395\n",
      "\t Loss: 8.818925857543945\n",
      "\t Loss: 42.55194854736328\n",
      "Outer loss: 15.648334503173828\n",
      "# 3: outer_epoch:[416] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.98314666748047\n",
      "\t Loss: 7.362056732177734\n",
      "\t Loss: 3.798680543899536\n",
      "\t Loss: 6.136960029602051\n",
      "\t Loss: 15.63147258758545\n",
      "Outer loss: 20.37584686279297\n",
      "# 3: outer_epoch:[417] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.981966018676758\n",
      "\t Loss: 7.828497409820557\n",
      "\t Loss: 3.4387307167053223\n",
      "\t Loss: 9.0201997756958\n",
      "\t Loss: 12.907358169555664\n",
      "Outer loss: 26.413368225097656\n",
      "# 3: outer_epoch:[418] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.092071533203125\n",
      "\t Loss: 9.562080383300781\n",
      "\t Loss: 0.5959776639938354\n",
      "\t Loss: 8.194196701049805\n",
      "\t Loss: 3.6265387535095215\n",
      "Outer loss: 51.28897476196289\n",
      "# 3: outer_epoch:[419] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.88208770751953\n",
      "\t Loss: 25.945789337158203\n",
      "\t Loss: 7.9769182205200195\n",
      "\t Loss: 0.06897667795419693\n",
      "\t Loss: 7.52556037902832\n",
      "Outer loss: 29.62403106689453\n",
      "# 3: outer_epoch:[420] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.827672958374023\n",
      "\t Loss: 8.701512336730957\n",
      "\t Loss: 0.14905774593353271\n",
      "\t Loss: 4.863367557525635\n",
      "\t Loss: 32.2017936706543\n",
      "Outer loss: 29.283124923706055\n",
      "# 3: outer_epoch:[421] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.735687255859375\n",
      "\t Loss: 12.46614933013916\n",
      "\t Loss: 4.580604076385498\n",
      "\t Loss: 5.786334037780762\n",
      "\t Loss: 13.180072784423828\n",
      "Outer loss: 39.96832275390625\n",
      "# 3: outer_epoch:[422] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.56987762451172\n",
      "\t Loss: 7.05989408493042\n",
      "\t Loss: 1.696522831916809\n",
      "\t Loss: 6.913107395172119\n",
      "\t Loss: 40.856231689453125\n",
      "Outer loss: 43.29985046386719\n",
      "# 3: outer_epoch:[423] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.082136154174805\n",
      "\t Loss: 2.0985093116760254\n",
      "\t Loss: 2.672001361846924\n",
      "\t Loss: 12.305689811706543\n",
      "\t Loss: 71.02011108398438\n",
      "Outer loss: 12.430868148803711\n",
      "# 3: outer_epoch:[424] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.159595489501953\n",
      "\t Loss: 1.754957675933838\n",
      "\t Loss: 5.634939193725586\n",
      "\t Loss: 11.860845565795898\n",
      "\t Loss: 45.064735412597656\n",
      "Outer loss: 12.943119049072266\n",
      "# 3: outer_epoch:[425] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.726917266845703\n",
      "\t Loss: 1.5607537031173706\n",
      "\t Loss: 2.8300397396087646\n",
      "\t Loss: 27.50886344909668\n",
      "\t Loss: 28.679716110229492\n",
      "Outer loss: 44.78327941894531\n",
      "# 3: outer_epoch:[426] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.170296669006348\n",
      "\t Loss: 4.897207736968994\n",
      "\t Loss: 12.748501777648926\n",
      "\t Loss: 38.34323501586914\n",
      "\t Loss: 71.76321411132812\n",
      "Outer loss: 21.818248748779297\n",
      "# 3: outer_epoch:[427] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.045154571533203\n",
      "\t Loss: 1.04194176197052\n",
      "\t Loss: 7.769039630889893\n",
      "\t Loss: 16.315452575683594\n",
      "\t Loss: 18.433639526367188\n",
      "Outer loss: 34.41460418701172\n",
      "# 3: outer_epoch:[428] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.29689598083496\n",
      "\t Loss: 11.64531135559082\n",
      "\t Loss: 2.404371500015259\n",
      "\t Loss: 16.952072143554688\n",
      "\t Loss: 18.13918685913086\n",
      "Outer loss: 27.864673614501953\n",
      "# 3: outer_epoch:[429] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.87763977050781\n",
      "\t Loss: 10.799190521240234\n",
      "\t Loss: 6.292140007019043\n",
      "\t Loss: 9.270036697387695\n",
      "\t Loss: 22.188304901123047\n",
      "Outer loss: 16.608747482299805\n",
      "# 3: outer_epoch:[430] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.845001220703125\n",
      "\t Loss: 5.80211067199707\n",
      "\t Loss: 1.2017320394515991\n",
      "\t Loss: 6.684103488922119\n",
      "\t Loss: 26.13700294494629\n",
      "Outer loss: 27.670974731445312\n",
      "# 3: outer_epoch:[431] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.87382507324219\n",
      "\t Loss: 11.757161140441895\n",
      "\t Loss: 16.543193817138672\n",
      "\t Loss: 9.34125804901123\n",
      "\t Loss: 28.480735778808594\n",
      "Outer loss: 31.45904541015625\n",
      "# 3: outer_epoch:[432] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 64.74258422851562\n",
      "\t Loss: 23.552534103393555\n",
      "\t Loss: 4.938916206359863\n",
      "\t Loss: 3.608859062194824\n",
      "\t Loss: 19.02113914489746\n",
      "Outer loss: 14.456914901733398\n",
      "# 3: outer_epoch:[433] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 52.938804626464844\n",
      "\t Loss: 16.808624267578125\n",
      "\t Loss: 3.914947271347046\n",
      "\t Loss: 2.257986068725586\n",
      "\t Loss: 21.07585334777832\n",
      "Outer loss: 8.890159606933594\n",
      "# 3: outer_epoch:[434] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 56.78187942504883\n",
      "\t Loss: 15.07138729095459\n",
      "\t Loss: 1.9436101913452148\n",
      "\t Loss: 2.4724128246307373\n",
      "\t Loss: 11.935972213745117\n",
      "Outer loss: 11.722396850585938\n",
      "# 3: outer_epoch:[435] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.4150276184082\n",
      "\t Loss: 27.668020248413086\n",
      "\t Loss: 2.0420684814453125\n",
      "\t Loss: 1.996773362159729\n",
      "\t Loss: 11.213334083557129\n",
      "Outer loss: 18.796131134033203\n",
      "# 3: outer_epoch:[436] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.07440948486328\n",
      "\t Loss: 10.621074676513672\n",
      "\t Loss: 1.993247389793396\n",
      "\t Loss: 2.1422905921936035\n",
      "\t Loss: 20.726715087890625\n",
      "Outer loss: 25.016508102416992\n",
      "# 3: outer_epoch:[437] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.653655529022217\n",
      "\t Loss: 2.901202917098999\n",
      "\t Loss: 2.8132193088531494\n",
      "\t Loss: 10.013298988342285\n",
      "\t Loss: 42.997108459472656\n",
      "Outer loss: 19.591642379760742\n",
      "# 3: outer_epoch:[438] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.261051177978516\n",
      "\t Loss: 1.189689040184021\n",
      "\t Loss: 5.468731880187988\n",
      "\t Loss: 19.976308822631836\n",
      "\t Loss: 58.80862045288086\n",
      "Outer loss: 13.513426780700684\n",
      "# 3: outer_epoch:[439] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.95780611038208\n",
      "\t Loss: 3.6729140281677246\n",
      "\t Loss: 6.9759931564331055\n",
      "\t Loss: 17.838045120239258\n",
      "\t Loss: 32.86231231689453\n",
      "Outer loss: 18.032093048095703\n",
      "# 3: outer_epoch:[440] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.77728271484375\n",
      "\t Loss: 2.9928650856018066\n",
      "\t Loss: 2.3774771690368652\n",
      "\t Loss: 9.013708114624023\n",
      "\t Loss: 46.427085876464844\n",
      "Outer loss: 21.584821701049805\n",
      "# 3: outer_epoch:[441] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.13277816772461\n",
      "\t Loss: 1.5831865072250366\n",
      "\t Loss: 3.226031541824341\n",
      "\t Loss: 16.376489639282227\n",
      "\t Loss: 28.10106658935547\n",
      "Outer loss: 17.367429733276367\n",
      "# 3: outer_epoch:[442] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.040343999862671\n",
      "\t Loss: 3.4123141765594482\n",
      "\t Loss: 1.6415859460830688\n",
      "\t Loss: 9.946969032287598\n",
      "\t Loss: 42.24918746948242\n",
      "Outer loss: 13.027833938598633\n",
      "# 3: outer_epoch:[443] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.697749137878418\n",
      "\t Loss: 0.8246802687644958\n",
      "\t Loss: 1.613319754600525\n",
      "\t Loss: 23.261642456054688\n",
      "\t Loss: 19.22559928894043\n",
      "Outer loss: 26.405088424682617\n",
      "# 3: outer_epoch:[444] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.644850730895996\n",
      "\t Loss: 2.622533082962036\n",
      "\t Loss: 3.061037302017212\n",
      "\t Loss: 9.140498161315918\n",
      "\t Loss: 56.11935043334961\n",
      "Outer loss: 12.909524917602539\n",
      "# 3: outer_epoch:[445] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.806804656982422\n",
      "\t Loss: 1.7720123529434204\n",
      "\t Loss: 4.820857524871826\n",
      "\t Loss: 15.574338912963867\n",
      "\t Loss: 31.101688385009766\n",
      "Outer loss: 29.893827438354492\n",
      "# 3: outer_epoch:[446] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.956635475158691\n",
      "\t Loss: 2.6701459884643555\n",
      "\t Loss: 18.15965461730957\n",
      "\t Loss: 30.922496795654297\n",
      "\t Loss: 63.480072021484375\n",
      "Outer loss: 14.648932456970215\n",
      "# 3: outer_epoch:[447] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.79067611694336\n",
      "\t Loss: 0.9099187850952148\n",
      "\t Loss: 2.3783681392669678\n",
      "\t Loss: 11.949796676635742\n",
      "\t Loss: 39.532073974609375\n",
      "Outer loss: 17.05171012878418\n",
      "# 3: outer_epoch:[448] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.488716125488281\n",
      "\t Loss: 3.477952241897583\n",
      "\t Loss: 4.35869026184082\n",
      "\t Loss: 21.240867614746094\n",
      "\t Loss: 56.071929931640625\n",
      "Outer loss: 15.449180603027344\n",
      "# 3: outer_epoch:[449] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.856784343719482\n",
      "\t Loss: 1.322174072265625\n",
      "\t Loss: 3.679880142211914\n",
      "\t Loss: 20.297561645507812\n",
      "\t Loss: 43.73664855957031\n",
      "Outer loss: 18.337677001953125\n",
      "# 3: outer_epoch:[450] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.836925506591797\n",
      "\t Loss: 3.6010570526123047\n",
      "\t Loss: 1.62110435962677\n",
      "\t Loss: 17.099945068359375\n",
      "\t Loss: 63.02458953857422\n",
      "Outer loss: 21.915735244750977\n",
      "# 3: outer_epoch:[451] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.776166915893555\n",
      "\t Loss: 6.82086181640625\n",
      "\t Loss: 2.0119431018829346\n",
      "\t Loss: 11.998412132263184\n",
      "\t Loss: 43.34444046020508\n",
      "Outer loss: 18.43041229248047\n",
      "# 3: outer_epoch:[452] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.294278144836426\n",
      "\t Loss: 2.0641112327575684\n",
      "\t Loss: 6.956515312194824\n",
      "\t Loss: 16.254928588867188\n",
      "\t Loss: 84.15301513671875\n",
      "Outer loss: 20.708641052246094\n",
      "# 3: outer_epoch:[453] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.604040145874023\n",
      "\t Loss: 5.497823238372803\n",
      "\t Loss: 2.022723913192749\n",
      "\t Loss: 13.968016624450684\n",
      "\t Loss: 12.387121200561523\n",
      "Outer loss: 36.448909759521484\n",
      "# 3: outer_epoch:[454] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.87657165527344\n",
      "\t Loss: 23.75222396850586\n",
      "\t Loss: 3.173358201980591\n",
      "\t Loss: 1.3188039064407349\n",
      "\t Loss: 15.580551147460938\n",
      "Outer loss: 19.307857513427734\n",
      "# 3: outer_epoch:[455] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.96452331542969\n",
      "\t Loss: 19.25750160217285\n",
      "\t Loss: 3.5145413875579834\n",
      "\t Loss: 0.745098352432251\n",
      "\t Loss: 13.80409049987793\n",
      "Outer loss: 19.980609893798828\n",
      "# 3: outer_epoch:[456] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.090587615966797\n",
      "\t Loss: 8.155627250671387\n",
      "\t Loss: 1.051186442375183\n",
      "\t Loss: 3.8564276695251465\n",
      "\t Loss: 8.14783000946045\n",
      "Outer loss: 17.095619201660156\n",
      "# 3: outer_epoch:[457] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.685760498046875\n",
      "\t Loss: 13.905046463012695\n",
      "\t Loss: 3.5735018253326416\n",
      "\t Loss: 2.7587499618530273\n",
      "\t Loss: 10.0093412399292\n",
      "Outer loss: 11.381181716918945\n",
      "# 3: outer_epoch:[458] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.243648529052734\n",
      "\t Loss: 20.453968048095703\n",
      "\t Loss: 5.78405237197876\n",
      "\t Loss: 1.1394808292388916\n",
      "\t Loss: 18.33063316345215\n",
      "Outer loss: 17.18010139465332\n",
      "# 3: outer_epoch:[459] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.01378631591797\n",
      "\t Loss: 16.221389770507812\n",
      "\t Loss: 1.9806146621704102\n",
      "\t Loss: 2.9719884395599365\n",
      "\t Loss: 13.167135238647461\n",
      "Outer loss: 14.945378303527832\n",
      "# 3: outer_epoch:[460] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.448604583740234\n",
      "\t Loss: 11.869075775146484\n",
      "\t Loss: 0.6060222387313843\n",
      "\t Loss: 6.448007583618164\n",
      "\t Loss: 12.804990768432617\n",
      "Outer loss: 19.199382781982422\n",
      "# 3: outer_epoch:[461] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.50083923339844\n",
      "\t Loss: 10.785120964050293\n",
      "\t Loss: 1.9721890687942505\n",
      "\t Loss: 5.863092422485352\n",
      "\t Loss: 21.880281448364258\n",
      "Outer loss: 12.837653160095215\n",
      "# 3: outer_epoch:[462] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.185304641723633\n",
      "\t Loss: 5.995833396911621\n",
      "\t Loss: 1.6025089025497437\n",
      "\t Loss: 7.731975078582764\n",
      "\t Loss: 19.586767196655273\n",
      "Outer loss: 13.998275756835938\n",
      "# 3: outer_epoch:[463] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.745258331298828\n",
      "\t Loss: 9.802876472473145\n",
      "\t Loss: 0.7793814539909363\n",
      "\t Loss: 3.5067050457000732\n",
      "\t Loss: 14.776796340942383\n",
      "Outer loss: 17.736167907714844\n",
      "# 3: outer_epoch:[464] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.2844123840332\n",
      "\t Loss: 10.548218727111816\n",
      "\t Loss: 0.9372501373291016\n",
      "\t Loss: 5.298654079437256\n",
      "\t Loss: 31.410118103027344\n",
      "Outer loss: 12.105382919311523\n",
      "# 3: outer_epoch:[465] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.21446418762207\n",
      "\t Loss: 11.7866849899292\n",
      "\t Loss: 0.0607469379901886\n",
      "\t Loss: 4.236819267272949\n",
      "\t Loss: 19.176456451416016\n",
      "Outer loss: 7.3045830726623535\n",
      "# 3: outer_epoch:[466] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.270980834960938\n",
      "\t Loss: 6.579070568084717\n",
      "\t Loss: 3.639301061630249\n",
      "\t Loss: 3.5135419368743896\n",
      "\t Loss: 13.232666969299316\n",
      "Outer loss: 20.404888153076172\n",
      "# 3: outer_epoch:[467] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.2183723449707\n",
      "\t Loss: 16.893529891967773\n",
      "\t Loss: 2.6507568359375\n",
      "\t Loss: 2.4012882709503174\n",
      "\t Loss: 21.70984649658203\n",
      "Outer loss: 19.092498779296875\n",
      "# 3: outer_epoch:[468] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.882331848144531\n",
      "\t Loss: 2.3422927856445312\n",
      "\t Loss: 3.5002224445343018\n",
      "\t Loss: 13.114359855651855\n",
      "\t Loss: 33.929805755615234\n",
      "Outer loss: 17.453292846679688\n",
      "# 3: outer_epoch:[469] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.65910339355469\n",
      "\t Loss: 52.89189147949219\n",
      "\t Loss: 26.637025833129883\n",
      "\t Loss: 82.21209716796875\n",
      "\t Loss: 125.88804626464844\n",
      "Outer loss: 71.2025375366211\n",
      "# 3: outer_epoch:[470] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 142.9246368408203\n",
      "\t Loss: 62.67338562011719\n",
      "\t Loss: 55.81782531738281\n",
      "\t Loss: 10.258270263671875\n",
      "\t Loss: 11.463973999023438\n",
      "Outer loss: 39.43375015258789\n",
      "# 3: outer_epoch:[471] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 65.4473876953125\n",
      "\t Loss: 38.008872985839844\n",
      "\t Loss: 5.775169372558594\n",
      "\t Loss: 1.7953720092773438\n",
      "\t Loss: 5.563572406768799\n",
      "Outer loss: 26.825153350830078\n",
      "# 3: outer_epoch:[472] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.31167984008789\n",
      "\t Loss: 10.219242095947266\n",
      "\t Loss: 1.1872217655181885\n",
      "\t Loss: 1.9136408567428589\n",
      "\t Loss: 16.86402130126953\n",
      "Outer loss: 30.397476196289062\n",
      "# 3: outer_epoch:[473] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.956968307495117\n",
      "\t Loss: 2.753512144088745\n",
      "\t Loss: 3.931973695755005\n",
      "\t Loss: 13.740738868713379\n",
      "\t Loss: 63.77376937866211\n",
      "Outer loss: 9.575366973876953\n",
      "# 3: outer_epoch:[474] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.103689193725586\n",
      "\t Loss: 0.49487239122390747\n",
      "\t Loss: 7.053339958190918\n",
      "\t Loss: 14.929652214050293\n",
      "\t Loss: 24.776100158691406\n",
      "Outer loss: 39.515541076660156\n",
      "# 3: outer_epoch:[475] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.314836502075195\n",
      "\t Loss: 9.404841423034668\n",
      "\t Loss: 2.881378412246704\n",
      "\t Loss: 5.800182342529297\n",
      "\t Loss: 31.491268157958984\n",
      "Outer loss: 16.629016876220703\n",
      "# 3: outer_epoch:[476] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.35564613342285\n",
      "\t Loss: 9.60446834564209\n",
      "\t Loss: 0.9215266704559326\n",
      "\t Loss: 4.321401596069336\n",
      "\t Loss: 31.149293899536133\n",
      "Outer loss: 9.259749412536621\n",
      "# 3: outer_epoch:[477] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.27948570251465\n",
      "\t Loss: 10.423965454101562\n",
      "\t Loss: 2.61401104927063\n",
      "\t Loss: 5.606241703033447\n",
      "\t Loss: 22.992631912231445\n",
      "Outer loss: 17.75520133972168\n",
      "# 3: outer_epoch:[478] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.950130462646484\n",
      "\t Loss: 7.079636096954346\n",
      "\t Loss: 0.23730787634849548\n",
      "\t Loss: 10.885466575622559\n",
      "\t Loss: 17.93551254272461\n",
      "Outer loss: 24.186431884765625\n",
      "# 3: outer_epoch:[479] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.431683540344238\n",
      "\t Loss: 5.391394138336182\n",
      "\t Loss: 2.1952242851257324\n",
      "\t Loss: 3.7237305641174316\n",
      "\t Loss: 24.52365493774414\n",
      "Outer loss: 23.86859893798828\n",
      "# 3: outer_epoch:[480] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.82795238494873\n",
      "\t Loss: 2.4505507946014404\n",
      "\t Loss: 5.477245330810547\n",
      "\t Loss: 20.649654388427734\n",
      "\t Loss: 41.503456115722656\n",
      "Outer loss: 13.125875473022461\n",
      "# 3: outer_epoch:[481] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 1.6928160190582275\n",
      "\t Loss: 1.9227814674377441\n",
      "\t Loss: 4.434638977050781\n",
      "\t Loss: 7.77398157119751\n",
      "\t Loss: 31.64310646057129\n",
      "Outer loss: 14.048233032226562\n",
      "# 3: outer_epoch:[482] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.805885314941406\n",
      "\t Loss: 0.6241869926452637\n",
      "\t Loss: 1.66764235496521\n",
      "\t Loss: 16.23784828186035\n",
      "\t Loss: 21.466617584228516\n",
      "Outer loss: 20.549104690551758\n",
      "# 3: outer_epoch:[483] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.703649520874023\n",
      "\t Loss: 4.554059982299805\n",
      "\t Loss: 2.6590116024017334\n",
      "\t Loss: 3.0185136795043945\n",
      "\t Loss: 23.740680694580078\n",
      "Outer loss: 27.920352935791016\n",
      "# 3: outer_epoch:[484] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.816635131835938\n",
      "\t Loss: 14.253170013427734\n",
      "\t Loss: 4.663430213928223\n",
      "\t Loss: 10.114713668823242\n",
      "\t Loss: 23.879789352416992\n",
      "Outer loss: 14.224308013916016\n",
      "# 3: outer_epoch:[485] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.95040512084961\n",
      "\t Loss: 6.093024253845215\n",
      "\t Loss: 2.5390138626098633\n",
      "\t Loss: 7.389963150024414\n",
      "\t Loss: 28.670129776000977\n",
      "Outer loss: 14.350768089294434\n",
      "# 3: outer_epoch:[486] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.799617767333984\n",
      "\t Loss: 1.9117287397384644\n",
      "\t Loss: 1.4732730388641357\n",
      "\t Loss: 12.420914649963379\n",
      "\t Loss: 37.858726501464844\n",
      "Outer loss: 13.155821800231934\n",
      "# 3: outer_epoch:[487] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.625350952148438\n",
      "\t Loss: 4.899532794952393\n",
      "\t Loss: 7.778257846832275\n",
      "\t Loss: 24.940752029418945\n",
      "\t Loss: 36.872318267822266\n",
      "Outer loss: 8.098780632019043\n",
      "# 3: outer_epoch:[488] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.490988731384277\n",
      "\t Loss: 1.8705904483795166\n",
      "\t Loss: 2.7989516258239746\n",
      "\t Loss: 26.18696403503418\n",
      "\t Loss: 59.427452087402344\n",
      "Outer loss: 14.615540504455566\n",
      "# 3: outer_epoch:[489] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.289146423339844\n",
      "\t Loss: 2.821176052093506\n",
      "\t Loss: 2.0872418880462646\n",
      "\t Loss: 8.528840065002441\n",
      "\t Loss: 35.416908264160156\n",
      "Outer loss: 12.254209518432617\n",
      "# 3: outer_epoch:[490] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.965446472167969\n",
      "\t Loss: 9.97517204284668\n",
      "\t Loss: 7.05748176574707\n",
      "\t Loss: 17.884824752807617\n",
      "\t Loss: 33.48939895629883\n",
      "Outer loss: 25.127819061279297\n",
      "# 3: outer_epoch:[491] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.941963195800781\n",
      "\t Loss: 4.968709468841553\n",
      "\t Loss: 8.935615539550781\n",
      "\t Loss: 28.644813537597656\n",
      "\t Loss: 46.286861419677734\n",
      "Outer loss: 20.543148040771484\n",
      "# 3: outer_epoch:[492] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.813867568969727\n",
      "\t Loss: 2.6308794021606445\n",
      "\t Loss: 14.7190523147583\n",
      "\t Loss: 25.221908569335938\n",
      "\t Loss: 68.6435317993164\n",
      "Outer loss: 18.48460578918457\n",
      "# 3: outer_epoch:[493] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.11383056640625\n",
      "\t Loss: 9.96592903137207\n",
      "\t Loss: 1.2565107345581055\n",
      "\t Loss: 3.4676685333251953\n",
      "\t Loss: 29.40595817565918\n",
      "Outer loss: 17.413837432861328\n",
      "# 3: outer_epoch:[494] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.1135311126709\n",
      "\t Loss: 3.8896522521972656\n",
      "\t Loss: 4.479268550872803\n",
      "\t Loss: 9.623531341552734\n",
      "\t Loss: 29.30643081665039\n",
      "Outer loss: 15.951751708984375\n",
      "# 3: outer_epoch:[495] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.90786361694336\n",
      "\t Loss: 5.050386905670166\n",
      "\t Loss: 0.20593999326229095\n",
      "\t Loss: 4.625057220458984\n",
      "\t Loss: 19.298416137695312\n",
      "Outer loss: 19.204811096191406\n",
      "# 3: outer_epoch:[496] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.6337890625\n",
      "\t Loss: 14.012354850769043\n",
      "\t Loss: 9.591230392456055\n",
      "\t Loss: 4.393057346343994\n",
      "\t Loss: 22.503618240356445\n",
      "Outer loss: 23.95029067993164\n",
      "# 3: outer_epoch:[497] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.691164016723633\n",
      "\t Loss: 0.9971418976783752\n",
      "\t Loss: 3.9727587699890137\n",
      "\t Loss: 15.592256546020508\n",
      "\t Loss: 47.71510314941406\n",
      "Outer loss: 13.338808059692383\n",
      "# 3: outer_epoch:[498] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.86775779724121\n",
      "\t Loss: 1.405538558959961\n",
      "\t Loss: 3.6168553829193115\n",
      "\t Loss: 12.088287353515625\n",
      "\t Loss: 36.75715637207031\n",
      "Outer loss: 16.477069854736328\n",
      "# 3: outer_epoch:[499] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.33669662475586\n",
      "\t Loss: 4.899606227874756\n",
      "\t Loss: 5.122220516204834\n",
      "\t Loss: 14.720609664916992\n",
      "\t Loss: 38.05541229248047\n",
      "Outer loss: 13.636009216308594\n",
      "# 3: outer_epoch:[500] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.612913131713867\n",
      "\t Loss: 2.4373276233673096\n",
      "\t Loss: 5.868748664855957\n",
      "\t Loss: 18.723003387451172\n",
      "\t Loss: 57.02558135986328\n",
      "Outer loss: 29.2496337890625\n",
      "# 3: outer_epoch:[501] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.444665908813477\n",
      "\t Loss: 4.490592956542969\n",
      "\t Loss: 8.579105377197266\n",
      "\t Loss: 32.89215087890625\n",
      "\t Loss: 44.2426643371582\n",
      "Outer loss: 27.578271865844727\n",
      "# 3: outer_epoch:[502] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.120235443115234\n",
      "\t Loss: 3.2224984169006348\n",
      "\t Loss: 12.654704093933105\n",
      "\t Loss: 34.13754653930664\n",
      "\t Loss: 77.42137908935547\n",
      "Outer loss: 12.173158645629883\n",
      "# 3: outer_epoch:[503] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.064246654510498\n",
      "\t Loss: 2.6514742374420166\n",
      "\t Loss: 13.35261058807373\n",
      "\t Loss: 13.326774597167969\n",
      "\t Loss: 80.0610122680664\n",
      "Outer loss: 23.182954788208008\n",
      "# 3: outer_epoch:[504] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.64398193359375\n",
      "\t Loss: 2.8550257682800293\n",
      "\t Loss: 3.1646194458007812\n",
      "\t Loss: 14.157889366149902\n",
      "\t Loss: 56.07969284057617\n",
      "Outer loss: 12.982248306274414\n",
      "# 3: outer_epoch:[505] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.887807846069336\n",
      "\t Loss: 2.2251553535461426\n",
      "\t Loss: 3.159329652786255\n",
      "\t Loss: 14.117417335510254\n",
      "\t Loss: 31.291011810302734\n",
      "Outer loss: 16.392826080322266\n",
      "# 3: outer_epoch:[506] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.448392868041992\n",
      "\t Loss: 3.282520294189453\n",
      "\t Loss: 1.8118540048599243\n",
      "\t Loss: 15.811908721923828\n",
      "\t Loss: 42.993038177490234\n",
      "Outer loss: 14.701455116271973\n",
      "# 3: outer_epoch:[507] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.315185546875\n",
      "\t Loss: 0.7360479831695557\n",
      "\t Loss: 6.2723894119262695\n",
      "\t Loss: 15.310419082641602\n",
      "\t Loss: 34.9351692199707\n",
      "Outer loss: 31.829320907592773\n",
      "# 3: outer_epoch:[508] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.838661193847656\n",
      "\t Loss: 5.312803268432617\n",
      "\t Loss: 7.411524772644043\n",
      "\t Loss: 18.509693145751953\n",
      "\t Loss: 53.00871658325195\n",
      "Outer loss: 23.666452407836914\n",
      "# 3: outer_epoch:[509] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.800582885742188\n",
      "\t Loss: 1.7795766592025757\n",
      "\t Loss: 5.698422908782959\n",
      "\t Loss: 12.209476470947266\n",
      "\t Loss: 61.91832733154297\n",
      "Outer loss: 18.40896224975586\n",
      "# 3: outer_epoch:[510] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.696815490722656\n",
      "\t Loss: 3.5907294750213623\n",
      "\t Loss: 3.9108474254608154\n",
      "\t Loss: 9.976646423339844\n",
      "\t Loss: 19.95344352722168\n",
      "Outer loss: 15.144998550415039\n",
      "# 3: outer_epoch:[511] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.3612060546875\n",
      "\t Loss: 8.732854843139648\n",
      "\t Loss: 0.8436790704727173\n",
      "\t Loss: 11.932406425476074\n",
      "\t Loss: 25.050025939941406\n",
      "Outer loss: 10.239245414733887\n",
      "# 3: outer_epoch:[512] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.894886016845703\n",
      "\t Loss: 7.388566493988037\n",
      "\t Loss: 1.8502494096755981\n",
      "\t Loss: 13.547990798950195\n",
      "\t Loss: 36.87114334106445\n",
      "Outer loss: 9.875509262084961\n",
      "# 3: outer_epoch:[513] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.933225631713867\n",
      "\t Loss: 3.4200258255004883\n",
      "\t Loss: 3.7450358867645264\n",
      "\t Loss: 17.310670852661133\n",
      "\t Loss: 67.90829467773438\n",
      "Outer loss: 12.1387300491333\n",
      "# 3: outer_epoch:[514] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.560239791870117\n",
      "\t Loss: 6.2841715812683105\n",
      "\t Loss: 0.7861698865890503\n",
      "\t Loss: 4.009740829467773\n",
      "\t Loss: 47.44287872314453\n",
      "Outer loss: 18.623085021972656\n",
      "# 3: outer_epoch:[515] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.481317520141602\n",
      "\t Loss: 4.9310407638549805\n",
      "\t Loss: 3.9794833660125732\n",
      "\t Loss: 15.45633602142334\n",
      "\t Loss: 59.02103805541992\n",
      "Outer loss: 10.754661560058594\n",
      "# 3: outer_epoch:[516] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.991960525512695\n",
      "\t Loss: 6.004673004150391\n",
      "\t Loss: 2.045602798461914\n",
      "\t Loss: 9.084988594055176\n",
      "\t Loss: 25.872825622558594\n",
      "Outer loss: 9.090126037597656\n",
      "# 3: outer_epoch:[517] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.751068115234375\n",
      "\t Loss: 5.045530319213867\n",
      "\t Loss: 0.9029889106750488\n",
      "\t Loss: 7.094661235809326\n",
      "\t Loss: 11.516087532043457\n",
      "Outer loss: 31.765823364257812\n",
      "# 3: outer_epoch:[518] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.689605712890625\n",
      "\t Loss: 16.55181884765625\n",
      "\t Loss: 0.7293230295181274\n",
      "\t Loss: 4.546664237976074\n",
      "\t Loss: 21.318830490112305\n",
      "Outer loss: 24.0465087890625\n",
      "# 3: outer_epoch:[519] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.018707275390625\n",
      "\t Loss: 4.297843933105469\n",
      "\t Loss: 1.5771511793136597\n",
      "\t Loss: 16.756114959716797\n",
      "\t Loss: 55.951866149902344\n",
      "Outer loss: 12.022953033447266\n",
      "# 3: outer_epoch:[520] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.599050521850586\n",
      "\t Loss: 4.133563995361328\n",
      "\t Loss: 1.6214854717254639\n",
      "\t Loss: 13.40738296508789\n",
      "\t Loss: 43.615203857421875\n",
      "Outer loss: 8.254395484924316\n",
      "# 3: outer_epoch:[521] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.118100166320801\n",
      "\t Loss: 3.0367355346679688\n",
      "\t Loss: 3.046523094177246\n",
      "\t Loss: 5.706430435180664\n",
      "\t Loss: 28.51236343383789\n",
      "Outer loss: 22.01318359375\n",
      "# 3: outer_epoch:[522] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.736133575439453\n",
      "\t Loss: 6.877915859222412\n",
      "\t Loss: 1.3127720355987549\n",
      "\t Loss: 8.377575874328613\n",
      "\t Loss: 8.942181587219238\n",
      "Outer loss: 22.057659149169922\n",
      "# 3: outer_epoch:[523] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.656158447265625\n",
      "\t Loss: 16.56027603149414\n",
      "\t Loss: 7.148556709289551\n",
      "\t Loss: 19.197175979614258\n",
      "\t Loss: 29.924222946166992\n",
      "Outer loss: 35.77143096923828\n",
      "# 3: outer_epoch:[524] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.17840003967285\n",
      "\t Loss: 3.6764473915100098\n",
      "\t Loss: 1.529472827911377\n",
      "\t Loss: 11.774662017822266\n",
      "\t Loss: 35.02368927001953\n",
      "Outer loss: 20.44687271118164\n",
      "# 3: outer_epoch:[525] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.892988204956055\n",
      "\t Loss: 3.038719415664673\n",
      "\t Loss: 9.570045471191406\n",
      "\t Loss: 18.477066040039062\n",
      "\t Loss: 37.08323669433594\n",
      "Outer loss: 24.73019790649414\n",
      "# 3: outer_epoch:[526] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.29174041748047\n",
      "\t Loss: 4.3086137771606445\n",
      "\t Loss: 0.2981363534927368\n",
      "\t Loss: 2.0034852027893066\n",
      "\t Loss: 23.07259750366211\n",
      "Outer loss: 34.09844970703125\n",
      "# 3: outer_epoch:[527] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.094030380249023\n",
      "\t Loss: 7.070196628570557\n",
      "\t Loss: 6.4804863929748535\n",
      "\t Loss: 21.97743797302246\n",
      "\t Loss: 46.33333969116211\n",
      "Outer loss: 16.337156295776367\n",
      "# 3: outer_epoch:[528] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.108650207519531\n",
      "\t Loss: 5.5293731689453125\n",
      "\t Loss: 0.24011726677417755\n",
      "\t Loss: 9.481614112854004\n",
      "\t Loss: 13.580546379089355\n",
      "Outer loss: 23.23997688293457\n",
      "# 3: outer_epoch:[529] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.140525817871094\n",
      "\t Loss: 12.406882286071777\n",
      "\t Loss: 4.283690929412842\n",
      "\t Loss: 3.195003032684326\n",
      "\t Loss: 9.699390411376953\n",
      "Outer loss: 11.760614395141602\n",
      "# 3: outer_epoch:[530] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 52.02408981323242\n",
      "\t Loss: 6.719002723693848\n",
      "\t Loss: 3.403036594390869\n",
      "\t Loss: 5.289914608001709\n",
      "\t Loss: 25.389129638671875\n",
      "Outer loss: 19.681461334228516\n",
      "# 3: outer_epoch:[531] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.45201587677002\n",
      "\t Loss: 6.782196998596191\n",
      "\t Loss: 3.341700315475464\n",
      "\t Loss: 14.991259574890137\n",
      "\t Loss: 27.85879898071289\n",
      "Outer loss: 10.52405834197998\n",
      "# 3: outer_epoch:[532] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.958311080932617\n",
      "\t Loss: 4.299012184143066\n",
      "\t Loss: 1.5003290176391602\n",
      "\t Loss: 14.241291999816895\n",
      "\t Loss: 15.191777229309082\n",
      "Outer loss: 16.375465393066406\n",
      "# 3: outer_epoch:[533] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.942001342773438\n",
      "\t Loss: 6.552746772766113\n",
      "\t Loss: 5.959429740905762\n",
      "\t Loss: 10.665570259094238\n",
      "\t Loss: 30.54955291748047\n",
      "Outer loss: 10.928962707519531\n",
      "# 3: outer_epoch:[534] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.780045509338379\n",
      "\t Loss: 4.212756156921387\n",
      "\t Loss: 1.2523376941680908\n",
      "\t Loss: 8.428948402404785\n",
      "\t Loss: 30.751747131347656\n",
      "Outer loss: 14.313942909240723\n",
      "# 3: outer_epoch:[535] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.496435165405273\n",
      "\t Loss: 1.6679017543792725\n",
      "\t Loss: 2.615719795227051\n",
      "\t Loss: 13.429441452026367\n",
      "\t Loss: 46.98905944824219\n",
      "Outer loss: 13.55398941040039\n",
      "# 3: outer_epoch:[536] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.598739624023438\n",
      "\t Loss: 1.8383021354675293\n",
      "\t Loss: 1.718658447265625\n",
      "\t Loss: 10.409319877624512\n",
      "\t Loss: 41.65679931640625\n",
      "Outer loss: 7.912921905517578\n",
      "# 3: outer_epoch:[537] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.199355125427246\n",
      "\t Loss: 2.8090243339538574\n",
      "\t Loss: 1.3736096620559692\n",
      "\t Loss: 12.914056777954102\n",
      "\t Loss: 16.572769165039062\n",
      "Outer loss: 28.578842163085938\n",
      "# 3: outer_epoch:[538] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.07889938354492\n",
      "\t Loss: 18.35215950012207\n",
      "\t Loss: 1.9034594297409058\n",
      "\t Loss: 3.7858219146728516\n",
      "\t Loss: 12.501106262207031\n",
      "Outer loss: 11.964563369750977\n",
      "# 3: outer_epoch:[539] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.9619026184082\n",
      "\t Loss: 10.527915954589844\n",
      "\t Loss: 1.4427605867385864\n",
      "\t Loss: 2.5448334217071533\n",
      "\t Loss: 18.209936141967773\n",
      "Outer loss: 8.562527656555176\n",
      "# 3: outer_epoch:[540] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 54.423492431640625\n",
      "\t Loss: 16.376928329467773\n",
      "\t Loss: 1.4253110885620117\n",
      "\t Loss: 2.0038158893585205\n",
      "\t Loss: 15.485248565673828\n",
      "Outer loss: 11.16658878326416\n",
      "# 3: outer_epoch:[541] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.984251976013184\n",
      "\t Loss: 13.837377548217773\n",
      "\t Loss: 4.330761432647705\n",
      "\t Loss: 3.1584420204162598\n",
      "\t Loss: 5.745771884918213\n",
      "Outer loss: 36.59547424316406\n",
      "# 3: outer_epoch:[542] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.235780715942383\n",
      "\t Loss: 3.056469440460205\n",
      "\t Loss: 3.171295166015625\n",
      "\t Loss: 20.091859817504883\n",
      "\t Loss: 54.31916046142578\n",
      "Outer loss: 16.71805191040039\n",
      "# 3: outer_epoch:[543] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.75251770019531\n",
      "\t Loss: 7.709957122802734\n",
      "\t Loss: 2.107731819152832\n",
      "\t Loss: 4.884129047393799\n",
      "\t Loss: 22.848102569580078\n",
      "Outer loss: 22.6763916015625\n",
      "# 3: outer_epoch:[544] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.144978523254395\n",
      "\t Loss: 3.7286429405212402\n",
      "\t Loss: 5.378175258636475\n",
      "\t Loss: 12.331406593322754\n",
      "\t Loss: 57.99283218383789\n",
      "Outer loss: 24.47012710571289\n",
      "# 3: outer_epoch:[545] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.04794692993164\n",
      "\t Loss: 2.3581910133361816\n",
      "\t Loss: 5.019326686859131\n",
      "\t Loss: 12.45616626739502\n",
      "\t Loss: 35.69293212890625\n",
      "Outer loss: 15.628460884094238\n",
      "# 3: outer_epoch:[546] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.434524536132812\n",
      "\t Loss: 3.5245096683502197\n",
      "\t Loss: 1.1136548519134521\n",
      "\t Loss: 8.886185646057129\n",
      "\t Loss: 17.157917022705078\n",
      "Outer loss: 19.21169662475586\n",
      "# 3: outer_epoch:[547] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.823898315429688\n",
      "\t Loss: 8.23083209991455\n",
      "\t Loss: 3.2376203536987305\n",
      "\t Loss: 10.850131034851074\n",
      "\t Loss: 25.30055809020996\n",
      "Outer loss: 11.239538192749023\n",
      "# 3: outer_epoch:[548] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.496880531311035\n",
      "\t Loss: 7.892289161682129\n",
      "\t Loss: 4.894689559936523\n",
      "\t Loss: 21.139236450195312\n",
      "\t Loss: 61.51693344116211\n",
      "Outer loss: 19.140995025634766\n",
      "# 3: outer_epoch:[549] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.931236267089844\n",
      "\t Loss: 1.0749198198318481\n",
      "\t Loss: 1.7626806497573853\n",
      "\t Loss: 12.715492248535156\n",
      "\t Loss: 40.85655212402344\n",
      "Outer loss: 9.813222885131836\n",
      "# 3: outer_epoch:[550] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.538877487182617\n",
      "\t Loss: 5.57018518447876\n",
      "\t Loss: 1.4861595630645752\n",
      "\t Loss: 17.293121337890625\n",
      "\t Loss: 15.217248916625977\n",
      "Outer loss: 21.39948844909668\n",
      "# 3: outer_epoch:[551] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.143651962280273\n",
      "\t Loss: 10.973294258117676\n",
      "\t Loss: 0.5405710339546204\n",
      "\t Loss: 6.279833793640137\n",
      "\t Loss: 26.551342010498047\n",
      "Outer loss: 5.114435195922852\n",
      "# 3: outer_epoch:[552] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.537416458129883\n",
      "\t Loss: 3.564012289047241\n",
      "\t Loss: 0.21324622631072998\n",
      "\t Loss: 9.378523826599121\n",
      "\t Loss: 24.554372787475586\n",
      "Outer loss: 6.058099269866943\n",
      "# 3: outer_epoch:[553] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.294462203979492\n",
      "\t Loss: 6.039546966552734\n",
      "\t Loss: 0.5158017873764038\n",
      "\t Loss: 6.722538471221924\n",
      "\t Loss: 26.40866470336914\n",
      "Outer loss: 12.200202941894531\n",
      "# 3: outer_epoch:[554] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.882487297058105\n",
      "\t Loss: 0.4191492199897766\n",
      "\t Loss: 4.243655681610107\n",
      "\t Loss: 8.633305549621582\n",
      "\t Loss: 48.34361267089844\n",
      "Outer loss: 29.572601318359375\n",
      "# 3: outer_epoch:[555] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.745784759521484\n",
      "\t Loss: 7.906146049499512\n",
      "\t Loss: 5.122835159301758\n",
      "\t Loss: 15.106463432312012\n",
      "\t Loss: 43.324501037597656\n",
      "Outer loss: 12.187607765197754\n",
      "# 3: outer_epoch:[556] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.107897758483887\n",
      "\t Loss: 3.0061235427856445\n",
      "\t Loss: 10.461920738220215\n",
      "\t Loss: 32.269412994384766\n",
      "\t Loss: 79.63765716552734\n",
      "Outer loss: 20.204544067382812\n",
      "# 3: outer_epoch:[557] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.988990783691406\n",
      "\t Loss: 4.928256988525391\n",
      "\t Loss: 3.7812509536743164\n",
      "\t Loss: 8.869572639465332\n",
      "\t Loss: 28.302772521972656\n",
      "Outer loss: 25.33327293395996\n",
      "# 3: outer_epoch:[558] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.636783599853516\n",
      "\t Loss: 13.597522735595703\n",
      "\t Loss: 0.9832950830459595\n",
      "\t Loss: 4.8356804847717285\n",
      "\t Loss: 17.51609992980957\n",
      "Outer loss: 17.160778045654297\n",
      "# 3: outer_epoch:[559] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.02119255065918\n",
      "\t Loss: 7.59151029586792\n",
      "\t Loss: 1.5526622533798218\n",
      "\t Loss: 11.177916526794434\n",
      "\t Loss: 30.417360305786133\n",
      "Outer loss: 11.12992000579834\n",
      "# 3: outer_epoch:[560] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.54941177368164\n",
      "\t Loss: 10.228991508483887\n",
      "\t Loss: 0.802057683467865\n",
      "\t Loss: 5.53371524810791\n",
      "\t Loss: 12.78936767578125\n",
      "Outer loss: 20.50579261779785\n",
      "# 3: outer_epoch:[561] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.51093673706055\n",
      "\t Loss: 9.344599723815918\n",
      "\t Loss: 0.2036435753107071\n",
      "\t Loss: 9.752147674560547\n",
      "\t Loss: 12.429762840270996\n",
      "Outer loss: 23.984569549560547\n",
      "# 3: outer_epoch:[562] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.283119201660156\n",
      "\t Loss: 0.7349144220352173\n",
      "\t Loss: 3.7348012924194336\n",
      "\t Loss: 17.528339385986328\n",
      "\t Loss: 45.91475296020508\n",
      "Outer loss: 9.666587829589844\n",
      "# 3: outer_epoch:[563] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.842309951782227\n",
      "\t Loss: 0.34918615221977234\n",
      "\t Loss: 6.724750995635986\n",
      "\t Loss: 28.395349502563477\n",
      "\t Loss: 46.319515228271484\n",
      "Outer loss: 14.441141128540039\n",
      "# 3: outer_epoch:[564] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.101656913757324\n",
      "\t Loss: 5.823659420013428\n",
      "\t Loss: 5.898404121398926\n",
      "\t Loss: 28.837081909179688\n",
      "\t Loss: 43.05510330200195\n",
      "Outer loss: 10.169300079345703\n",
      "# 3: outer_epoch:[565] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.611414432525635\n",
      "\t Loss: 3.5229220390319824\n",
      "\t Loss: 6.225078582763672\n",
      "\t Loss: 20.029743194580078\n",
      "\t Loss: 61.77597427368164\n",
      "Outer loss: 30.44759750366211\n",
      "# 3: outer_epoch:[566] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.092044830322266\n",
      "\t Loss: 26.843799591064453\n",
      "\t Loss: 12.823631286621094\n",
      "\t Loss: 13.408750534057617\n",
      "\t Loss: 30.420150756835938\n",
      "Outer loss: 20.56441879272461\n",
      "# 3: outer_epoch:[567] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.017728805541992\n",
      "\t Loss: 1.0854549407958984\n",
      "\t Loss: 7.169997692108154\n",
      "\t Loss: 24.412109375\n",
      "\t Loss: 32.14097213745117\n",
      "Outer loss: 17.12820816040039\n",
      "# 3: outer_epoch:[568] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.035274505615234\n",
      "\t Loss: 2.2506587505340576\n",
      "\t Loss: 2.7465333938598633\n",
      "\t Loss: 11.72413444519043\n",
      "\t Loss: 26.673236846923828\n",
      "Outer loss: 13.338930130004883\n",
      "# 3: outer_epoch:[569] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.17082595825195\n",
      "\t Loss: 3.315863609313965\n",
      "\t Loss: 0.3390546441078186\n",
      "\t Loss: 8.133781433105469\n",
      "\t Loss: 35.71156311035156\n",
      "Outer loss: 25.4775390625\n",
      "# 3: outer_epoch:[570] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.941280364990234\n",
      "\t Loss: 6.8507208824157715\n",
      "\t Loss: 3.254190444946289\n",
      "\t Loss: 17.206018447875977\n",
      "\t Loss: 38.7172737121582\n",
      "Outer loss: 14.140359878540039\n",
      "# 3: outer_epoch:[571] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.375993728637695\n",
      "\t Loss: 5.448076248168945\n",
      "\t Loss: 1.7400023937225342\n",
      "\t Loss: 8.364490509033203\n",
      "\t Loss: 22.641387939453125\n",
      "Outer loss: 19.109416961669922\n",
      "# 3: outer_epoch:[572] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.735824584960938\n",
      "\t Loss: 6.331740379333496\n",
      "\t Loss: 0.9257121086120605\n",
      "\t Loss: 4.235888957977295\n",
      "\t Loss: 11.8430814743042\n",
      "Outer loss: 17.174152374267578\n",
      "# 3: outer_epoch:[573] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.2746524810791\n",
      "\t Loss: 6.0675153732299805\n",
      "\t Loss: 0.7473281025886536\n",
      "\t Loss: 6.7344465255737305\n",
      "\t Loss: 28.586246490478516\n",
      "Outer loss: 4.8504252433776855\n",
      "# 3: outer_epoch:[574] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.715490341186523\n",
      "\t Loss: 7.348389148712158\n",
      "\t Loss: 0.17584745585918427\n",
      "\t Loss: 13.107786178588867\n",
      "\t Loss: 35.74095916748047\n",
      "Outer loss: 23.727685928344727\n",
      "# 3: outer_epoch:[575] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.463874816894531\n",
      "\t Loss: 6.618255138397217\n",
      "\t Loss: 6.749035358428955\n",
      "\t Loss: 10.541589736938477\n",
      "\t Loss: 37.81610870361328\n",
      "Outer loss: 16.41197967529297\n",
      "# 3: outer_epoch:[576] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.746514320373535\n",
      "\t Loss: 3.566577911376953\n",
      "\t Loss: 4.24887752532959\n",
      "\t Loss: 16.34456443786621\n",
      "\t Loss: 22.4397029876709\n",
      "Outer loss: 25.94793128967285\n",
      "# 3: outer_epoch:[577] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.824151039123535\n",
      "\t Loss: 0.867555558681488\n",
      "\t Loss: 11.930610656738281\n",
      "\t Loss: 40.25696563720703\n",
      "\t Loss: 59.60228729248047\n",
      "Outer loss: 9.522933959960938\n",
      "# 3: outer_epoch:[578] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.398385524749756\n",
      "\t Loss: 0.2897334694862366\n",
      "\t Loss: 6.627737045288086\n",
      "\t Loss: 25.75345802307129\n",
      "\t Loss: 50.21028137207031\n",
      "Outer loss: 8.22635269165039\n",
      "# 3: outer_epoch:[579] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.821654796600342\n",
      "\t Loss: 0.8164973258972168\n",
      "\t Loss: 8.578661918640137\n",
      "\t Loss: 30.076515197753906\n",
      "\t Loss: 61.126731872558594\n",
      "Outer loss: 7.426003932952881\n",
      "# 3: outer_epoch:[580] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.417271614074707\n",
      "\t Loss: 2.446878433227539\n",
      "\t Loss: 0.3553394079208374\n",
      "\t Loss: 28.09223175048828\n",
      "\t Loss: 28.946908950805664\n",
      "Outer loss: 19.422542572021484\n",
      "# 3: outer_epoch:[581] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.751197814941406\n",
      "\t Loss: 12.289176940917969\n",
      "\t Loss: 11.946012496948242\n",
      "\t Loss: 4.853418350219727\n",
      "\t Loss: 21.016244888305664\n",
      "Outer loss: 31.57498550415039\n",
      "# 3: outer_epoch:[582] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.788362979888916\n",
      "\t Loss: 1.6959831714630127\n",
      "\t Loss: 4.129575252532959\n",
      "\t Loss: 21.3077335357666\n",
      "\t Loss: 50.3648567199707\n",
      "Outer loss: 13.522573471069336\n",
      "# 3: outer_epoch:[583] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.21577262878418\n",
      "\t Loss: 3.1445319652557373\n",
      "\t Loss: 7.115664482116699\n",
      "\t Loss: 23.518314361572266\n",
      "\t Loss: 79.48738098144531\n",
      "Outer loss: 21.37018585205078\n",
      "# 3: outer_epoch:[584] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.372343063354492\n",
      "\t Loss: 4.480921745300293\n",
      "\t Loss: 5.119668960571289\n",
      "\t Loss: 14.863104820251465\n",
      "\t Loss: 50.13286590576172\n",
      "Outer loss: 8.692747116088867\n",
      "# 3: outer_epoch:[585] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.313077926635742\n",
      "\t Loss: 1.6544169187545776\n",
      "\t Loss: 3.634770393371582\n",
      "\t Loss: 13.315839767456055\n",
      "\t Loss: 51.85298538208008\n",
      "Outer loss: 17.737323760986328\n",
      "# 3: outer_epoch:[586] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.17591857910156\n",
      "\t Loss: 6.837183475494385\n",
      "\t Loss: 2.842219591140747\n",
      "\t Loss: 21.472917556762695\n",
      "\t Loss: 17.67989730834961\n",
      "Outer loss: 22.302858352661133\n",
      "# 3: outer_epoch:[587] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.502300262451172\n",
      "\t Loss: 3.6899027824401855\n",
      "\t Loss: 0.9039328098297119\n",
      "\t Loss: 8.871358871459961\n",
      "\t Loss: 18.902206420898438\n",
      "Outer loss: 10.676609992980957\n",
      "# 3: outer_epoch:[588] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.159488677978516\n",
      "\t Loss: 11.080238342285156\n",
      "\t Loss: 0.5760326385498047\n",
      "\t Loss: 5.858188629150391\n",
      "\t Loss: 31.470212936401367\n",
      "Outer loss: 5.414282321929932\n",
      "# 3: outer_epoch:[589] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.261502265930176\n",
      "\t Loss: 7.723180294036865\n",
      "\t Loss: 1.0144870281219482\n",
      "\t Loss: 13.373605728149414\n",
      "\t Loss: 38.04475784301758\n",
      "Outer loss: 18.789270401000977\n",
      "# 3: outer_epoch:[590] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.22764778137207\n",
      "\t Loss: 0.5416140556335449\n",
      "\t Loss: 2.1355080604553223\n",
      "\t Loss: 16.76319694519043\n",
      "\t Loss: 34.581111907958984\n",
      "Outer loss: 13.461446762084961\n",
      "# 3: outer_epoch:[591] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.145806312561035\n",
      "\t Loss: 0.665968120098114\n",
      "\t Loss: 2.1556196212768555\n",
      "\t Loss: 6.490293979644775\n",
      "\t Loss: 66.82736206054688\n",
      "Outer loss: 16.216468811035156\n",
      "# 3: outer_epoch:[592] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.893831253051758\n",
      "\t Loss: 3.8319339752197266\n",
      "\t Loss: 2.467806577682495\n",
      "\t Loss: 12.046107292175293\n",
      "\t Loss: 23.955148696899414\n",
      "Outer loss: 14.898155212402344\n",
      "# 3: outer_epoch:[593] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.418773651123047\n",
      "\t Loss: 6.718502044677734\n",
      "\t Loss: 0.5312108993530273\n",
      "\t Loss: 7.474547386169434\n",
      "\t Loss: 36.34387969970703\n",
      "Outer loss: 7.7035298347473145\n",
      "# 3: outer_epoch:[594] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.340301513671875\n",
      "\t Loss: 2.3504364490509033\n",
      "\t Loss: 0.4549574553966522\n",
      "\t Loss: 3.9612913131713867\n",
      "\t Loss: 27.16222381591797\n",
      "Outer loss: 22.105045318603516\n",
      "# 3: outer_epoch:[595] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.734784126281738\n",
      "\t Loss: 0.22084981203079224\n",
      "\t Loss: 5.383868217468262\n",
      "\t Loss: 27.130931854248047\n",
      "\t Loss: 60.29103469848633\n",
      "Outer loss: 20.819692611694336\n",
      "# 3: outer_epoch:[596] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.43824768066406\n",
      "\t Loss: 44.38475799560547\n",
      "\t Loss: 100.83804321289062\n",
      "\t Loss: 161.79122924804688\n",
      "\t Loss: 209.55517578125\n",
      "Outer loss: 98.4734878540039\n",
      "# 3: outer_epoch:[597] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.06150436401367\n",
      "\t Loss: 15.529906272888184\n",
      "\t Loss: 14.332757949829102\n",
      "\t Loss: 0.16749852895736694\n",
      "\t Loss: 8.055700302124023\n",
      "Outer loss: 31.695030212402344\n",
      "# 3: outer_epoch:[598] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 57.85863494873047\n",
      "\t Loss: 10.921134948730469\n",
      "\t Loss: 2.025136709213257\n",
      "\t Loss: 3.0230209827423096\n",
      "\t Loss: 25.555191040039062\n",
      "Outer loss: 11.824163436889648\n",
      "# 3: outer_epoch:[599] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.19941329956055\n",
      "\t Loss: 7.838577747344971\n",
      "\t Loss: 0.9765791296958923\n",
      "\t Loss: 4.77236270904541\n",
      "\t Loss: 21.206478118896484\n",
      "Outer loss: 10.048664093017578\n",
      "# 3: outer_epoch:[600] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.306236267089844\n",
      "\t Loss: 11.39264965057373\n",
      "\t Loss: 1.1759823560714722\n",
      "\t Loss: 6.144643306732178\n",
      "\t Loss: 8.828666687011719\n",
      "Outer loss: 9.202262878417969\n",
      "# 3: outer_epoch:[601] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.125358581542969\n",
      "\t Loss: 7.282011985778809\n",
      "\t Loss: 0.9578957557678223\n",
      "\t Loss: 7.116368293762207\n",
      "\t Loss: 19.840312957763672\n",
      "Outer loss: 30.30084800720215\n",
      "# 3: outer_epoch:[602] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.372644424438477\n",
      "\t Loss: 0.9745134711265564\n",
      "\t Loss: 4.826585292816162\n",
      "\t Loss: 9.28402328491211\n",
      "\t Loss: 40.517757415771484\n",
      "Outer loss: 16.43183135986328\n",
      "# 3: outer_epoch:[603] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.849123001098633\n",
      "\t Loss: 7.484508514404297\n",
      "\t Loss: 1.517113208770752\n",
      "\t Loss: 12.491183280944824\n",
      "\t Loss: 17.879074096679688\n",
      "Outer loss: 30.77037239074707\n",
      "# 3: outer_epoch:[604] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.73699951171875\n",
      "\t Loss: 9.546380043029785\n",
      "\t Loss: 2.4279799461364746\n",
      "\t Loss: 5.566648960113525\n",
      "\t Loss: 22.688955307006836\n",
      "Outer loss: 19.256237030029297\n",
      "# 3: outer_epoch:[605] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.65789031982422\n",
      "\t Loss: 11.590995788574219\n",
      "\t Loss: 4.500115394592285\n",
      "\t Loss: 4.137632369995117\n",
      "\t Loss: 14.424219131469727\n",
      "Outer loss: 15.967904090881348\n",
      "# 3: outer_epoch:[606] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.47728729248047\n",
      "\t Loss: 10.720755577087402\n",
      "\t Loss: 2.33129620552063\n",
      "\t Loss: 5.771096229553223\n",
      "\t Loss: 30.504375457763672\n",
      "Outer loss: 8.741185188293457\n",
      "# 3: outer_epoch:[607] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.10868453979492\n",
      "\t Loss: 8.022967338562012\n",
      "\t Loss: 0.5393536686897278\n",
      "\t Loss: 8.58230972290039\n",
      "\t Loss: 21.909120559692383\n",
      "Outer loss: 14.416604995727539\n",
      "# 3: outer_epoch:[608] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.394622802734375\n",
      "\t Loss: 7.076091289520264\n",
      "\t Loss: 1.8045490980148315\n",
      "\t Loss: 10.028082847595215\n",
      "\t Loss: 50.1087646484375\n",
      "Outer loss: 21.098827362060547\n",
      "# 3: outer_epoch:[609] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.14085865020752\n",
      "\t Loss: 1.7642704248428345\n",
      "\t Loss: 2.2761707305908203\n",
      "\t Loss: 10.90970516204834\n",
      "\t Loss: 42.20625686645508\n",
      "Outer loss: 16.812307357788086\n",
      "# 3: outer_epoch:[610] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.08009147644043\n",
      "\t Loss: 5.657858371734619\n",
      "\t Loss: 0.704439640045166\n",
      "\t Loss: 6.185903549194336\n",
      "\t Loss: 10.740165710449219\n",
      "Outer loss: 28.169654846191406\n",
      "# 3: outer_epoch:[611] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.003129959106445\n",
      "\t Loss: 13.68472671508789\n",
      "\t Loss: 1.8469420671463013\n",
      "\t Loss: 3.0850830078125\n",
      "\t Loss: 19.035072326660156\n",
      "Outer loss: 16.969722747802734\n",
      "# 3: outer_epoch:[612] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.96368980407715\n",
      "\t Loss: 2.949172258377075\n",
      "\t Loss: 2.918551445007324\n",
      "\t Loss: 16.470218658447266\n",
      "\t Loss: 39.27643966674805\n",
      "Outer loss: 6.564111709594727\n",
      "# 3: outer_epoch:[613] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.271970748901367\n",
      "\t Loss: 2.152224063873291\n",
      "\t Loss: 0.7219634056091309\n",
      "\t Loss: 16.28771209716797\n",
      "\t Loss: 21.802034378051758\n",
      "Outer loss: 16.969179153442383\n",
      "# 3: outer_epoch:[614] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.24678611755371\n",
      "\t Loss: 1.4073206186294556\n",
      "\t Loss: 1.668168544769287\n",
      "\t Loss: 7.227612495422363\n",
      "\t Loss: 23.13697624206543\n",
      "Outer loss: 11.800810813903809\n",
      "# 3: outer_epoch:[615] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.842421054840088\n",
      "\t Loss: 2.3020031452178955\n",
      "\t Loss: 4.198975086212158\n",
      "\t Loss: 10.995772361755371\n",
      "\t Loss: 33.988243103027344\n",
      "Outer loss: 15.170517921447754\n",
      "# 3: outer_epoch:[616] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.655701160430908\n",
      "\t Loss: 6.622215747833252\n",
      "\t Loss: 6.698448657989502\n",
      "\t Loss: 24.219419479370117\n",
      "\t Loss: 68.53001403808594\n",
      "Outer loss: 11.324518203735352\n",
      "# 3: outer_epoch:[617] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.344003677368164\n",
      "\t Loss: 4.447559833526611\n",
      "\t Loss: 7.235751152038574\n",
      "\t Loss: 35.95411682128906\n",
      "\t Loss: 39.75935363769531\n",
      "Outer loss: 13.938881874084473\n",
      "# 3: outer_epoch:[618] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.158601760864258\n",
      "\t Loss: 1.6086652278900146\n",
      "\t Loss: 2.9083456993103027\n",
      "\t Loss: 19.332923889160156\n",
      "\t Loss: 48.96810531616211\n",
      "Outer loss: 8.079707145690918\n",
      "# 3: outer_epoch:[619] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.55019760131836\n",
      "\t Loss: 5.513152599334717\n",
      "\t Loss: 3.2852635383605957\n",
      "\t Loss: 11.091297149658203\n",
      "\t Loss: 57.81048583984375\n",
      "Outer loss: 13.703412055969238\n",
      "# 3: outer_epoch:[620] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.836676597595215\n",
      "\t Loss: 0.4896063208580017\n",
      "\t Loss: 4.083179950714111\n",
      "\t Loss: 12.629904747009277\n",
      "\t Loss: 39.28627395629883\n",
      "Outer loss: 10.85837459564209\n",
      "# 3: outer_epoch:[621] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.729337692260742\n",
      "\t Loss: 4.342729568481445\n",
      "\t Loss: 3.0720067024230957\n",
      "\t Loss: 22.704639434814453\n",
      "\t Loss: 46.34971618652344\n",
      "Outer loss: 11.247395515441895\n",
      "# 3: outer_epoch:[622] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.171415328979492\n",
      "\t Loss: 5.1244988441467285\n",
      "\t Loss: 2.363426685333252\n",
      "\t Loss: 10.462594985961914\n",
      "\t Loss: 15.909996032714844\n",
      "Outer loss: 29.89556121826172\n",
      "# 3: outer_epoch:[623] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.776203155517578\n",
      "\t Loss: 4.521023273468018\n",
      "\t Loss: 1.5625436305999756\n",
      "\t Loss: 4.938717365264893\n",
      "\t Loss: 38.59091567993164\n",
      "Outer loss: 19.609676361083984\n",
      "# 3: outer_epoch:[624] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.763548851013184\n",
      "\t Loss: 1.9598183631896973\n",
      "\t Loss: 3.2002058029174805\n",
      "\t Loss: 16.4498291015625\n",
      "\t Loss: 54.661842346191406\n",
      "Outer loss: 14.385446548461914\n",
      "# 3: outer_epoch:[625] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.828338623046875\n",
      "\t Loss: 2.9673142433166504\n",
      "\t Loss: 1.8620374202728271\n",
      "\t Loss: 10.00763988494873\n",
      "\t Loss: 28.834230422973633\n",
      "Outer loss: 12.040486335754395\n",
      "# 3: outer_epoch:[626] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.060531616210938\n",
      "\t Loss: 7.9566802978515625\n",
      "\t Loss: 0.3080689609050751\n",
      "\t Loss: 9.607213973999023\n",
      "\t Loss: 26.653263092041016\n",
      "Outer loss: 15.387845993041992\n",
      "# 3: outer_epoch:[627] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.216327667236328\n",
      "\t Loss: 2.937113046646118\n",
      "\t Loss: 1.3103725910186768\n",
      "\t Loss: 10.185729026794434\n",
      "\t Loss: 44.07444381713867\n",
      "Outer loss: 11.171280860900879\n",
      "# 3: outer_epoch:[628] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.2572021484375\n",
      "\t Loss: 3.734170913696289\n",
      "\t Loss: 0.09458906203508377\n",
      "\t Loss: 3.804936170578003\n",
      "\t Loss: 26.86102867126465\n",
      "Outer loss: 13.209762573242188\n",
      "# 3: outer_epoch:[629] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.191003799438477\n",
      "\t Loss: 5.816903114318848\n",
      "\t Loss: 0.9430325627326965\n",
      "\t Loss: 8.78449821472168\n",
      "\t Loss: 15.892084121704102\n",
      "Outer loss: 13.195510864257812\n",
      "# 3: outer_epoch:[630] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.532981872558594\n",
      "\t Loss: 3.1106460094451904\n",
      "\t Loss: 1.0743508338928223\n",
      "\t Loss: 3.038811445236206\n",
      "\t Loss: 14.15757942199707\n",
      "Outer loss: 22.685558319091797\n",
      "# 3: outer_epoch:[631] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.724494934082031\n",
      "\t Loss: 0.215079203248024\n",
      "\t Loss: 6.281973361968994\n",
      "\t Loss: 22.918399810791016\n",
      "\t Loss: 48.5612678527832\n",
      "Outer loss: 16.45153045654297\n",
      "# 3: outer_epoch:[632] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.376689910888672\n",
      "\t Loss: 3.248016357421875\n",
      "\t Loss: 3.486149549484253\n",
      "\t Loss: 13.470148086547852\n",
      "\t Loss: 41.03606033325195\n",
      "Outer loss: 11.179036140441895\n",
      "# 3: outer_epoch:[633] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.881741523742676\n",
      "\t Loss: 6.549394607543945\n",
      "\t Loss: 17.295555114746094\n",
      "\t Loss: 28.051036834716797\n",
      "\t Loss: 92.66203308105469\n",
      "Outer loss: 11.809505462646484\n",
      "# 3: outer_epoch:[634] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.54227066040039\n",
      "\t Loss: 0.3194829821586609\n",
      "\t Loss: 2.944715976715088\n",
      "\t Loss: 9.786327362060547\n",
      "\t Loss: 48.979270935058594\n",
      "Outer loss: 17.08716583251953\n",
      "# 3: outer_epoch:[635] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.105405807495117\n",
      "\t Loss: 2.772040605545044\n",
      "\t Loss: 2.214684009552002\n",
      "\t Loss: 19.005443572998047\n",
      "\t Loss: 44.055633544921875\n",
      "Outer loss: 12.647492408752441\n",
      "# 3: outer_epoch:[636] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.169577598571777\n",
      "\t Loss: 0.7412289977073669\n",
      "\t Loss: 2.7046029567718506\n",
      "\t Loss: 15.210078239440918\n",
      "\t Loss: 58.70500183105469\n",
      "Outer loss: 12.200906753540039\n",
      "# 3: outer_epoch:[637] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.12542152404785\n",
      "\t Loss: 4.53797721862793\n",
      "\t Loss: 5.0990681648254395\n",
      "\t Loss: 13.080611228942871\n",
      "\t Loss: 51.66762161254883\n",
      "Outer loss: 7.926086902618408\n",
      "# 3: outer_epoch:[638] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.800804138183594\n",
      "\t Loss: 2.7810311317443848\n",
      "\t Loss: 1.2145788669586182\n",
      "\t Loss: 18.558673858642578\n",
      "\t Loss: 35.73321533203125\n",
      "Outer loss: 10.781132698059082\n",
      "# 3: outer_epoch:[639] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.753576278686523\n",
      "\t Loss: 11.336030960083008\n",
      "\t Loss: 0.15717989206314087\n",
      "\t Loss: 8.669174194335938\n",
      "\t Loss: 36.51467514038086\n",
      "Outer loss: 9.597983360290527\n",
      "# 3: outer_epoch:[640] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.175212860107422\n",
      "\t Loss: 4.550121307373047\n",
      "\t Loss: 1.0768017768859863\n",
      "\t Loss: 16.389713287353516\n",
      "\t Loss: 36.34880828857422\n",
      "Outer loss: 13.314932823181152\n",
      "# 3: outer_epoch:[641] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.939252853393555\n",
      "\t Loss: 8.097472190856934\n",
      "\t Loss: 0.7420070767402649\n",
      "\t Loss: 9.544240951538086\n",
      "\t Loss: 21.8399658203125\n",
      "Outer loss: 21.9327392578125\n",
      "# 3: outer_epoch:[642] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.88307762145996\n",
      "\t Loss: 9.86860466003418\n",
      "\t Loss: 2.2526302337646484\n",
      "\t Loss: 1.7556164264678955\n",
      "\t Loss: 23.212621688842773\n",
      "Outer loss: 10.213347434997559\n",
      "# 3: outer_epoch:[643] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.391801834106445\n",
      "\t Loss: 11.979596138000488\n",
      "\t Loss: 1.1486152410507202\n",
      "\t Loss: 7.6614179611206055\n",
      "\t Loss: 10.459678649902344\n",
      "Outer loss: 28.376054763793945\n",
      "# 3: outer_epoch:[644] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.571855545043945\n",
      "\t Loss: 4.167157173156738\n",
      "\t Loss: 0.6683240532875061\n",
      "\t Loss: 10.426523208618164\n",
      "\t Loss: 7.654184341430664\n",
      "Outer loss: 30.146440505981445\n",
      "# 3: outer_epoch:[645] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.51985168457031\n",
      "\t Loss: 4.452148914337158\n",
      "\t Loss: 1.2789639234542847\n",
      "\t Loss: 7.8675689697265625\n",
      "\t Loss: 30.111064910888672\n",
      "Outer loss: 10.26723575592041\n",
      "# 3: outer_epoch:[646] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.85003662109375\n",
      "\t Loss: 2.9486191272735596\n",
      "\t Loss: 2.960451602935791\n",
      "\t Loss: 18.245807647705078\n",
      "\t Loss: 22.98297691345215\n",
      "Outer loss: 17.259138107299805\n",
      "# 3: outer_epoch:[647] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.85732650756836\n",
      "\t Loss: 5.853853225708008\n",
      "\t Loss: 2.6089048385620117\n",
      "\t Loss: 7.140905857086182\n",
      "\t Loss: 18.751785278320312\n",
      "Outer loss: 14.243119239807129\n",
      "# 3: outer_epoch:[648] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.570045471191406\n",
      "\t Loss: 8.631497383117676\n",
      "\t Loss: 1.7485917806625366\n",
      "\t Loss: 7.192111015319824\n",
      "\t Loss: 5.918585777282715\n",
      "Outer loss: 21.589998245239258\n",
      "# 3: outer_epoch:[649] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.28274154663086\n",
      "\t Loss: 18.978071212768555\n",
      "\t Loss: 2.626734495162964\n",
      "\t Loss: 1.4328584671020508\n",
      "\t Loss: 6.821802139282227\n",
      "Outer loss: 9.30361557006836\n",
      "# 3: outer_epoch:[650] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 54.09651565551758\n",
      "\t Loss: 16.128490447998047\n",
      "\t Loss: 4.292262554168701\n",
      "\t Loss: 1.1291543245315552\n",
      "\t Loss: 10.755393981933594\n",
      "Outer loss: 12.163309097290039\n",
      "# 3: outer_epoch:[651] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.770694732666016\n",
      "\t Loss: 19.638275146484375\n",
      "\t Loss: 5.285300254821777\n",
      "\t Loss: 2.235755205154419\n",
      "\t Loss: 6.291374206542969\n",
      "Outer loss: 24.222795486450195\n",
      "# 3: outer_epoch:[652] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 51.91560745239258\n",
      "\t Loss: 13.241482734680176\n",
      "\t Loss: 6.0328874588012695\n",
      "\t Loss: 0.5281842350959778\n",
      "\t Loss: 8.877227783203125\n",
      "Outer loss: 15.812893867492676\n",
      "# 3: outer_epoch:[653] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.3099365234375\n",
      "\t Loss: 11.17819595336914\n",
      "\t Loss: 0.5607450008392334\n",
      "\t Loss: 1.8133538961410522\n",
      "\t Loss: 23.557201385498047\n",
      "Outer loss: 4.831801414489746\n",
      "# 3: outer_epoch:[654] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.429338455200195\n",
      "\t Loss: 18.954729080200195\n",
      "\t Loss: 1.9730030298233032\n",
      "\t Loss: 2.4576761722564697\n",
      "\t Loss: 15.504408836364746\n",
      "Outer loss: 6.0468950271606445\n",
      "# 3: outer_epoch:[655] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.60637092590332\n",
      "\t Loss: 9.946564674377441\n",
      "\t Loss: 1.778428554534912\n",
      "\t Loss: 2.314042806625366\n",
      "\t Loss: 16.63216209411621\n",
      "Outer loss: 9.227547645568848\n",
      "# 3: outer_epoch:[656] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.18906021118164\n",
      "\t Loss: 13.46947956085205\n",
      "\t Loss: 5.685013294219971\n",
      "\t Loss: 6.402044296264648\n",
      "\t Loss: 25.444730758666992\n",
      "Outer loss: 21.25461769104004\n",
      "# 3: outer_epoch:[657] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.501241683959961\n",
      "\t Loss: 2.975595235824585\n",
      "\t Loss: 6.958132743835449\n",
      "\t Loss: 16.492053985595703\n",
      "\t Loss: 53.66322708129883\n",
      "Outer loss: 16.15958023071289\n",
      "# 3: outer_epoch:[658] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.89824676513672\n",
      "\t Loss: 3.54500675201416\n",
      "\t Loss: 2.172125816345215\n",
      "\t Loss: 12.7804594039917\n",
      "\t Loss: 27.78960609436035\n",
      "Outer loss: 8.544235229492188\n",
      "# 3: outer_epoch:[659] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.21102523803711\n",
      "\t Loss: 5.108508586883545\n",
      "\t Loss: 2.0563042163848877\n",
      "\t Loss: 9.80720043182373\n",
      "\t Loss: 35.23403549194336\n",
      "Outer loss: 11.370617866516113\n",
      "# 3: outer_epoch:[660] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.262598991394043\n",
      "\t Loss: 0.5458964705467224\n",
      "\t Loss: 4.821690559387207\n",
      "\t Loss: 23.267183303833008\n",
      "\t Loss: 36.86559295654297\n",
      "Outer loss: 11.735599517822266\n",
      "# 3: outer_epoch:[661] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.64527702331543\n",
      "\t Loss: 4.145051956176758\n",
      "\t Loss: 2.767271041870117\n",
      "\t Loss: 15.224221229553223\n",
      "\t Loss: 39.87990951538086\n",
      "Outer loss: 15.798091888427734\n",
      "# 3: outer_epoch:[662] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.73509407043457\n",
      "\t Loss: 2.860591173171997\n",
      "\t Loss: 6.0840229988098145\n",
      "\t Loss: 30.129154205322266\n",
      "\t Loss: 13.624932289123535\n",
      "Outer loss: 43.90138626098633\n",
      "# 3: outer_epoch:[663] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 74.9168701171875\n",
      "\t Loss: 25.30748176574707\n",
      "\t Loss: 6.214262962341309\n",
      "\t Loss: 1.5039746761322021\n",
      "\t Loss: 8.910229682922363\n",
      "Outer loss: 20.215124130249023\n",
      "# 3: outer_epoch:[664] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 51.8980712890625\n",
      "\t Loss: 16.657005310058594\n",
      "\t Loss: 3.4801642894744873\n",
      "\t Loss: 1.8879691362380981\n",
      "\t Loss: 14.533063888549805\n",
      "Outer loss: 9.685768127441406\n",
      "# 3: outer_epoch:[665] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.24974250793457\n",
      "\t Loss: 16.63157081604004\n",
      "\t Loss: 4.825988292694092\n",
      "\t Loss: 4.865589618682861\n",
      "\t Loss: 19.73017120361328\n",
      "Outer loss: 12.585461616516113\n",
      "# 3: outer_epoch:[666] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.668903350830078\n",
      "\t Loss: 7.0720415115356445\n",
      "\t Loss: 1.0669230222702026\n",
      "\t Loss: 4.781734466552734\n",
      "\t Loss: 17.803367614746094\n",
      "Outer loss: 20.05513572692871\n",
      "# 3: outer_epoch:[667] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.64588165283203\n",
      "\t Loss: 4.526253700256348\n",
      "\t Loss: 0.4548387825489044\n",
      "\t Loss: 4.955427169799805\n",
      "\t Loss: 21.84471893310547\n",
      "Outer loss: 12.05142879486084\n",
      "# 3: outer_epoch:[668] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.739971160888672\n",
      "\t Loss: 4.856228351593018\n",
      "\t Loss: 1.369197964668274\n",
      "\t Loss: 11.147028923034668\n",
      "\t Loss: 49.291595458984375\n",
      "Outer loss: 14.314069747924805\n",
      "# 3: outer_epoch:[669] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.69003677368164\n",
      "\t Loss: 9.661141395568848\n",
      "\t Loss: 0.6753217577934265\n",
      "\t Loss: 7.491015434265137\n",
      "\t Loss: 12.692490577697754\n",
      "Outer loss: 11.673553466796875\n",
      "# 3: outer_epoch:[670] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.232383728027344\n",
      "\t Loss: 10.654077529907227\n",
      "\t Loss: 0.2964053452014923\n",
      "\t Loss: 6.424446105957031\n",
      "\t Loss: 20.3570556640625\n",
      "Outer loss: 13.932188987731934\n",
      "# 3: outer_epoch:[671] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.692076683044434\n",
      "\t Loss: 2.6833951473236084\n",
      "\t Loss: 1.563368797302246\n",
      "\t Loss: 17.173006057739258\n",
      "\t Loss: 42.870765686035156\n",
      "Outer loss: 13.073450088500977\n",
      "# 3: outer_epoch:[672] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.540491104125977\n",
      "\t Loss: 7.279697418212891\n",
      "\t Loss: 4.123668193817139\n",
      "\t Loss: 24.999887466430664\n",
      "\t Loss: 49.06792068481445\n",
      "Outer loss: 20.2122802734375\n",
      "# 3: outer_epoch:[673] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.687734603881836\n",
      "\t Loss: 3.7681944370269775\n",
      "\t Loss: 4.51578426361084\n",
      "\t Loss: 22.334077835083008\n",
      "\t Loss: 37.55112838745117\n",
      "Outer loss: 9.075858116149902\n",
      "# 3: outer_epoch:[674] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.544157981872559\n",
      "\t Loss: 2.075864553451538\n",
      "\t Loss: 1.2028337717056274\n",
      "\t Loss: 10.888324737548828\n",
      "\t Loss: 25.060819625854492\n",
      "Outer loss: 7.143509387969971\n",
      "# 3: outer_epoch:[675] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.368850708007812\n",
      "\t Loss: 3.132917642593384\n",
      "\t Loss: 0.6478590965270996\n",
      "\t Loss: 9.777767181396484\n",
      "\t Loss: 28.148731231689453\n",
      "Outer loss: 21.97527313232422\n",
      "# 3: outer_epoch:[676] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.448511123657227\n",
      "\t Loss: 2.1751599311828613\n",
      "\t Loss: 5.612590789794922\n",
      "\t Loss: 26.017568588256836\n",
      "\t Loss: 54.846290588378906\n",
      "Outer loss: 8.687164306640625\n",
      "# 3: outer_epoch:[677] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.203811645507812\n",
      "\t Loss: 1.4274871349334717\n",
      "\t Loss: 1.8455593585968018\n",
      "\t Loss: 10.912704467773438\n",
      "\t Loss: 40.229759216308594\n",
      "Outer loss: 26.45046043395996\n",
      "# 3: outer_epoch:[678] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 1.111940860748291\n",
      "\t Loss: 13.682718276977539\n",
      "\t Loss: 16.4251651763916\n",
      "\t Loss: 68.4854965209961\n",
      "\t Loss: 119.08000183105469\n",
      "Outer loss: 24.61183738708496\n",
      "# 3: outer_epoch:[679] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.97854232788086\n",
      "\t Loss: 19.01579475402832\n",
      "\t Loss: 12.445841789245605\n",
      "\t Loss: 5.060755729675293\n",
      "\t Loss: 12.004883766174316\n",
      "Outer loss: 31.110633850097656\n",
      "# 3: outer_epoch:[680] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.359745025634766\n",
      "\t Loss: 5.8601789474487305\n",
      "\t Loss: 1.010286569595337\n",
      "\t Loss: 8.547985076904297\n",
      "\t Loss: 43.77300262451172\n",
      "Outer loss: 8.40831184387207\n",
      "# 3: outer_epoch:[681] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.34576416015625\n",
      "\t Loss: 4.547885417938232\n",
      "\t Loss: 0.6261366009712219\n",
      "\t Loss: 5.877287864685059\n",
      "\t Loss: 30.061147689819336\n",
      "Outer loss: 7.509281635284424\n",
      "# 3: outer_epoch:[682] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.225502014160156\n",
      "\t Loss: 10.554301261901855\n",
      "\t Loss: 0.5957419872283936\n",
      "\t Loss: 4.935788154602051\n",
      "\t Loss: 23.656902313232422\n",
      "Outer loss: 7.275113105773926\n",
      "# 3: outer_epoch:[683] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.243656158447266\n",
      "\t Loss: 9.761881828308105\n",
      "\t Loss: 0.6968040466308594\n",
      "\t Loss: 6.206845283508301\n",
      "\t Loss: 24.203054428100586\n",
      "Outer loss: 8.139681816101074\n",
      "# 3: outer_epoch:[684] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.084768295288086\n",
      "\t Loss: 12.13348388671875\n",
      "\t Loss: 2.5087921619415283\n",
      "\t Loss: 5.682886600494385\n",
      "\t Loss: 24.304277420043945\n",
      "Outer loss: 11.439811706542969\n",
      "# 3: outer_epoch:[685] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.829959869384766\n",
      "\t Loss: 6.522154808044434\n",
      "\t Loss: 0.08492865413427353\n",
      "\t Loss: 5.383768558502197\n",
      "\t Loss: 25.859140396118164\n",
      "Outer loss: 9.749727249145508\n",
      "# 3: outer_epoch:[686] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.692230224609375\n",
      "\t Loss: 2.7815117835998535\n",
      "\t Loss: 0.9534891843795776\n",
      "\t Loss: 5.213705062866211\n",
      "\t Loss: 8.081478118896484\n",
      "Outer loss: 25.734601974487305\n",
      "# 3: outer_epoch:[687] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.373065948486328\n",
      "\t Loss: 9.006891250610352\n",
      "\t Loss: 1.221521019935608\n",
      "\t Loss: 8.3660306930542\n",
      "\t Loss: 25.413259506225586\n",
      "Outer loss: 14.226758003234863\n",
      "# 3: outer_epoch:[688] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.386878967285156\n",
      "\t Loss: 6.056960582733154\n",
      "\t Loss: 3.146961212158203\n",
      "\t Loss: 17.202661514282227\n",
      "\t Loss: 47.84401321411133\n",
      "Outer loss: 6.315760135650635\n",
      "# 3: outer_epoch:[689] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.615303993225098\n",
      "\t Loss: 2.0559282302856445\n",
      "\t Loss: 2.4295077323913574\n",
      "\t Loss: 6.005382537841797\n",
      "\t Loss: 25.894758224487305\n",
      "Outer loss: 18.096067428588867\n",
      "# 3: outer_epoch:[690] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.83209991455078\n",
      "\t Loss: 16.87425422668457\n",
      "\t Loss: 3.3048455715179443\n",
      "\t Loss: 2.140665054321289\n",
      "\t Loss: 17.66505241394043\n",
      "Outer loss: 12.215243339538574\n",
      "# 3: outer_epoch:[691] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.16364860534668\n",
      "\t Loss: 8.634340286254883\n",
      "\t Loss: 0.5310767292976379\n",
      "\t Loss: 6.153019905090332\n",
      "\t Loss: 15.511234283447266\n",
      "Outer loss: 8.796486854553223\n",
      "# 3: outer_epoch:[692] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.69559860229492\n",
      "\t Loss: 5.290079593658447\n",
      "\t Loss: 0.16301670670509338\n",
      "\t Loss: 4.030247688293457\n",
      "\t Loss: 19.407928466796875\n",
      "Outer loss: 16.528003692626953\n",
      "# 3: outer_epoch:[693] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.9077091217041\n",
      "\t Loss: 5.711541175842285\n",
      "\t Loss: 1.6373984813690186\n",
      "\t Loss: 7.5355448722839355\n",
      "\t Loss: 21.097309112548828\n",
      "Outer loss: 9.649136543273926\n",
      "# 3: outer_epoch:[694] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.231229782104492\n",
      "\t Loss: 6.667788982391357\n",
      "\t Loss: 1.2863556146621704\n",
      "\t Loss: 7.485015392303467\n",
      "\t Loss: 19.135663986206055\n",
      "Outer loss: 33.63029861450195\n",
      "# 3: outer_epoch:[695] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.421730041503906\n",
      "\t Loss: 1.637122631072998\n",
      "\t Loss: 15.756987571716309\n",
      "\t Loss: 22.159807205200195\n",
      "\t Loss: 101.15612030029297\n",
      "Outer loss: 20.651288986206055\n",
      "# 3: outer_epoch:[696] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.611547470092773\n",
      "\t Loss: 6.871181011199951\n",
      "\t Loss: 5.488773822784424\n",
      "\t Loss: 23.582754135131836\n",
      "\t Loss: 39.75053024291992\n",
      "Outer loss: 21.254667282104492\n",
      "# 3: outer_epoch:[697] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.965883255004883\n",
      "\t Loss: 6.8939290046691895\n",
      "\t Loss: 5.898451805114746\n",
      "\t Loss: 25.205364227294922\n",
      "\t Loss: 29.985843658447266\n",
      "Outer loss: 18.855178833007812\n",
      "# 3: outer_epoch:[698] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.36772346496582\n",
      "\t Loss: 2.78338360786438\n",
      "\t Loss: 3.4243857860565186\n",
      "\t Loss: 6.032209873199463\n",
      "\t Loss: 33.97913360595703\n",
      "Outer loss: 9.877805709838867\n",
      "# 3: outer_epoch:[699] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.899896621704102\n",
      "\t Loss: 4.565169334411621\n",
      "\t Loss: 0.4900265634059906\n",
      "\t Loss: 9.266471862792969\n",
      "\t Loss: 22.90955352783203\n",
      "Outer loss: 29.508359909057617\n",
      "# 3: outer_epoch:[700] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.693746328353882\n",
      "\t Loss: 1.3394062519073486\n",
      "\t Loss: 19.71195411682129\n",
      "\t Loss: 20.130809783935547\n",
      "\t Loss: 64.97784423828125\n",
      "Outer loss: 19.725698471069336\n",
      "# 3: outer_epoch:[701] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.795966625213623\n",
      "\t Loss: 1.0210890769958496\n",
      "\t Loss: 12.052529335021973\n",
      "\t Loss: 35.36066818237305\n",
      "\t Loss: 74.6343994140625\n",
      "Outer loss: 3.486025333404541\n",
      "# 3: outer_epoch:[702] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.702188014984131\n",
      "\t Loss: 1.6159186363220215\n",
      "\t Loss: 10.39743423461914\n",
      "\t Loss: 30.14663314819336\n",
      "\t Loss: 105.55094909667969\n",
      "Outer loss: 27.577404022216797\n",
      "# 3: outer_epoch:[703] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.26999282836914\n",
      "\t Loss: 3.1317663192749023\n",
      "\t Loss: 6.262085437774658\n",
      "\t Loss: 18.88457489013672\n",
      "\t Loss: 37.91927719116211\n",
      "Outer loss: 8.596016883850098\n",
      "# 3: outer_epoch:[704] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.721920013427734\n",
      "\t Loss: 8.608942985534668\n",
      "\t Loss: 3.4061641693115234\n",
      "\t Loss: 18.061521530151367\n",
      "\t Loss: 32.77909851074219\n",
      "Outer loss: 20.579763412475586\n",
      "# 3: outer_epoch:[705] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.39479923248291\n",
      "\t Loss: 1.800809621810913\n",
      "\t Loss: 8.3973388671875\n",
      "\t Loss: 26.5169677734375\n",
      "\t Loss: 49.935569763183594\n",
      "Outer loss: 32.90742874145508\n",
      "# 3: outer_epoch:[706] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.733592987060547\n",
      "\t Loss: 2.829245090484619\n",
      "\t Loss: 13.281269073486328\n",
      "\t Loss: 34.37558364868164\n",
      "\t Loss: 51.79624938964844\n",
      "Outer loss: 9.209997177124023\n",
      "# 3: outer_epoch:[707] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.665653228759766\n",
      "\t Loss: 2.3860371112823486\n",
      "\t Loss: 5.712758541107178\n",
      "\t Loss: 10.593164443969727\n",
      "\t Loss: 43.78705596923828\n",
      "Outer loss: 12.285327911376953\n",
      "# 3: outer_epoch:[708] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.864028930664062\n",
      "\t Loss: 7.333856582641602\n",
      "\t Loss: 7.61648416519165\n",
      "\t Loss: 11.63747501373291\n",
      "\t Loss: 43.75924301147461\n",
      "Outer loss: 13.702478408813477\n",
      "# 3: outer_epoch:[709] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.652503967285156\n",
      "\t Loss: 4.0445051193237305\n",
      "\t Loss: 1.8137390613555908\n",
      "\t Loss: 2.2703592777252197\n",
      "\t Loss: 26.4653263092041\n",
      "Outer loss: 15.34398078918457\n",
      "# 3: outer_epoch:[710] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.198158264160156\n",
      "\t Loss: 4.704479217529297\n",
      "\t Loss: 1.0677309036254883\n",
      "\t Loss: 2.8483896255493164\n",
      "\t Loss: 13.979972839355469\n",
      "Outer loss: 9.782822608947754\n",
      "# 3: outer_epoch:[711] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.120052337646484\n",
      "\t Loss: 8.946313858032227\n",
      "\t Loss: 0.8363034129142761\n",
      "\t Loss: 2.299224615097046\n",
      "\t Loss: 15.22026538848877\n",
      "Outer loss: 15.306296348571777\n",
      "# 3: outer_epoch:[712] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.05447769165039\n",
      "\t Loss: 14.856332778930664\n",
      "\t Loss: 3.5714077949523926\n",
      "\t Loss: 0.6400303840637207\n",
      "\t Loss: 13.057531356811523\n",
      "Outer loss: 4.641178131103516\n",
      "# 3: outer_epoch:[713] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 47.57331085205078\n",
      "\t Loss: 7.978450298309326\n",
      "\t Loss: 2.7170896530151367\n",
      "\t Loss: 2.4674150943756104\n",
      "\t Loss: 12.90381908416748\n",
      "Outer loss: 28.343013763427734\n",
      "# 3: outer_epoch:[714] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.600570678710938\n",
      "\t Loss: 8.380574226379395\n",
      "\t Loss: 6.34313440322876\n",
      "\t Loss: 13.08016300201416\n",
      "\t Loss: 30.741470336914062\n",
      "Outer loss: 8.164926528930664\n",
      "# 3: outer_epoch:[715] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.655447006225586\n",
      "\t Loss: 5.465989589691162\n",
      "\t Loss: 0.8183727264404297\n",
      "\t Loss: 12.863292694091797\n",
      "\t Loss: 22.111610412597656\n",
      "Outer loss: 16.802743911743164\n",
      "# 3: outer_epoch:[716] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.991910934448242\n",
      "\t Loss: 5.24670934677124\n",
      "\t Loss: 0.5788898468017578\n",
      "\t Loss: 9.01034164428711\n",
      "\t Loss: 34.43974304199219\n",
      "Outer loss: 10.118772506713867\n",
      "# 3: outer_epoch:[717] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.06499481201172\n",
      "\t Loss: 10.567556381225586\n",
      "\t Loss: 0.34080660343170166\n",
      "\t Loss: 8.246084213256836\n",
      "\t Loss: 32.714481353759766\n",
      "Outer loss: 21.65521240234375\n",
      "# 3: outer_epoch:[718] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.8981990814209\n",
      "\t Loss: 1.888655662536621\n",
      "\t Loss: 5.170158386230469\n",
      "\t Loss: 20.81930923461914\n",
      "\t Loss: 30.408191680908203\n",
      "Outer loss: 21.349327087402344\n",
      "# 3: outer_epoch:[719] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.32616424560547\n",
      "\t Loss: 1.930931568145752\n",
      "\t Loss: 2.1045892238616943\n",
      "\t Loss: 14.394826889038086\n",
      "\t Loss: 48.17283630371094\n",
      "Outer loss: 12.604924201965332\n",
      "# 3: outer_epoch:[720] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.335613250732422\n",
      "\t Loss: 1.8336085081100464\n",
      "\t Loss: 5.5267863273620605\n",
      "\t Loss: 24.195499420166016\n",
      "\t Loss: 51.720314025878906\n",
      "Outer loss: 17.276350021362305\n",
      "# 3: outer_epoch:[721] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.8153793811798096\n",
      "\t Loss: 0.7309964299201965\n",
      "\t Loss: 9.858134269714355\n",
      "\t Loss: 21.13173484802246\n",
      "\t Loss: 39.64049530029297\n",
      "Outer loss: 18.5811710357666\n",
      "# 3: outer_epoch:[722] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.244123458862305\n",
      "\t Loss: 3.9213340282440186\n",
      "\t Loss: 2.5166215896606445\n",
      "\t Loss: 14.608891487121582\n",
      "\t Loss: 46.22272491455078\n",
      "Outer loss: 14.73884105682373\n",
      "# 3: outer_epoch:[723] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.945388793945312\n",
      "\t Loss: 1.757860779762268\n",
      "\t Loss: 3.2917938232421875\n",
      "\t Loss: 14.344671249389648\n",
      "\t Loss: 54.933841705322266\n",
      "Outer loss: 9.056072235107422\n",
      "# 3: outer_epoch:[724] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.056875228881836\n",
      "\t Loss: 3.7624452114105225\n",
      "\t Loss: 1.5443625450134277\n",
      "\t Loss: 9.565286636352539\n",
      "\t Loss: 30.05780792236328\n",
      "Outer loss: 15.269010543823242\n",
      "# 3: outer_epoch:[725] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.504436492919922\n",
      "\t Loss: 4.288945198059082\n",
      "\t Loss: 0.426236629486084\n",
      "\t Loss: 16.41924476623535\n",
      "\t Loss: 43.994564056396484\n",
      "Outer loss: 10.538955688476562\n",
      "# 3: outer_epoch:[726] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.216482162475586\n",
      "\t Loss: 8.136988639831543\n",
      "\t Loss: 0.22003774344921112\n",
      "\t Loss: 7.070133209228516\n",
      "\t Loss: 11.683631896972656\n",
      "Outer loss: 17.54512596130371\n",
      "# 3: outer_epoch:[727] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.7065315246582\n",
      "\t Loss: 14.259075164794922\n",
      "\t Loss: 4.20517110824585\n",
      "\t Loss: 6.7676472663879395\n",
      "\t Loss: 33.59199523925781\n",
      "Outer loss: 9.805831909179688\n",
      "# 3: outer_epoch:[728] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.005334854125977\n",
      "\t Loss: 4.191712379455566\n",
      "\t Loss: 1.6598179340362549\n",
      "\t Loss: 5.726110935211182\n",
      "\t Loss: 15.250250816345215\n",
      "Outer loss: 18.623790740966797\n",
      "# 3: outer_epoch:[729] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.481037139892578\n",
      "\t Loss: 2.9821534156799316\n",
      "\t Loss: 1.4660301208496094\n",
      "\t Loss: 9.525818824768066\n",
      "\t Loss: 45.90754699707031\n",
      "Outer loss: 10.545600891113281\n",
      "# 3: outer_epoch:[730] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.523164749145508\n",
      "\t Loss: 1.3045854568481445\n",
      "\t Loss: 3.395430088043213\n",
      "\t Loss: 4.161927223205566\n",
      "\t Loss: 23.271381378173828\n",
      "Outer loss: 22.270614624023438\n",
      "# 3: outer_epoch:[731] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 55.92051696777344\n",
      "\t Loss: 16.519933700561523\n",
      "\t Loss: 4.387826919555664\n",
      "\t Loss: 3.273756980895996\n",
      "\t Loss: 11.428352355957031\n",
      "Outer loss: 11.81693172454834\n",
      "# 3: outer_epoch:[732] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.75822830200195\n",
      "\t Loss: 16.028196334838867\n",
      "\t Loss: 3.488990306854248\n",
      "\t Loss: 1.2394518852233887\n",
      "\t Loss: 11.092428207397461\n",
      "Outer loss: 12.158075332641602\n",
      "# 3: outer_epoch:[733] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 48.598121643066406\n",
      "\t Loss: 16.038366317749023\n",
      "\t Loss: 10.22315502166748\n",
      "\t Loss: 15.814253807067871\n",
      "\t Loss: 21.07196044921875\n",
      "Outer loss: 22.551889419555664\n",
      "# 3: outer_epoch:[734] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 49.61002731323242\n",
      "\t Loss: 15.784173011779785\n",
      "\t Loss: 2.881542444229126\n",
      "\t Loss: 4.705989837646484\n",
      "\t Loss: 18.618959426879883\n",
      "Outer loss: 16.132556915283203\n",
      "# 3: outer_epoch:[735] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.93819808959961\n",
      "\t Loss: 9.647201538085938\n",
      "\t Loss: 2.2405495643615723\n",
      "\t Loss: 1.2039319276809692\n",
      "\t Loss: 13.934967994689941\n",
      "Outer loss: 7.766763687133789\n",
      "# 3: outer_epoch:[736] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.962981224060059\n",
      "\t Loss: 4.699913024902344\n",
      "\t Loss: 0.4742072820663452\n",
      "\t Loss: 12.793575286865234\n",
      "\t Loss: 36.11225128173828\n",
      "Outer loss: 17.673625946044922\n",
      "# 3: outer_epoch:[737] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.109046936035156\n",
      "\t Loss: 4.082479000091553\n",
      "\t Loss: 1.269455909729004\n",
      "\t Loss: 8.787993431091309\n",
      "\t Loss: 44.77543640136719\n",
      "Outer loss: 12.957740783691406\n",
      "# 3: outer_epoch:[738] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.726781845092773\n",
      "\t Loss: 7.883571147918701\n",
      "\t Loss: 1.1665523052215576\n",
      "\t Loss: 6.511683940887451\n",
      "\t Loss: 9.897774696350098\n",
      "Outer loss: 15.059618949890137\n",
      "# 3: outer_epoch:[739] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.809268951416016\n",
      "\t Loss: 21.175600051879883\n",
      "\t Loss: 7.4789509773254395\n",
      "\t Loss: 0.9095739722251892\n",
      "\t Loss: 7.535770893096924\n",
      "Outer loss: 16.72757339477539\n",
      "# 3: outer_epoch:[740] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.624059677124023\n",
      "\t Loss: 12.147316932678223\n",
      "\t Loss: 0.39035356044769287\n",
      "\t Loss: 7.779910564422607\n",
      "\t Loss: 21.97257423400879\n",
      "Outer loss: 8.723593711853027\n",
      "# 3: outer_epoch:[741] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.696529388427734\n",
      "\t Loss: 4.8050537109375\n",
      "\t Loss: 0.11216811090707779\n",
      "\t Loss: 2.796880006790161\n",
      "\t Loss: 36.01050567626953\n",
      "Outer loss: 11.552671432495117\n",
      "# 3: outer_epoch:[742] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.734375\n",
      "\t Loss: 8.54525375366211\n",
      "\t Loss: 1.517685890197754\n",
      "\t Loss: 2.5903730392456055\n",
      "\t Loss: 20.768400192260742\n",
      "Outer loss: 16.171653747558594\n",
      "# 3: outer_epoch:[743] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.00883674621582\n",
      "\t Loss: 9.323737144470215\n",
      "\t Loss: 1.2094061374664307\n",
      "\t Loss: 4.435696601867676\n",
      "\t Loss: 7.61077880859375\n",
      "Outer loss: 12.959721565246582\n",
      "# 3: outer_epoch:[744] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.50373649597168\n",
      "\t Loss: 9.187768936157227\n",
      "\t Loss: 1.423719048500061\n",
      "\t Loss: 5.801095485687256\n",
      "\t Loss: 25.4278507232666\n",
      "Outer loss: 7.590331077575684\n",
      "# 3: outer_epoch:[745] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.29792022705078\n",
      "\t Loss: 6.3758544921875\n",
      "\t Loss: 0.4848426580429077\n",
      "\t Loss: 4.677839279174805\n",
      "\t Loss: 40.79180908203125\n",
      "Outer loss: 11.433349609375\n",
      "# 3: outer_epoch:[746] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.706087112426758\n",
      "\t Loss: 7.3053436279296875\n",
      "\t Loss: 0.8492189645767212\n",
      "\t Loss: 2.417818307876587\n",
      "\t Loss: 16.592082977294922\n",
      "Outer loss: 10.130918502807617\n",
      "# 3: outer_epoch:[747] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.097373962402344\n",
      "\t Loss: 4.015320777893066\n",
      "\t Loss: 2.3271803855895996\n",
      "\t Loss: 16.44489097595215\n",
      "\t Loss: 50.179466247558594\n",
      "Outer loss: 10.110050201416016\n",
      "# 3: outer_epoch:[748] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.719297409057617\n",
      "\t Loss: 9.938361167907715\n",
      "\t Loss: 0.10135950893163681\n",
      "\t Loss: 8.391975402832031\n",
      "\t Loss: 37.40787887573242\n",
      "Outer loss: 12.57381534576416\n",
      "# 3: outer_epoch:[749] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 32.59901428222656\n",
      "\t Loss: 7.562207221984863\n",
      "\t Loss: 0.8078436851501465\n",
      "\t Loss: 8.653181076049805\n",
      "\t Loss: 16.75409698486328\n",
      "Outer loss: 10.95661735534668\n",
      "# 3: outer_epoch:[750] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.77093505859375\n",
      "\t Loss: 15.517020225524902\n",
      "\t Loss: 0.744661808013916\n",
      "\t Loss: 4.8900628089904785\n",
      "\t Loss: 26.458707809448242\n",
      "Outer loss: 9.724599838256836\n",
      "# 3: outer_epoch:[751] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.042264938354492\n",
      "\t Loss: 10.325389862060547\n",
      "\t Loss: 1.656856656074524\n",
      "\t Loss: 5.373283863067627\n",
      "\t Loss: 16.505626678466797\n",
      "Outer loss: 15.708473205566406\n",
      "# 3: outer_epoch:[752] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.390979766845703\n",
      "\t Loss: 8.75146484375\n",
      "\t Loss: 1.6490941047668457\n",
      "\t Loss: 7.480485439300537\n",
      "\t Loss: 24.094783782958984\n",
      "Outer loss: 9.666570663452148\n",
      "# 3: outer_epoch:[753] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.658576011657715\n",
      "\t Loss: 5.677392959594727\n",
      "\t Loss: 0.3148989677429199\n",
      "\t Loss: 8.498858451843262\n",
      "\t Loss: 23.743606567382812\n",
      "Outer loss: 19.56462860107422\n",
      "# 3: outer_epoch:[754] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.337671756744385\n",
      "\t Loss: 0.14051291346549988\n",
      "\t Loss: 6.796791076660156\n",
      "\t Loss: 25.437763214111328\n",
      "\t Loss: 45.96306610107422\n",
      "Outer loss: 15.334001541137695\n",
      "# 3: outer_epoch:[755] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.52008056640625\n",
      "\t Loss: 1.081548810005188\n",
      "\t Loss: 11.137187957763672\n",
      "\t Loss: 24.40978240966797\n",
      "\t Loss: 31.256811141967773\n",
      "Outer loss: 26.48595428466797\n",
      "# 3: outer_epoch:[756] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.45378494262695\n",
      "\t Loss: 6.51485538482666\n",
      "\t Loss: 2.5638036727905273\n",
      "\t Loss: 8.946556091308594\n",
      "\t Loss: 33.896209716796875\n",
      "Outer loss: 23.763019561767578\n",
      "# 3: outer_epoch:[757] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.114456176757812\n",
      "\t Loss: 7.087708473205566\n",
      "\t Loss: 3.7948362827301025\n",
      "\t Loss: 12.184080123901367\n",
      "\t Loss: 21.067411422729492\n",
      "Outer loss: 40.398189544677734\n",
      "# 3: outer_epoch:[758] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.104469299316406\n",
      "\t Loss: 15.991668701171875\n",
      "\t Loss: 0.8183435797691345\n",
      "\t Loss: 3.889967918395996\n",
      "\t Loss: 11.774502754211426\n",
      "Outer loss: 17.251707077026367\n",
      "# 3: outer_epoch:[759] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.3198356628418\n",
      "\t Loss: 6.14891242980957\n",
      "\t Loss: 0.35363492369651794\n",
      "\t Loss: 13.386655807495117\n",
      "\t Loss: 22.808732986450195\n",
      "Outer loss: 16.558868408203125\n",
      "# 3: outer_epoch:[760] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.682672500610352\n",
      "\t Loss: 3.6854710578918457\n",
      "\t Loss: 2.7729740142822266\n",
      "\t Loss: 7.763192653656006\n",
      "\t Loss: 29.546201705932617\n",
      "Outer loss: 13.906449317932129\n",
      "# 3: outer_epoch:[761] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.541105270385742\n",
      "\t Loss: 0.2531057298183441\n",
      "\t Loss: 7.01836633682251\n",
      "\t Loss: 20.70272445678711\n",
      "\t Loss: 40.34073257446289\n",
      "Outer loss: 13.100540161132812\n",
      "# 3: outer_epoch:[762] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.0361857414245605\n",
      "\t Loss: 2.292844533920288\n",
      "\t Loss: 1.6552808284759521\n",
      "\t Loss: 12.164137840270996\n",
      "\t Loss: 62.017578125\n",
      "Outer loss: 12.197223663330078\n",
      "# 3: outer_epoch:[763] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.913147926330566\n",
      "\t Loss: 2.9992563724517822\n",
      "\t Loss: 3.430103063583374\n",
      "\t Loss: 18.238492965698242\n",
      "\t Loss: 36.665794372558594\n",
      "Outer loss: 6.340923309326172\n",
      "# 3: outer_epoch:[764] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.832528114318848\n",
      "\t Loss: 4.490380764007568\n",
      "\t Loss: 3.6369595527648926\n",
      "\t Loss: 18.815654754638672\n",
      "\t Loss: 32.32716369628906\n",
      "Outer loss: 4.861167907714844\n",
      "# 3: outer_epoch:[765] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.753071784973145\n",
      "\t Loss: 7.908113479614258\n",
      "\t Loss: 2.8443214893341064\n",
      "\t Loss: 15.173336029052734\n",
      "\t Loss: 45.10877227783203\n",
      "Outer loss: 9.349098205566406\n",
      "# 3: outer_epoch:[766] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.17709732055664\n",
      "\t Loss: 0.9617249369621277\n",
      "\t Loss: 2.0013480186462402\n",
      "\t Loss: 12.817313194274902\n",
      "\t Loss: 34.7153434753418\n",
      "Outer loss: 11.503877639770508\n",
      "# 3: outer_epoch:[767] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.579699993133545\n",
      "\t Loss: 2.202232837677002\n",
      "\t Loss: 3.0366156101226807\n",
      "\t Loss: 25.16575813293457\n",
      "\t Loss: 65.1890640258789\n",
      "Outer loss: 10.880924224853516\n",
      "# 3: outer_epoch:[768] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.397212982177734\n",
      "\t Loss: 1.523574709892273\n",
      "\t Loss: 5.807477951049805\n",
      "\t Loss: 11.701452255249023\n",
      "\t Loss: 53.86953353881836\n",
      "Outer loss: 16.275711059570312\n",
      "# 3: outer_epoch:[769] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.91871452331543\n",
      "\t Loss: 0.2564065754413605\n",
      "\t Loss: 3.2640185356140137\n",
      "\t Loss: 19.927696228027344\n",
      "\t Loss: 37.979400634765625\n",
      "Outer loss: 18.051240921020508\n",
      "# 3: outer_epoch:[770] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.0849905014038086\n",
      "\t Loss: 1.3846056461334229\n",
      "\t Loss: 4.8126912117004395\n",
      "\t Loss: 26.609222412109375\n",
      "\t Loss: 67.40637969970703\n",
      "Outer loss: 11.890668869018555\n",
      "# 3: outer_epoch:[771] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.648848533630371\n",
      "\t Loss: 1.4391746520996094\n",
      "\t Loss: 7.681273937225342\n",
      "\t Loss: 27.90019989013672\n",
      "\t Loss: 44.076820373535156\n",
      "Outer loss: 9.293573379516602\n",
      "# 3: outer_epoch:[772] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.58776330947876\n",
      "\t Loss: 0.9890621900558472\n",
      "\t Loss: 2.17287015914917\n",
      "\t Loss: 26.615137100219727\n",
      "\t Loss: 27.971271514892578\n",
      "Outer loss: 17.951045989990234\n",
      "# 3: outer_epoch:[773] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.333232879638672\n",
      "\t Loss: 4.668104648590088\n",
      "\t Loss: 3.8443541526794434\n",
      "\t Loss: 10.205178260803223\n",
      "\t Loss: 18.271799087524414\n",
      "Outer loss: 22.209611892700195\n",
      "# 3: outer_epoch:[774] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.858863830566406\n",
      "\t Loss: 14.317716598510742\n",
      "\t Loss: 2.523998737335205\n",
      "\t Loss: 3.208810329437256\n",
      "\t Loss: 11.85342025756836\n",
      "Outer loss: 16.657337188720703\n",
      "# 3: outer_epoch:[775] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.290847778320312\n",
      "\t Loss: 9.197036743164062\n",
      "\t Loss: 4.343249320983887\n",
      "\t Loss: 9.233654022216797\n",
      "\t Loss: 24.52117156982422\n",
      "Outer loss: 14.22692584991455\n",
      "# 3: outer_epoch:[776] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.144837379455566\n",
      "\t Loss: 3.592491388320923\n",
      "\t Loss: 0.4761301875114441\n",
      "\t Loss: 12.138484001159668\n",
      "\t Loss: 37.17957305908203\n",
      "Outer loss: 6.775348663330078\n",
      "# 3: outer_epoch:[777] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.184623718261719\n",
      "\t Loss: 1.9042702913284302\n",
      "\t Loss: 2.4127073287963867\n",
      "\t Loss: 19.437482833862305\n",
      "\t Loss: 50.72629928588867\n",
      "Outer loss: 5.777090072631836\n",
      "# 3: outer_epoch:[778] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.47960662841797\n",
      "\t Loss: 1.949710726737976\n",
      "\t Loss: 1.9251900911331177\n",
      "\t Loss: 14.487031936645508\n",
      "\t Loss: 40.10345458984375\n",
      "Outer loss: 4.434041500091553\n",
      "# 3: outer_epoch:[779] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.044912338256836\n",
      "\t Loss: 2.5464913845062256\n",
      "\t Loss: 1.594581961631775\n",
      "\t Loss: 13.806388854980469\n",
      "\t Loss: 43.0457763671875\n",
      "Outer loss: 7.177468299865723\n",
      "# 3: outer_epoch:[780] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.124046325683594\n",
      "\t Loss: 2.4889135360717773\n",
      "\t Loss: 1.9376683235168457\n",
      "\t Loss: 13.529109001159668\n",
      "\t Loss: 37.073341369628906\n",
      "Outer loss: 10.620591163635254\n",
      "# 3: outer_epoch:[781] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.757275581359863\n",
      "\t Loss: 3.390230178833008\n",
      "\t Loss: 0.9390203356742859\n",
      "\t Loss: 12.321788787841797\n",
      "\t Loss: 29.811710357666016\n",
      "Outer loss: 15.915695190429688\n",
      "# 3: outer_epoch:[782] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.980203628540039\n",
      "\t Loss: 4.5278496742248535\n",
      "\t Loss: 0.2948351800441742\n",
      "\t Loss: 6.357950687408447\n",
      "\t Loss: 28.398555755615234\n",
      "Outer loss: 13.39006519317627\n",
      "# 3: outer_epoch:[783] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.05495262145996\n",
      "\t Loss: 7.7593674659729\n",
      "\t Loss: 0.9226807355880737\n",
      "\t Loss: 13.716432571411133\n",
      "\t Loss: 28.121198654174805\n",
      "Outer loss: 11.515263557434082\n",
      "# 3: outer_epoch:[784] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.057348251342773\n",
      "\t Loss: 4.7079386711120605\n",
      "\t Loss: 1.2183781862258911\n",
      "\t Loss: 12.777278900146484\n",
      "\t Loss: 20.543371200561523\n",
      "Outer loss: 16.335678100585938\n",
      "# 3: outer_epoch:[785] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.03459167480469\n",
      "\t Loss: 9.914212226867676\n",
      "\t Loss: 1.3010767698287964\n",
      "\t Loss: 5.147150039672852\n",
      "\t Loss: 30.909818649291992\n",
      "Outer loss: 14.239142417907715\n",
      "# 3: outer_epoch:[786] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.63318634033203\n",
      "\t Loss: 8.890278816223145\n",
      "\t Loss: 2.3958613872528076\n",
      "\t Loss: 5.896157264709473\n",
      "\t Loss: 22.927732467651367\n",
      "Outer loss: 7.729456901550293\n",
      "# 3: outer_epoch:[787] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.14106750488281\n",
      "\t Loss: 7.021734714508057\n",
      "\t Loss: 0.23771680891513824\n",
      "\t Loss: 3.8491904735565186\n",
      "\t Loss: 36.92045593261719\n",
      "Outer loss: 7.429717540740967\n",
      "# 3: outer_epoch:[788] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.785377502441406\n",
      "\t Loss: 13.387201309204102\n",
      "\t Loss: 1.2869435548782349\n",
      "\t Loss: 5.523028373718262\n",
      "\t Loss: 19.737232208251953\n",
      "Outer loss: 8.91334342956543\n",
      "# 3: outer_epoch:[789] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.205345153808594\n",
      "\t Loss: 8.163812637329102\n",
      "\t Loss: 1.374028205871582\n",
      "\t Loss: 5.6853556632995605\n",
      "\t Loss: 15.111631393432617\n",
      "Outer loss: 9.698712348937988\n",
      "# 3: outer_epoch:[790] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.727983474731445\n",
      "\t Loss: 16.887413024902344\n",
      "\t Loss: 1.5478047132492065\n",
      "\t Loss: 6.836757659912109\n",
      "\t Loss: 24.22745132446289\n",
      "Outer loss: 10.56059741973877\n",
      "# 3: outer_epoch:[791] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.27482795715332\n",
      "\t Loss: 7.158319473266602\n",
      "\t Loss: 2.838716745376587\n",
      "\t Loss: 13.306326866149902\n",
      "\t Loss: 16.340730667114258\n",
      "Outer loss: 24.296123504638672\n",
      "# 3: outer_epoch:[792] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 55.072933197021484\n",
      "\t Loss: 7.078490257263184\n",
      "\t Loss: 2.42030668258667\n",
      "\t Loss: 2.150775194168091\n",
      "\t Loss: 5.121551990509033\n",
      "Outer loss: 13.393600463867188\n",
      "# 3: outer_epoch:[793] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.527341842651367\n",
      "\t Loss: 10.229519844055176\n",
      "\t Loss: 3.744150161743164\n",
      "\t Loss: 7.3812150955200195\n",
      "\t Loss: 11.078455924987793\n",
      "Outer loss: 24.265779495239258\n",
      "# 3: outer_epoch:[794] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 26.539960861206055\n",
      "\t Loss: 10.621901512145996\n",
      "\t Loss: 1.4986592531204224\n",
      "\t Loss: 7.066053867340088\n",
      "\t Loss: 37.95082473754883\n",
      "Outer loss: 12.85972785949707\n",
      "# 3: outer_epoch:[795] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.721561431884766\n",
      "\t Loss: 10.65999698638916\n",
      "\t Loss: 1.3871392011642456\n",
      "\t Loss: 3.449446201324463\n",
      "\t Loss: 21.871810913085938\n",
      "Outer loss: 13.604484558105469\n",
      "# 3: outer_epoch:[796] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.929401397705078\n",
      "\t Loss: 9.402411460876465\n",
      "\t Loss: 0.6071955561637878\n",
      "\t Loss: 9.920762062072754\n",
      "\t Loss: 35.35407257080078\n",
      "Outer loss: 7.490230083465576\n",
      "# 3: outer_epoch:[797] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.107341766357422\n",
      "\t Loss: 4.4608354568481445\n",
      "\t Loss: 0.5414310097694397\n",
      "\t Loss: 8.219878196716309\n",
      "\t Loss: 36.167991638183594\n",
      "Outer loss: 5.11379337310791\n",
      "# 3: outer_epoch:[798] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.671810150146484\n",
      "\t Loss: 3.972930669784546\n",
      "\t Loss: 0.4537631869316101\n",
      "\t Loss: 14.795523643493652\n",
      "\t Loss: 37.433319091796875\n",
      "Outer loss: 6.686832904815674\n",
      "# 3: outer_epoch:[799] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.343778610229492\n",
      "\t Loss: 2.4989047050476074\n",
      "\t Loss: 1.3776700496673584\n",
      "\t Loss: 17.225788116455078\n",
      "\t Loss: 37.19489288330078\n",
      "Outer loss: 12.729114532470703\n",
      "# 3: outer_epoch:[800] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.101192474365234\n",
      "\t Loss: 1.3279684782028198\n",
      "\t Loss: 2.542508125305176\n",
      "\t Loss: 11.787903785705566\n",
      "\t Loss: 21.15410614013672\n",
      "Outer loss: 17.97892951965332\n",
      "# 3: outer_epoch:[801] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.6798152923584\n",
      "\t Loss: 1.786705493927002\n",
      "\t Loss: 1.8451964855194092\n",
      "\t Loss: 16.819936752319336\n",
      "\t Loss: 29.115827560424805\n",
      "Outer loss: 11.174348831176758\n",
      "# 3: outer_epoch:[802] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.670644760131836\n",
      "\t Loss: 15.151544570922852\n",
      "\t Loss: 0.4880698025226593\n",
      "\t Loss: 3.0858402252197266\n",
      "\t Loss: 11.887033462524414\n",
      "Outer loss: 13.87308120727539\n",
      "# 3: outer_epoch:[803] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 40.25193405151367\n",
      "\t Loss: 16.705322265625\n",
      "\t Loss: 0.33986136317253113\n",
      "\t Loss: 2.733232259750366\n",
      "\t Loss: 6.867595195770264\n",
      "Outer loss: 7.898963928222656\n",
      "# 3: outer_epoch:[804] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.34983444213867\n",
      "\t Loss: 8.787168502807617\n",
      "\t Loss: 0.7031196355819702\n",
      "\t Loss: 6.2606000900268555\n",
      "\t Loss: 14.363585472106934\n",
      "Outer loss: 9.528804779052734\n",
      "# 3: outer_epoch:[805] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 44.05039978027344\n",
      "\t Loss: 9.257054328918457\n",
      "\t Loss: 1.73506760597229\n",
      "\t Loss: 3.1493070125579834\n",
      "\t Loss: 19.98505401611328\n",
      "Outer loss: 7.815293788909912\n",
      "# 3: outer_epoch:[806] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.35198211669922\n",
      "\t Loss: 16.06509017944336\n",
      "\t Loss: 0.46247199177742004\n",
      "\t Loss: 4.925786018371582\n",
      "\t Loss: 25.484153747558594\n",
      "Outer loss: 11.851919174194336\n",
      "# 3: outer_epoch:[807] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.632368087768555\n",
      "\t Loss: 3.9190316200256348\n",
      "\t Loss: 1.622867465019226\n",
      "\t Loss: 15.15270709991455\n",
      "\t Loss: 39.888214111328125\n",
      "Outer loss: 15.811746597290039\n",
      "# 3: outer_epoch:[808] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.807014465332031\n",
      "\t Loss: 2.793285846710205\n",
      "\t Loss: 2.0429494380950928\n",
      "\t Loss: 9.89326286315918\n",
      "\t Loss: 8.639670372009277\n",
      "Outer loss: 36.16510009765625\n",
      "# 3: outer_epoch:[809] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 70.69015502929688\n",
      "\t Loss: 9.781085014343262\n",
      "\t Loss: 3.953953266143799\n",
      "\t Loss: 3.817281723022461\n",
      "\t Loss: 10.172327041625977\n",
      "Outer loss: 19.580425262451172\n",
      "# 3: outer_epoch:[810] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.014293670654297\n",
      "\t Loss: 10.055806159973145\n",
      "\t Loss: 1.5960015058517456\n",
      "\t Loss: 1.8230235576629639\n",
      "\t Loss: 23.789352416992188\n",
      "Outer loss: 9.274807929992676\n",
      "# 3: outer_epoch:[811] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.97998046875\n",
      "\t Loss: 7.645172119140625\n",
      "\t Loss: 3.8818166255950928\n",
      "\t Loss: 2.9423351287841797\n",
      "\t Loss: 20.1345157623291\n",
      "Outer loss: 7.849084854125977\n",
      "# 3: outer_epoch:[812] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.660903930664062\n",
      "\t Loss: 13.959554672241211\n",
      "\t Loss: 0.23658549785614014\n",
      "\t Loss: 2.722756862640381\n",
      "\t Loss: 7.844968318939209\n",
      "Outer loss: 19.005409240722656\n",
      "# 3: outer_epoch:[813] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.15379524230957\n",
      "\t Loss: 4.777126312255859\n",
      "\t Loss: 0.4769406020641327\n",
      "\t Loss: 11.551058769226074\n",
      "\t Loss: 30.706214904785156\n",
      "Outer loss: 10.800766944885254\n",
      "# 3: outer_epoch:[814] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.32809829711914\n",
      "\t Loss: 6.037069797515869\n",
      "\t Loss: 1.7122015953063965\n",
      "\t Loss: 8.299209594726562\n",
      "\t Loss: 16.483978271484375\n",
      "Outer loss: 8.620040893554688\n",
      "# 3: outer_epoch:[815] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.81391143798828\n",
      "\t Loss: 8.416037559509277\n",
      "\t Loss: 0.30815503001213074\n",
      "\t Loss: 4.0812811851501465\n",
      "\t Loss: 26.246156692504883\n",
      "Outer loss: 8.656876564025879\n",
      "# 3: outer_epoch:[816] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.255521774291992\n",
      "\t Loss: 10.99108600616455\n",
      "\t Loss: 2.6227753162384033\n",
      "\t Loss: 4.726437568664551\n",
      "\t Loss: 28.25579071044922\n",
      "Outer loss: 7.473384380340576\n",
      "# 3: outer_epoch:[817] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.3896484375\n",
      "\t Loss: 10.601335525512695\n",
      "\t Loss: 0.6203874349594116\n",
      "\t Loss: 3.450305938720703\n",
      "\t Loss: 28.627765655517578\n",
      "Outer loss: 8.429939270019531\n",
      "# 3: outer_epoch:[818] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.572404861450195\n",
      "\t Loss: 16.75037384033203\n",
      "\t Loss: 0.8033196926116943\n",
      "\t Loss: 3.51904296875\n",
      "\t Loss: 11.31201171875\n",
      "Outer loss: 17.16830825805664\n",
      "# 3: outer_epoch:[819] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.113433837890625\n",
      "\t Loss: 3.8383593559265137\n",
      "\t Loss: 2.4709019660949707\n",
      "\t Loss: 11.4385986328125\n",
      "\t Loss: 23.83932113647461\n",
      "Outer loss: 23.699234008789062\n",
      "# 3: outer_epoch:[820] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.65523338317871\n",
      "\t Loss: 9.687786102294922\n",
      "\t Loss: 0.4310852587223053\n",
      "\t Loss: 5.520679473876953\n",
      "\t Loss: 25.552812576293945\n",
      "Outer loss: 8.464399337768555\n",
      "# 3: outer_epoch:[821] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.95819854736328\n",
      "\t Loss: 7.789181709289551\n",
      "\t Loss: 2.5929489135742188\n",
      "\t Loss: 7.95266580581665\n",
      "\t Loss: 16.04775619506836\n",
      "Outer loss: 12.903619766235352\n",
      "# 3: outer_epoch:[822] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.239360809326172\n",
      "\t Loss: 5.53778600692749\n",
      "\t Loss: 0.33214908838272095\n",
      "\t Loss: 10.348562240600586\n",
      "\t Loss: 32.793701171875\n",
      "Outer loss: 33.86806869506836\n",
      "# 3: outer_epoch:[823] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.077778816223145\n",
      "\t Loss: 3.856475353240967\n",
      "\t Loss: 5.37095832824707\n",
      "\t Loss: 30.843273162841797\n",
      "\t Loss: 37.14533233642578\n",
      "Outer loss: 18.243629455566406\n",
      "# 3: outer_epoch:[824] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.99239730834961\n",
      "\t Loss: 2.920375347137451\n",
      "\t Loss: 1.487108826637268\n",
      "\t Loss: 7.245489120483398\n",
      "\t Loss: 22.954233169555664\n",
      "Outer loss: 16.637012481689453\n",
      "# 3: outer_epoch:[825] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.162537097930908\n",
      "\t Loss: 2.408445358276367\n",
      "\t Loss: 3.339048385620117\n",
      "\t Loss: 15.415096282958984\n",
      "\t Loss: 55.44371032714844\n",
      "Outer loss: 15.39317798614502\n",
      "# 3: outer_epoch:[826] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.997316360473633\n",
      "\t Loss: 1.524207592010498\n",
      "\t Loss: 7.680651664733887\n",
      "\t Loss: 21.541696548461914\n",
      "\t Loss: 50.26387023925781\n",
      "Outer loss: 6.933806896209717\n",
      "# 3: outer_epoch:[827] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.746476173400879\n",
      "\t Loss: 1.7106415033340454\n",
      "\t Loss: 2.5744471549987793\n",
      "\t Loss: 12.59725570678711\n",
      "\t Loss: 39.34245681762695\n",
      "Outer loss: 7.776556015014648\n",
      "# 3: outer_epoch:[828] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.5181024074554443\n",
      "\t Loss: 1.3673609495162964\n",
      "\t Loss: 2.3323373794555664\n",
      "\t Loss: 15.133318901062012\n",
      "\t Loss: 54.37044143676758\n",
      "Outer loss: 12.052713394165039\n",
      "# 3: outer_epoch:[829] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.7791547775268555\n",
      "\t Loss: 0.13492271304130554\n",
      "\t Loss: 10.382369995117188\n",
      "\t Loss: 20.95370864868164\n",
      "\t Loss: 43.72416305541992\n",
      "Outer loss: 11.929716110229492\n",
      "# 3: outer_epoch:[830] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.9747886657714844\n",
      "\t Loss: 0.8807306885719299\n",
      "\t Loss: 1.884376049041748\n",
      "\t Loss: 35.947967529296875\n",
      "\t Loss: 66.68888092041016\n",
      "Outer loss: 11.63208293914795\n",
      "# 3: outer_epoch:[831] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.439865112304688\n",
      "\t Loss: 5.260495185852051\n",
      "\t Loss: 1.4171165227890015\n",
      "\t Loss: 3.4078586101531982\n",
      "\t Loss: 24.37278175354004\n",
      "Outer loss: 10.480632781982422\n",
      "# 3: outer_epoch:[832] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.195415496826172\n",
      "\t Loss: 5.58331298828125\n",
      "\t Loss: 0.6305031180381775\n",
      "\t Loss: 10.801931381225586\n",
      "\t Loss: 43.511714935302734\n",
      "Outer loss: 9.105676651000977\n",
      "# 3: outer_epoch:[833] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 41.75552749633789\n",
      "\t Loss: 11.304574966430664\n",
      "\t Loss: 1.1232209205627441\n",
      "\t Loss: 1.0682425498962402\n",
      "\t Loss: 13.935091018676758\n",
      "Outer loss: 9.488487243652344\n",
      "# 3: outer_epoch:[834] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.79581642150879\n",
      "\t Loss: 23.28218650817871\n",
      "\t Loss: 3.6034510135650635\n",
      "\t Loss: 0.6616069078445435\n",
      "\t Loss: 8.87792682647705\n",
      "Outer loss: 11.545581817626953\n",
      "# 3: outer_epoch:[835] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.205141067504883\n",
      "\t Loss: 11.576613426208496\n",
      "\t Loss: 0.461063027381897\n",
      "\t Loss: 4.11190128326416\n",
      "\t Loss: 16.03403663635254\n",
      "Outer loss: 20.56943130493164\n",
      "# 3: outer_epoch:[836] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.31781578063965\n",
      "\t Loss: 4.681731224060059\n",
      "\t Loss: 1.1742217540740967\n",
      "\t Loss: 6.154966354370117\n",
      "\t Loss: 29.439510345458984\n",
      "Outer loss: 12.317327499389648\n",
      "# 3: outer_epoch:[837] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.063613891601562\n",
      "\t Loss: 9.659975051879883\n",
      "\t Loss: 0.28551989793777466\n",
      "\t Loss: 7.91903018951416\n",
      "\t Loss: 22.29981231689453\n",
      "Outer loss: 7.3439459800720215\n",
      "# 3: outer_epoch:[838] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.577478408813477\n",
      "\t Loss: 9.74629020690918\n",
      "\t Loss: 0.2336120754480362\n",
      "\t Loss: 5.4077606201171875\n",
      "\t Loss: 16.44288444519043\n",
      "Outer loss: 6.168850421905518\n",
      "# 3: outer_epoch:[839] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.292272567749023\n",
      "\t Loss: 5.320315837860107\n",
      "\t Loss: 0.027479274198412895\n",
      "\t Loss: 5.101420879364014\n",
      "\t Loss: 21.0445556640625\n",
      "Outer loss: 9.278594970703125\n",
      "# 3: outer_epoch:[840] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.596879959106445\n",
      "\t Loss: 4.639317512512207\n",
      "\t Loss: 0.5472012162208557\n",
      "\t Loss: 15.812220573425293\n",
      "\t Loss: 18.911073684692383\n",
      "Outer loss: 14.825008392333984\n",
      "# 3: outer_epoch:[841] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.831071853637695\n",
      "\t Loss: 5.37316370010376\n",
      "\t Loss: 0.3785511255264282\n",
      "\t Loss: 5.640176773071289\n",
      "\t Loss: 37.47949981689453\n",
      "Outer loss: 16.34297752380371\n",
      "# 3: outer_epoch:[842] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.123717308044434\n",
      "\t Loss: 7.334300994873047\n",
      "\t Loss: 0.6009622812271118\n",
      "\t Loss: 14.198737144470215\n",
      "\t Loss: 26.456865310668945\n",
      "Outer loss: 18.452556610107422\n",
      "# 3: outer_epoch:[843] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.080833435058594\n",
      "\t Loss: 2.740999698638916\n",
      "\t Loss: 0.2539201080799103\n",
      "\t Loss: 5.0899529457092285\n",
      "\t Loss: 19.900043487548828\n",
      "Outer loss: 11.971314430236816\n",
      "# 3: outer_epoch:[844] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.047927856445312\n",
      "\t Loss: 5.121607780456543\n",
      "\t Loss: 5.739535808563232\n",
      "\t Loss: 15.75339412689209\n",
      "\t Loss: 33.635005950927734\n",
      "Outer loss: 22.49295425415039\n",
      "# 3: outer_epoch:[845] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.895578384399414\n",
      "\t Loss: 11.99417781829834\n",
      "\t Loss: 1.4333934783935547\n",
      "\t Loss: 2.2705867290496826\n",
      "\t Loss: 14.68249225616455\n",
      "Outer loss: 6.469394683837891\n",
      "# 3: outer_epoch:[846] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.759353637695312\n",
      "\t Loss: 5.446211814880371\n",
      "\t Loss: 2.2942593097686768\n",
      "\t Loss: 4.1000542640686035\n",
      "\t Loss: 16.38941192626953\n",
      "Outer loss: 26.54796028137207\n",
      "# 3: outer_epoch:[847] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.971116542816162\n",
      "\t Loss: 5.644865989685059\n",
      "\t Loss: 18.72134780883789\n",
      "\t Loss: 33.11078643798828\n",
      "\t Loss: 78.14163970947266\n",
      "Outer loss: 10.651487350463867\n",
      "# 3: outer_epoch:[848] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.75339126586914\n",
      "\t Loss: 1.4585785865783691\n",
      "\t Loss: 8.062609672546387\n",
      "\t Loss: 31.068330764770508\n",
      "\t Loss: 53.84391784667969\n",
      "Outer loss: 26.03043556213379\n",
      "# 3: outer_epoch:[849] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.962730407714844\n",
      "\t Loss: 40.39588928222656\n",
      "\t Loss: 26.43145751953125\n",
      "\t Loss: 199.0362548828125\n",
      "\t Loss: 219.2575225830078\n",
      "Outer loss: 68.08976745605469\n",
      "# 3: outer_epoch:[850] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.562522888183594\n",
      "\t Loss: 1.4439873695373535\n",
      "\t Loss: 4.88299036026001\n",
      "\t Loss: 18.814167022705078\n",
      "\t Loss: 53.42890167236328\n",
      "Outer loss: 10.052056312561035\n",
      "# 3: outer_epoch:[851] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.27726936340332\n",
      "\t Loss: 2.229752540588379\n",
      "\t Loss: 4.967377662658691\n",
      "\t Loss: 20.834157943725586\n",
      "\t Loss: 47.06511306762695\n",
      "Outer loss: 13.82779598236084\n",
      "# 3: outer_epoch:[852] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.612127304077148\n",
      "\t Loss: 2.4186489582061768\n",
      "\t Loss: 5.546192169189453\n",
      "\t Loss: 26.696273803710938\n",
      "\t Loss: 63.15395736694336\n",
      "Outer loss: 7.25376033782959\n",
      "# 3: outer_epoch:[853] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.376322746276855\n",
      "\t Loss: 1.5830929279327393\n",
      "\t Loss: 2.9641308784484863\n",
      "\t Loss: 18.56383514404297\n",
      "\t Loss: 44.91246795654297\n",
      "Outer loss: 13.05502700805664\n",
      "# 3: outer_epoch:[854] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.887313842773438\n",
      "\t Loss: 1.9841444492340088\n",
      "\t Loss: 4.145390033721924\n",
      "\t Loss: 19.735929489135742\n",
      "\t Loss: 26.967395782470703\n",
      "Outer loss: 18.78522491455078\n",
      "# 3: outer_epoch:[855] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.66256332397461\n",
      "\t Loss: 1.2703067064285278\n",
      "\t Loss: 3.6474318504333496\n",
      "\t Loss: 10.9224853515625\n",
      "\t Loss: 50.497413635253906\n",
      "Outer loss: 15.510892868041992\n",
      "# 3: outer_epoch:[856] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.407217979431152\n",
      "\t Loss: 2.1394567489624023\n",
      "\t Loss: 3.627530097961426\n",
      "\t Loss: 15.17106819152832\n",
      "\t Loss: 49.54051208496094\n",
      "Outer loss: 4.8823723793029785\n",
      "# 3: outer_epoch:[857] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.229670524597168\n",
      "\t Loss: 1.1363345384597778\n",
      "\t Loss: 2.5082414150238037\n",
      "\t Loss: 20.878108978271484\n",
      "\t Loss: 33.142269134521484\n",
      "Outer loss: 10.406872749328613\n",
      "# 3: outer_epoch:[858] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.53451919555664\n",
      "\t Loss: 2.176501750946045\n",
      "\t Loss: 2.3534884452819824\n",
      "\t Loss: 9.92763614654541\n",
      "\t Loss: 20.600202560424805\n",
      "Outer loss: 20.903362274169922\n",
      "# 3: outer_epoch:[859] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.986846923828125\n",
      "\t Loss: 5.267628192901611\n",
      "\t Loss: 0.7796210050582886\n",
      "\t Loss: 15.031025886535645\n",
      "\t Loss: 43.51292037963867\n",
      "Outer loss: 10.99139404296875\n",
      "# 3: outer_epoch:[860] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 46.95863723754883\n",
      "\t Loss: 13.2362699508667\n",
      "\t Loss: 3.9062795639038086\n",
      "\t Loss: 2.027991771697998\n",
      "\t Loss: 17.98777198791504\n",
      "Outer loss: 6.029577255249023\n",
      "# 3: outer_epoch:[861] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.5301628112793\n",
      "\t Loss: 11.085602760314941\n",
      "\t Loss: 1.446524739265442\n",
      "\t Loss: 4.552234649658203\n",
      "\t Loss: 15.44472885131836\n",
      "Outer loss: 11.044994354248047\n",
      "# 3: outer_epoch:[862] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.11575698852539\n",
      "\t Loss: 14.70348834991455\n",
      "\t Loss: 0.5847737789154053\n",
      "\t Loss: 4.555167198181152\n",
      "\t Loss: 12.870613098144531\n",
      "Outer loss: 14.698789596557617\n",
      "# 3: outer_epoch:[863] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.75007247924805\n",
      "\t Loss: 7.81010103225708\n",
      "\t Loss: 1.5197722911834717\n",
      "\t Loss: 3.893028736114502\n",
      "\t Loss: 11.98393440246582\n",
      "Outer loss: 9.9468994140625\n",
      "# 3: outer_epoch:[864] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.699241638183594\n",
      "\t Loss: 18.195106506347656\n",
      "\t Loss: 2.2013609409332275\n",
      "\t Loss: 2.340620994567871\n",
      "\t Loss: 13.380362510681152\n",
      "Outer loss: 9.683351516723633\n",
      "# 3: outer_epoch:[865] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.21759796142578\n",
      "\t Loss: 8.63769245147705\n",
      "\t Loss: 0.3433000147342682\n",
      "\t Loss: 4.8431854248046875\n",
      "\t Loss: 20.460596084594727\n",
      "Outer loss: 7.887191295623779\n",
      "# 3: outer_epoch:[866] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.33139419555664\n",
      "\t Loss: 5.239446640014648\n",
      "\t Loss: 0.37023723125457764\n",
      "\t Loss: 5.943890571594238\n",
      "\t Loss: 30.845264434814453\n",
      "Outer loss: 10.303808212280273\n",
      "# 3: outer_epoch:[867] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.265798568725586\n",
      "\t Loss: 5.121338367462158\n",
      "\t Loss: 1.9255766868591309\n",
      "\t Loss: 11.779382705688477\n",
      "\t Loss: 46.30662155151367\n",
      "Outer loss: 8.848254203796387\n",
      "# 3: outer_epoch:[868] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.96847915649414\n",
      "\t Loss: 5.280784606933594\n",
      "\t Loss: 4.517387390136719\n",
      "\t Loss: 18.96575355529785\n",
      "\t Loss: 34.14889144897461\n",
      "Outer loss: 20.35602569580078\n",
      "# 3: outer_epoch:[869] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.06376647949219\n",
      "\t Loss: 13.877875328063965\n",
      "\t Loss: 11.341046333312988\n",
      "\t Loss: 13.983020782470703\n",
      "\t Loss: 47.647300720214844\n",
      "Outer loss: 8.924921035766602\n",
      "# 3: outer_epoch:[870] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.701282501220703\n",
      "\t Loss: 4.337695121765137\n",
      "\t Loss: 0.931477427482605\n",
      "\t Loss: 17.506559371948242\n",
      "\t Loss: 19.16277313232422\n",
      "Outer loss: 15.699832916259766\n",
      "# 3: outer_epoch:[871] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.51073455810547\n",
      "\t Loss: 5.113400459289551\n",
      "\t Loss: 2.0223653316497803\n",
      "\t Loss: 10.253210067749023\n",
      "\t Loss: 16.46992301940918\n",
      "Outer loss: 18.400001525878906\n",
      "# 3: outer_epoch:[872] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.625675201416016\n",
      "\t Loss: 11.825746536254883\n",
      "\t Loss: 0.3599031865596771\n",
      "\t Loss: 3.5598807334899902\n",
      "\t Loss: 23.092947006225586\n",
      "Outer loss: 7.766311168670654\n",
      "# 3: outer_epoch:[873] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.8713960647583\n",
      "\t Loss: 5.436525344848633\n",
      "\t Loss: 0.8604266047477722\n",
      "\t Loss: 6.768251895904541\n",
      "\t Loss: 19.42331314086914\n",
      "Outer loss: 19.03130340576172\n",
      "# 3: outer_epoch:[874] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.91185188293457\n",
      "\t Loss: 5.417997360229492\n",
      "\t Loss: 0.14633993804454803\n",
      "\t Loss: 6.613151550292969\n",
      "\t Loss: 37.757545471191406\n",
      "Outer loss: 18.94231414794922\n",
      "# 3: outer_epoch:[875] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.945995807647705\n",
      "\t Loss: 4.08736515045166\n",
      "\t Loss: 6.343653678894043\n",
      "\t Loss: 21.399181365966797\n",
      "\t Loss: 49.41569900512695\n",
      "Outer loss: 22.22972297668457\n",
      "# 3: outer_epoch:[876] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.485544204711914\n",
      "\t Loss: 2.015336275100708\n",
      "\t Loss: 8.654264450073242\n",
      "\t Loss: 21.79780387878418\n",
      "\t Loss: 65.2752914428711\n",
      "Outer loss: 15.390684127807617\n",
      "# 3: outer_epoch:[877] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.672550201416016\n",
      "\t Loss: 0.3203812539577484\n",
      "\t Loss: 8.672656059265137\n",
      "\t Loss: 22.331998825073242\n",
      "\t Loss: 66.8837890625\n",
      "Outer loss: 5.273209571838379\n",
      "# 3: outer_epoch:[878] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.092905521392822\n",
      "\t Loss: 1.9356489181518555\n",
      "\t Loss: 7.289799690246582\n",
      "\t Loss: 31.762300491333008\n",
      "\t Loss: 58.53921127319336\n",
      "Outer loss: 6.72348690032959\n",
      "# 3: outer_epoch:[879] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.5508551597595215\n",
      "\t Loss: 0.9574360251426697\n",
      "\t Loss: 7.444879531860352\n",
      "\t Loss: 29.71578598022461\n",
      "\t Loss: 100.47608947753906\n",
      "Outer loss: 10.545969009399414\n",
      "# 3: outer_epoch:[880] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.5095272064209\n",
      "\t Loss: 2.9321107864379883\n",
      "\t Loss: 1.2935781478881836\n",
      "\t Loss: 14.78649616241455\n",
      "\t Loss: 33.799766540527344\n",
      "Outer loss: 15.856613159179688\n",
      "# 3: outer_epoch:[881] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.262203216552734\n",
      "\t Loss: 5.472601890563965\n",
      "\t Loss: 5.871382713317871\n",
      "\t Loss: 16.29264259338379\n",
      "\t Loss: 43.451416015625\n",
      "Outer loss: 13.09239673614502\n",
      "# 3: outer_epoch:[882] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.418861389160156\n",
      "\t Loss: 2.381498098373413\n",
      "\t Loss: 1.3993858098983765\n",
      "\t Loss: 10.222846031188965\n",
      "\t Loss: 34.64220428466797\n",
      "Outer loss: 10.339228630065918\n",
      "# 3: outer_epoch:[883] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.41647720336914\n",
      "\t Loss: 5.528504371643066\n",
      "\t Loss: 0.48729535937309265\n",
      "\t Loss: 8.735122680664062\n",
      "\t Loss: 42.728355407714844\n",
      "Outer loss: 4.293415069580078\n",
      "# 3: outer_epoch:[884] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.282669067382812\n",
      "\t Loss: 7.720508575439453\n",
      "\t Loss: 0.14897197484970093\n",
      "\t Loss: 8.211615562438965\n",
      "\t Loss: 13.348355293273926\n",
      "Outer loss: 19.47076988220215\n",
      "# 3: outer_epoch:[885] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.424211502075195\n",
      "\t Loss: 10.038217544555664\n",
      "\t Loss: 2.1244447231292725\n",
      "\t Loss: 5.471527099609375\n",
      "\t Loss: 24.185489654541016\n",
      "Outer loss: 10.556985855102539\n",
      "# 3: outer_epoch:[886] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.666473388671875\n",
      "\t Loss: 9.893519401550293\n",
      "\t Loss: 1.052536964416504\n",
      "\t Loss: 2.7317166328430176\n",
      "\t Loss: 9.358537673950195\n",
      "Outer loss: 8.93780517578125\n",
      "# 3: outer_epoch:[887] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.48859786987305\n",
      "\t Loss: 11.660148620605469\n",
      "\t Loss: 0.9386534094810486\n",
      "\t Loss: 2.1922497749328613\n",
      "\t Loss: 28.833921432495117\n",
      "Outer loss: 14.370811462402344\n",
      "# 3: outer_epoch:[888] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.89318084716797\n",
      "\t Loss: 12.404163360595703\n",
      "\t Loss: 0.7896943092346191\n",
      "\t Loss: 1.839516520500183\n",
      "\t Loss: 19.69196891784668\n",
      "Outer loss: 11.141068458557129\n",
      "# 3: outer_epoch:[889] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.167476654052734\n",
      "\t Loss: 16.840181350708008\n",
      "\t Loss: 3.256427049636841\n",
      "\t Loss: 3.6153478622436523\n",
      "\t Loss: 18.998998641967773\n",
      "Outer loss: 10.27367115020752\n",
      "# 3: outer_epoch:[890] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 45.11811447143555\n",
      "\t Loss: 9.132415771484375\n",
      "\t Loss: 0.22239632904529572\n",
      "\t Loss: 5.571197032928467\n",
      "\t Loss: 12.229588508605957\n",
      "Outer loss: 49.23873519897461\n",
      "# 3: outer_epoch:[891] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.419557571411133\n",
      "\t Loss: 12.271374702453613\n",
      "\t Loss: 13.509918212890625\n",
      "\t Loss: 16.357715606689453\n",
      "\t Loss: 73.96553039550781\n",
      "Outer loss: 25.54054832458496\n",
      "# 3: outer_epoch:[892] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.314680099487305\n",
      "\t Loss: 1.4156688451766968\n",
      "\t Loss: 6.413403511047363\n",
      "\t Loss: 23.06922721862793\n",
      "\t Loss: 46.75175857543945\n",
      "Outer loss: 12.325346946716309\n",
      "# 3: outer_epoch:[893] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.69914722442627\n",
      "\t Loss: 1.296094298362732\n",
      "\t Loss: 3.6452417373657227\n",
      "\t Loss: 15.196207046508789\n",
      "\t Loss: 89.98884582519531\n",
      "Outer loss: 17.166603088378906\n",
      "# 3: outer_epoch:[894] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.127811431884766\n",
      "\t Loss: 3.3998043537139893\n",
      "\t Loss: 5.604413032531738\n",
      "\t Loss: 21.47246742248535\n",
      "\t Loss: 25.144168853759766\n",
      "Outer loss: 23.493574142456055\n",
      "# 3: outer_epoch:[895] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.09649658203125\n",
      "\t Loss: 1.369654893875122\n",
      "\t Loss: 2.2598447799682617\n",
      "\t Loss: 22.089887619018555\n",
      "\t Loss: 38.02605438232422\n",
      "Outer loss: 8.059310913085938\n",
      "# 3: outer_epoch:[896] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.408275604248047\n",
      "\t Loss: 5.810839653015137\n",
      "\t Loss: 8.630674362182617\n",
      "\t Loss: 25.179264068603516\n",
      "\t Loss: 50.522823333740234\n",
      "Outer loss: 6.364150524139404\n",
      "# 3: outer_epoch:[897] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.779945373535156\n",
      "\t Loss: 11.431400299072266\n",
      "\t Loss: 8.765000343322754\n",
      "\t Loss: 20.76099395751953\n",
      "\t Loss: 76.88847351074219\n",
      "Outer loss: 30.699581146240234\n",
      "# 3: outer_epoch:[898] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.498693466186523\n",
      "\t Loss: 5.182775497436523\n",
      "\t Loss: 6.706180095672607\n",
      "\t Loss: 11.922001838684082\n",
      "\t Loss: 84.66410827636719\n",
      "Outer loss: 26.162639617919922\n",
      "# 3: outer_epoch:[899] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.12042236328125\n",
      "\t Loss: 10.596185684204102\n",
      "\t Loss: 0.9990614056587219\n",
      "\t Loss: 7.0915117263793945\n",
      "\t Loss: 27.727672576904297\n",
      "Outer loss: 8.676603317260742\n",
      "# 3: outer_epoch:[900] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.67105484008789\n",
      "\t Loss: 3.6555442810058594\n",
      "\t Loss: 0.6889172196388245\n",
      "\t Loss: 9.680554389953613\n",
      "\t Loss: 32.275543212890625\n",
      "Outer loss: 10.496282577514648\n",
      "# 3: outer_epoch:[901] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.973387718200684\n",
      "\t Loss: 4.4143195152282715\n",
      "\t Loss: 1.9892730712890625\n",
      "\t Loss: 10.155759811401367\n",
      "\t Loss: 27.87457847595215\n",
      "Outer loss: 15.549853324890137\n",
      "# 3: outer_epoch:[902] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.656349182128906\n",
      "\t Loss: 8.451899528503418\n",
      "\t Loss: 2.355238914489746\n",
      "\t Loss: 6.204043388366699\n",
      "\t Loss: 29.097476959228516\n",
      "Outer loss: 3.400512933731079\n",
      "# 3: outer_epoch:[903] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.105791091918945\n",
      "\t Loss: 8.300604820251465\n",
      "\t Loss: 0.660210371017456\n",
      "\t Loss: 4.73271369934082\n",
      "\t Loss: 30.244770050048828\n",
      "Outer loss: 13.816764831542969\n",
      "# 3: outer_epoch:[904] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.847033500671387\n",
      "\t Loss: 1.461569905281067\n",
      "\t Loss: 1.846562385559082\n",
      "\t Loss: 5.83940315246582\n",
      "\t Loss: 59.35047149658203\n",
      "Outer loss: 16.59003448486328\n",
      "# 3: outer_epoch:[905] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.358706474304199\n",
      "\t Loss: 6.942769527435303\n",
      "\t Loss: 3.771681785583496\n",
      "\t Loss: 6.382284164428711\n",
      "\t Loss: 24.871963500976562\n",
      "Outer loss: 23.357769012451172\n",
      "# 3: outer_epoch:[906] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.30508041381836\n",
      "\t Loss: 4.953866004943848\n",
      "\t Loss: 4.957074165344238\n",
      "\t Loss: 15.560623168945312\n",
      "\t Loss: 32.144691467285156\n",
      "Outer loss: 3.1067678928375244\n",
      "# 3: outer_epoch:[907] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.382575035095215\n",
      "\t Loss: 3.094069242477417\n",
      "\t Loss: 4.21534538269043\n",
      "\t Loss: 15.263920783996582\n",
      "\t Loss: 35.61138153076172\n",
      "Outer loss: 11.480653762817383\n",
      "# 3: outer_epoch:[908] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.26313018798828\n",
      "\t Loss: 6.067485809326172\n",
      "\t Loss: 8.554815292358398\n",
      "\t Loss: 32.7396354675293\n",
      "\t Loss: 50.34715270996094\n",
      "Outer loss: 16.15178108215332\n",
      "# 3: outer_epoch:[909] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.101482391357422\n",
      "\t Loss: 9.153274536132812\n",
      "\t Loss: 0.35274556279182434\n",
      "\t Loss: 9.238420486450195\n",
      "\t Loss: 26.9339656829834\n",
      "Outer loss: 5.235373497009277\n",
      "# 3: outer_epoch:[910] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.8399658203125\n",
      "\t Loss: 8.930732727050781\n",
      "\t Loss: 0.17406602203845978\n",
      "\t Loss: 7.191399574279785\n",
      "\t Loss: 24.324012756347656\n",
      "Outer loss: 4.848057270050049\n",
      "# 3: outer_epoch:[911] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.317785263061523\n",
      "\t Loss: 7.216887474060059\n",
      "\t Loss: 1.3293322324752808\n",
      "\t Loss: 8.955199241638184\n",
      "\t Loss: 36.830482482910156\n",
      "Outer loss: 4.1513214111328125\n",
      "# 3: outer_epoch:[912] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.238245010375977\n",
      "\t Loss: 5.592328071594238\n",
      "\t Loss: 0.9165538549423218\n",
      "\t Loss: 6.5315046310424805\n",
      "\t Loss: 14.392311096191406\n",
      "Outer loss: 15.00603199005127\n",
      "# 3: outer_epoch:[913] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.193686485290527\n",
      "\t Loss: 2.382932186126709\n",
      "\t Loss: 5.413811206817627\n",
      "\t Loss: 16.488126754760742\n",
      "\t Loss: 6.006855487823486\n",
      "Outer loss: 57.949562072753906\n",
      "# 3: outer_epoch:[914] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.47119903564453\n",
      "\t Loss: 15.114575386047363\n",
      "\t Loss: 6.420902252197266\n",
      "\t Loss: 0.42240437865257263\n",
      "\t Loss: 17.795265197753906\n",
      "Outer loss: 15.958700180053711\n",
      "# 3: outer_epoch:[915] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.23033142089844\n",
      "\t Loss: 9.95444107055664\n",
      "\t Loss: 0.29123103618621826\n",
      "\t Loss: 5.1620378494262695\n",
      "\t Loss: 15.596487045288086\n",
      "Outer loss: 15.133687019348145\n",
      "# 3: outer_epoch:[916] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.708492279052734\n",
      "\t Loss: 4.891891956329346\n",
      "\t Loss: 1.669267177581787\n",
      "\t Loss: 12.947219848632812\n",
      "\t Loss: 23.384883880615234\n",
      "Outer loss: 7.53934383392334\n",
      "# 3: outer_epoch:[917] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.383792877197266\n",
      "\t Loss: 3.7199370861053467\n",
      "\t Loss: 1.099474310874939\n",
      "\t Loss: 4.783926486968994\n",
      "\t Loss: 27.02496910095215\n",
      "Outer loss: 9.497583389282227\n",
      "# 3: outer_epoch:[918] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.338425874710083\n",
      "\t Loss: 7.619333744049072\n",
      "\t Loss: 1.284871220588684\n",
      "\t Loss: 7.6092987060546875\n",
      "\t Loss: 30.19387435913086\n",
      "Outer loss: 21.846097946166992\n",
      "# 3: outer_epoch:[919] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 1.794263243675232\n",
      "\t Loss: 1.5641728639602661\n",
      "\t Loss: 10.737937927246094\n",
      "\t Loss: 48.25130844116211\n",
      "\t Loss: 111.17692565917969\n",
      "Outer loss: 11.044203758239746\n",
      "# 3: outer_epoch:[920] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 10.24574089050293\n",
      "\t Loss: 1.3399755954742432\n",
      "\t Loss: 4.562959671020508\n",
      "\t Loss: 19.930965423583984\n",
      "\t Loss: 32.87675476074219\n",
      "Outer loss: 12.124154090881348\n",
      "# 3: outer_epoch:[921] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.294729232788086\n",
      "\t Loss: 0.8539841175079346\n",
      "\t Loss: 4.779700756072998\n",
      "\t Loss: 20.125398635864258\n",
      "\t Loss: 41.172794342041016\n",
      "Outer loss: 9.870559692382812\n",
      "# 3: outer_epoch:[922] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.228763580322266\n",
      "\t Loss: 0.24045929312705994\n",
      "\t Loss: 6.472080230712891\n",
      "\t Loss: 32.73320007324219\n",
      "\t Loss: 58.00819396972656\n",
      "Outer loss: 5.575455665588379\n",
      "# 3: outer_epoch:[923] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.562861442565918\n",
      "\t Loss: 0.33194053173065186\n",
      "\t Loss: 3.415788173675537\n",
      "\t Loss: 16.872177124023438\n",
      "\t Loss: 69.18402862548828\n",
      "Outer loss: 12.116266250610352\n",
      "# 3: outer_epoch:[924] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.991930961608887\n",
      "\t Loss: 0.818000853061676\n",
      "\t Loss: 5.392505168914795\n",
      "\t Loss: 11.810375213623047\n",
      "\t Loss: 29.619558334350586\n",
      "Outer loss: 20.735212326049805\n",
      "# 3: outer_epoch:[925] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.54542350769043\n",
      "\t Loss: 8.798455238342285\n",
      "\t Loss: 0.3080194890499115\n",
      "\t Loss: 6.500290393829346\n",
      "\t Loss: 29.450654983520508\n",
      "Outer loss: 5.051954746246338\n",
      "# 3: outer_epoch:[926] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 31.199796676635742\n",
      "\t Loss: 6.4563069343566895\n",
      "\t Loss: 0.3901408016681671\n",
      "\t Loss: 4.1639909744262695\n",
      "\t Loss: 40.119266510009766\n",
      "Outer loss: 9.762311935424805\n",
      "# 3: outer_epoch:[927] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.1005744934082\n",
      "\t Loss: 9.125748634338379\n",
      "\t Loss: 3.361629009246826\n",
      "\t Loss: 8.371912002563477\n",
      "\t Loss: 9.499444961547852\n",
      "Outer loss: 15.632770538330078\n",
      "# 3: outer_epoch:[928] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 50.391754150390625\n",
      "\t Loss: 11.997718811035156\n",
      "\t Loss: 3.5899720191955566\n",
      "\t Loss: 1.5967950820922852\n",
      "\t Loss: 8.745922088623047\n",
      "Outer loss: 10.695014953613281\n",
      "# 3: outer_epoch:[929] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.317203521728516\n",
      "\t Loss: 3.2560367584228516\n",
      "\t Loss: 1.4263187646865845\n",
      "\t Loss: 2.624253034591675\n",
      "\t Loss: 15.685096740722656\n",
      "Outer loss: 11.930558204650879\n",
      "# 3: outer_epoch:[930] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.761497497558594\n",
      "\t Loss: 2.9636194705963135\n",
      "\t Loss: 0.8743625283241272\n",
      "\t Loss: 14.166825294494629\n",
      "\t Loss: 17.7705020904541\n",
      "Outer loss: 11.61723518371582\n",
      "# 3: outer_epoch:[931] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.102447509765625\n",
      "\t Loss: 10.356555938720703\n",
      "\t Loss: 0.6908047199249268\n",
      "\t Loss: 12.717209815979004\n",
      "\t Loss: 37.45808410644531\n",
      "Outer loss: 7.303652286529541\n",
      "# 3: outer_epoch:[932] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 29.70295524597168\n",
      "\t Loss: 8.193453788757324\n",
      "\t Loss: 0.23202694952487946\n",
      "\t Loss: 5.544442176818848\n",
      "\t Loss: 22.370590209960938\n",
      "Outer loss: 4.641306400299072\n",
      "# 3: outer_epoch:[933] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.98525047302246\n",
      "\t Loss: 7.986299991607666\n",
      "\t Loss: 1.3004229068756104\n",
      "\t Loss: 6.594885349273682\n",
      "\t Loss: 13.722917556762695\n",
      "Outer loss: 13.352210998535156\n",
      "# 3: outer_epoch:[934] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.304834365844727\n",
      "\t Loss: 3.905160665512085\n",
      "\t Loss: 1.7571121454238892\n",
      "\t Loss: 14.812188148498535\n",
      "\t Loss: 15.872456550598145\n",
      "Outer loss: 32.962955474853516\n",
      "# 3: outer_epoch:[935] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.62289810180664\n",
      "\t Loss: 18.4045352935791\n",
      "\t Loss: 13.752164840698242\n",
      "\t Loss: 15.890954971313477\n",
      "\t Loss: 40.47234344482422\n",
      "Outer loss: 26.525503158569336\n",
      "# 3: outer_epoch:[936] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.325759887695312\n",
      "\t Loss: 1.7918133735656738\n",
      "\t Loss: 1.3568617105484009\n",
      "\t Loss: 14.368023872375488\n",
      "\t Loss: 36.12065505981445\n",
      "Outer loss: 5.599217891693115\n",
      "# 3: outer_epoch:[937] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.666501998901367\n",
      "\t Loss: 1.001185417175293\n",
      "\t Loss: 1.6021535396575928\n",
      "\t Loss: 13.97338581085205\n",
      "\t Loss: 36.0467529296875\n",
      "Outer loss: 17.294391632080078\n",
      "# 3: outer_epoch:[938] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.72593116760254\n",
      "\t Loss: 11.741713523864746\n",
      "\t Loss: 29.783720016479492\n",
      "\t Loss: 56.212039947509766\n",
      "\t Loss: 67.75884246826172\n",
      "Outer loss: 17.69231605529785\n",
      "# 3: outer_epoch:[939] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.899483680725098\n",
      "\t Loss: 9.722270965576172\n",
      "\t Loss: 15.828385353088379\n",
      "\t Loss: 42.50067901611328\n",
      "\t Loss: 57.808902740478516\n",
      "Outer loss: 13.32596206665039\n",
      "# 3: outer_epoch:[940] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.830944061279297\n",
      "\t Loss: 4.337550163269043\n",
      "\t Loss: 2.4433000087738037\n",
      "\t Loss: 6.149422645568848\n",
      "\t Loss: 30.90707778930664\n",
      "Outer loss: 12.01419448852539\n",
      "# 3: outer_epoch:[941] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.131561279296875\n",
      "\t Loss: 6.112826824188232\n",
      "\t Loss: 0.3971693515777588\n",
      "\t Loss: 3.3825109004974365\n",
      "\t Loss: 28.658084869384766\n",
      "Outer loss: 21.80088233947754\n",
      "# 3: outer_epoch:[942] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.960205078125\n",
      "\t Loss: 0.27326613664627075\n",
      "\t Loss: 4.707859992980957\n",
      "\t Loss: 10.765140533447266\n",
      "\t Loss: 58.63872528076172\n",
      "Outer loss: 14.31068229675293\n",
      "# 3: outer_epoch:[943] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.52583122253418\n",
      "\t Loss: 6.395409107208252\n",
      "\t Loss: 3.372042417526245\n",
      "\t Loss: 9.235945701599121\n",
      "\t Loss: 16.505571365356445\n",
      "Outer loss: 21.795988082885742\n",
      "# 3: outer_epoch:[944] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 34.21641159057617\n",
      "\t Loss: 5.912721633911133\n",
      "\t Loss: 1.0395777225494385\n",
      "\t Loss: 5.394887924194336\n",
      "\t Loss: 18.398468017578125\n",
      "Outer loss: 9.759465217590332\n",
      "# 3: outer_epoch:[945] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 43.61260986328125\n",
      "\t Loss: 12.818851470947266\n",
      "\t Loss: 1.5460550785064697\n",
      "\t Loss: 4.565479278564453\n",
      "\t Loss: 16.131746292114258\n",
      "Outer loss: 9.573711395263672\n",
      "# 3: outer_epoch:[946] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.35992431640625\n",
      "\t Loss: 7.650110721588135\n",
      "\t Loss: 1.668503761291504\n",
      "\t Loss: 6.149006366729736\n",
      "\t Loss: 26.96335792541504\n",
      "Outer loss: 7.779555797576904\n",
      "# 3: outer_epoch:[947] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.281596183776855\n",
      "\t Loss: 9.17057991027832\n",
      "\t Loss: 0.8453726768493652\n",
      "\t Loss: 5.419752597808838\n",
      "\t Loss: 42.06728744506836\n",
      "Outer loss: 17.337127685546875\n",
      "# 3: outer_epoch:[948] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.78029727935791\n",
      "\t Loss: 1.937761902809143\n",
      "\t Loss: 3.5383262634277344\n",
      "\t Loss: 7.434816837310791\n",
      "\t Loss: 54.56491470336914\n",
      "Outer loss: 7.979373931884766\n",
      "# 3: outer_epoch:[949] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.30238151550293\n",
      "\t Loss: 4.006304740905762\n",
      "\t Loss: 0.395891398191452\n",
      "\t Loss: 13.294717788696289\n",
      "\t Loss: 21.415273666381836\n",
      "Outer loss: 8.358290672302246\n",
      "# 3: outer_epoch:[950] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.130162239074707\n",
      "\t Loss: 4.879891395568848\n",
      "\t Loss: 0.5756708979606628\n",
      "\t Loss: 7.011409759521484\n",
      "\t Loss: 44.24380874633789\n",
      "Outer loss: 28.93642234802246\n",
      "# 3: outer_epoch:[951] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 2.085184097290039\n",
      "\t Loss: 3.9593348503112793\n",
      "\t Loss: 13.043745040893555\n",
      "\t Loss: 19.919742584228516\n",
      "\t Loss: 72.26981353759766\n",
      "Outer loss: 27.392898559570312\n",
      "# 3: outer_epoch:[952] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.39615249633789\n",
      "\t Loss: 6.028913497924805\n",
      "\t Loss: 7.3071417808532715\n",
      "\t Loss: 24.913928985595703\n",
      "\t Loss: 34.17967224121094\n",
      "Outer loss: 11.873104095458984\n",
      "# 3: outer_epoch:[953] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 15.220683097839355\n",
      "\t Loss: 1.3996870517730713\n",
      "\t Loss: 3.512483596801758\n",
      "\t Loss: 21.521413803100586\n",
      "\t Loss: 31.1627140045166\n",
      "Outer loss: 22.45478057861328\n",
      "# 3: outer_epoch:[954] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 3.5015909671783447\n",
      "\t Loss: 2.9750118255615234\n",
      "\t Loss: 0.7124667167663574\n",
      "\t Loss: 5.144004821777344\n",
      "\t Loss: 26.23672103881836\n",
      "Outer loss: 20.888851165771484\n",
      "# 3: outer_epoch:[955] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.286905765533447\n",
      "\t Loss: 4.9424333572387695\n",
      "\t Loss: 12.686920166015625\n",
      "\t Loss: 22.390155792236328\n",
      "\t Loss: 69.36095428466797\n",
      "Outer loss: 16.031843185424805\n",
      "# 3: outer_epoch:[956] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 5.8666157722473145\n",
      "\t Loss: 3.526093006134033\n",
      "\t Loss: 15.75497817993164\n",
      "\t Loss: 17.63609504699707\n",
      "\t Loss: 54.33209228515625\n",
      "Outer loss: 20.44873046875\n",
      "# 3: outer_epoch:[957] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 13.042655944824219\n",
      "\t Loss: 3.804252862930298\n",
      "\t Loss: 6.784628391265869\n",
      "\t Loss: 24.455934524536133\n",
      "\t Loss: 51.15622329711914\n",
      "Outer loss: 9.954358100891113\n",
      "# 3: outer_epoch:[958] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 4.3279500007629395\n",
      "\t Loss: 2.2068004608154297\n",
      "\t Loss: 6.140230178833008\n",
      "\t Loss: 24.791828155517578\n",
      "\t Loss: 45.821231842041016\n",
      "Outer loss: 7.337435245513916\n",
      "# 3: outer_epoch:[959] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.218696594238281\n",
      "\t Loss: 1.065898060798645\n",
      "\t Loss: 4.760075092315674\n",
      "\t Loss: 23.55258560180664\n",
      "\t Loss: 49.84282684326172\n",
      "Outer loss: 6.257503509521484\n",
      "# 3: outer_epoch:[960] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 8.439413070678711\n",
      "\t Loss: 0.24296869337558746\n",
      "\t Loss: 4.571439743041992\n",
      "\t Loss: 27.825733184814453\n",
      "\t Loss: 61.18061447143555\n",
      "Outer loss: 7.884922027587891\n",
      "# 3: outer_epoch:[961] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.702996253967285\n",
      "\t Loss: 4.44144868850708\n",
      "\t Loss: 4.093867301940918\n",
      "\t Loss: 15.625757217407227\n",
      "\t Loss: 39.9512939453125\n",
      "Outer loss: 9.031975746154785\n",
      "# 3: outer_epoch:[962] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 7.200284957885742\n",
      "\t Loss: 0.5333167314529419\n",
      "\t Loss: 5.952092170715332\n",
      "\t Loss: 35.396949768066406\n",
      "\t Loss: 32.61652755737305\n",
      "Outer loss: 23.44966697692871\n",
      "# 3: outer_epoch:[963] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.878710746765137\n",
      "\t Loss: 3.594616413116455\n",
      "\t Loss: 1.7473338842391968\n",
      "\t Loss: 14.68857192993164\n",
      "\t Loss: 37.031951904296875\n",
      "Outer loss: 11.120604515075684\n",
      "# 3: outer_epoch:[964] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 6.317667007446289\n",
      "\t Loss: 0.9208717346191406\n",
      "\t Loss: 3.1559207439422607\n",
      "\t Loss: 12.320053100585938\n",
      "\t Loss: 20.299081802368164\n",
      "Outer loss: 18.876846313476562\n",
      "# 3: outer_epoch:[965] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.61308288574219\n",
      "\t Loss: 8.122574806213379\n",
      "\t Loss: 0.10303634405136108\n",
      "\t Loss: 8.912288665771484\n",
      "\t Loss: 27.94219398498535\n",
      "Outer loss: 17.863327026367188\n",
      "# 3: outer_epoch:[966] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 27.573089599609375\n",
      "\t Loss: 6.451080799102783\n",
      "\t Loss: 1.1648547649383545\n",
      "\t Loss: 7.793365478515625\n",
      "\t Loss: 27.320898056030273\n",
      "Outer loss: 4.3846821784973145\n",
      "# 3: outer_epoch:[967] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 25.453655242919922\n",
      "\t Loss: 4.8522257804870605\n",
      "\t Loss: 0.10948042571544647\n",
      "\t Loss: 5.052267074584961\n",
      "\t Loss: 24.55906867980957\n",
      "Outer loss: 5.824295997619629\n",
      "# 3: outer_epoch:[968] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.08893585205078\n",
      "\t Loss: 6.278116703033447\n",
      "\t Loss: 0.5674899220466614\n",
      "\t Loss: 3.6368892192840576\n",
      "\t Loss: 9.054206848144531\n",
      "Outer loss: 14.029035568237305\n",
      "# 3: outer_epoch:[969] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 38.050994873046875\n",
      "\t Loss: 12.201410293579102\n",
      "\t Loss: 1.9541144371032715\n",
      "\t Loss: 2.7205238342285156\n",
      "\t Loss: 8.835078239440918\n",
      "Outer loss: 11.753518104553223\n",
      "# 3: outer_epoch:[970] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 63.50865936279297\n",
      "\t Loss: 29.037841796875\n",
      "\t Loss: 3.1083602905273438\n",
      "\t Loss: 0.4493056833744049\n",
      "\t Loss: 6.381206512451172\n",
      "Outer loss: 9.638612747192383\n",
      "# 3: outer_epoch:[971] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 53.7802734375\n",
      "\t Loss: 22.220325469970703\n",
      "\t Loss: 1.9543558359146118\n",
      "\t Loss: 1.4206962585449219\n",
      "\t Loss: 14.754481315612793\n",
      "Outer loss: 3.8337833881378174\n",
      "# 3: outer_epoch:[972] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 16.33062744140625\n",
      "\t Loss: 13.0363130569458\n",
      "\t Loss: 2.751569986343384\n",
      "\t Loss: 1.0781844854354858\n",
      "\t Loss: 7.355346202850342\n",
      "Outer loss: 30.603214263916016\n",
      "# 3: outer_epoch:[973] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 9.886639595031738\n",
      "\t Loss: 1.4880338907241821\n",
      "\t Loss: 5.218528747558594\n",
      "\t Loss: 20.204845428466797\n",
      "\t Loss: 47.611488342285156\n",
      "Outer loss: 4.773221015930176\n",
      "# 3: outer_epoch:[974] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 17.376819610595703\n",
      "\t Loss: 1.564442753791809\n",
      "\t Loss: 3.3306779861450195\n",
      "\t Loss: 10.926913261413574\n",
      "\t Loss: 41.453819274902344\n",
      "Outer loss: 8.567622184753418\n",
      "# 3: outer_epoch:[975] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.65622615814209\n",
      "\t Loss: 1.280470609664917\n",
      "\t Loss: 3.5851402282714844\n",
      "\t Loss: 31.13538360595703\n",
      "\t Loss: 61.45479202270508\n",
      "Outer loss: 16.517436981201172\n",
      "# 3: outer_epoch:[976] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 37.722251892089844\n",
      "\t Loss: 6.598896026611328\n",
      "\t Loss: 0.4166925847530365\n",
      "\t Loss: 7.4920196533203125\n",
      "\t Loss: 17.174842834472656\n",
      "Outer loss: 11.933159828186035\n",
      "# 3: outer_epoch:[977] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 28.45195198059082\n",
      "\t Loss: 13.090374946594238\n",
      "\t Loss: 0.9995930790901184\n",
      "\t Loss: 4.311903953552246\n",
      "\t Loss: 14.407454490661621\n",
      "Outer loss: 8.994024276733398\n",
      "# 3: outer_epoch:[978] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 39.74768829345703\n",
      "\t Loss: 14.907491683959961\n",
      "\t Loss: 0.5807703137397766\n",
      "\t Loss: 2.5607385635375977\n",
      "\t Loss: 21.26118278503418\n",
      "Outer loss: 6.590269565582275\n",
      "# 3: outer_epoch:[979] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 33.57597732543945\n",
      "\t Loss: 12.880105972290039\n",
      "\t Loss: 1.0005102157592773\n",
      "\t Loss: 5.597330093383789\n",
      "\t Loss: 7.231024742126465\n",
      "Outer loss: 16.824235916137695\n",
      "# 3: outer_epoch:[980] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 54.36472702026367\n",
      "\t Loss: 24.387243270874023\n",
      "\t Loss: 4.05044412612915\n",
      "\t Loss: 0.20595818758010864\n",
      "\t Loss: 6.713870525360107\n",
      "Outer loss: 31.692214965820312\n",
      "# 3: outer_epoch:[981] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 19.31926155090332\n",
      "\t Loss: 8.297883987426758\n",
      "\t Loss: 3.303579568862915\n",
      "\t Loss: 18.29959487915039\n",
      "\t Loss: 31.894668579101562\n",
      "Outer loss: 7.61448335647583\n",
      "# 3: outer_epoch:[982] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 11.321035385131836\n",
      "\t Loss: 8.133665084838867\n",
      "\t Loss: 4.299391746520996\n",
      "\t Loss: 27.51871681213379\n",
      "\t Loss: 40.20097732543945\n",
      "Outer loss: 14.111939430236816\n",
      "# 3: outer_epoch:[983] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.787761688232422\n",
      "\t Loss: 8.574411392211914\n",
      "\t Loss: 0.4035601019859314\n",
      "\t Loss: 2.485508918762207\n",
      "\t Loss: 20.733793258666992\n",
      "Outer loss: 10.516375541687012\n",
      "# 3: outer_epoch:[984] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.3304443359375\n",
      "\t Loss: 12.031164169311523\n",
      "\t Loss: 2.707019805908203\n",
      "\t Loss: 10.51612663269043\n",
      "\t Loss: 25.691938400268555\n",
      "Outer loss: 9.440483093261719\n",
      "# 3: outer_epoch:[985] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 30.11781883239746\n",
      "\t Loss: 9.410842895507812\n",
      "\t Loss: 1.2470557689666748\n",
      "\t Loss: 3.6556167602539062\n",
      "\t Loss: 17.990726470947266\n",
      "Outer loss: 6.666422367095947\n",
      "# 3: outer_epoch:[986] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 23.49601936340332\n",
      "\t Loss: 5.70491361618042\n",
      "\t Loss: 0.38398435711860657\n",
      "\t Loss: 8.165838241577148\n",
      "\t Loss: 20.011375427246094\n",
      "Outer loss: 9.961918830871582\n",
      "# 3: outer_epoch:[987] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 21.20703887939453\n",
      "\t Loss: 6.342006206512451\n",
      "\t Loss: 0.48754578828811646\n",
      "\t Loss: 9.022777557373047\n",
      "\t Loss: 25.966867446899414\n",
      "Outer loss: 4.179885387420654\n",
      "# 3: outer_epoch:[988] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 35.06901168823242\n",
      "\t Loss: 6.482071876525879\n",
      "\t Loss: 0.2686176598072052\n",
      "\t Loss: 6.856286525726318\n",
      "\t Loss: 22.218538284301758\n",
      "Outer loss: 14.099839210510254\n",
      "# 3: outer_epoch:[989] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 20.290983200073242\n",
      "\t Loss: 2.5889148712158203\n",
      "\t Loss: 1.6313846111297607\n",
      "\t Loss: 10.933303833007812\n",
      "\t Loss: 16.31243896484375\n",
      "Outer loss: 26.41997718811035\n",
      "# 3: outer_epoch:[990] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 24.812946319580078\n",
      "\t Loss: 13.3158597946167\n",
      "\t Loss: 1.1756000518798828\n",
      "\t Loss: 3.8543972969055176\n",
      "\t Loss: 7.842094421386719\n",
      "Outer loss: 13.853323936462402\n",
      "# 3: outer_epoch:[991] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 53.13203811645508\n",
      "\t Loss: 25.265960693359375\n",
      "\t Loss: 7.238772869110107\n",
      "\t Loss: 0.25136032700538635\n",
      "\t Loss: 4.167259693145752\n",
      "Outer loss: 10.910110473632812\n",
      "# 3: outer_epoch:[992] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 42.31187438964844\n",
      "\t Loss: 16.02020263671875\n",
      "\t Loss: 6.157371997833252\n",
      "\t Loss: 1.6565864086151123\n",
      "\t Loss: 14.741399765014648\n",
      "Outer loss: 12.460226058959961\n",
      "# 3: outer_epoch:[993] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 36.57140350341797\n",
      "\t Loss: 7.25877571105957\n",
      "\t Loss: 1.1932384967803955\n",
      "\t Loss: 4.33683967590332\n",
      "\t Loss: 26.39615821838379\n",
      "Outer loss: 12.013307571411133\n",
      "# 3: outer_epoch:[994] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.912714004516602\n",
      "\t Loss: 6.991245269775391\n",
      "\t Loss: 1.093332052230835\n",
      "\t Loss: 3.8529958724975586\n",
      "\t Loss: 18.701078414916992\n",
      "Outer loss: 13.228012084960938\n",
      "# 3: outer_epoch:[995] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 14.082627296447754\n",
      "\t Loss: 3.8553221225738525\n",
      "\t Loss: 1.0104409456253052\n",
      "\t Loss: 5.465877056121826\n",
      "\t Loss: 27.02218246459961\n",
      "Outer loss: 9.893936157226562\n",
      "# 3: outer_epoch:[996] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 18.27398109436035\n",
      "\t Loss: 5.384052753448486\n",
      "\t Loss: 1.3374799489974976\n",
      "\t Loss: 5.035479545593262\n",
      "\t Loss: 14.804229736328125\n",
      "Outer loss: 7.555955410003662\n",
      "# 3: outer_epoch:[997] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.309073448181152\n",
      "\t Loss: 2.384305000305176\n",
      "\t Loss: 0.327966570854187\n",
      "\t Loss: 14.603468894958496\n",
      "\t Loss: 39.11689758300781\n",
      "Outer loss: 9.051733016967773\n",
      "# 3: outer_epoch:[998] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 12.860684394836426\n",
      "\t Loss: 1.8989708423614502\n",
      "\t Loss: 2.16215181350708\n",
      "\t Loss: 13.84472370147705\n",
      "\t Loss: 49.62301254272461\n",
      "Outer loss: 6.152902603149414\n",
      "# 3: outer_epoch:[999] - sampled task batch: -([(7, 3), (6, 2), (5, 1), (4, 0), (3, -1)])-\n",
      "\t Loss: 22.330078125\n",
      "\t Loss: 6.418604850769043\n",
      "\t Loss: 1.8675100803375244\n",
      "\t Loss: 9.528450012207031\n",
      "\t Loss: 24.94621467590332\n",
      "Outer loss: 5.595783233642578\n",
      "# 11: end for\n"
     ]
    }
   ],
   "source": [
    "P('# 1: randomly initialize theta ')\n",
    "theta = NN()\n",
    "theta.info( show_vals=True, P=P)\n",
    "\n",
    "P('# 2: while not done ', outer_epochs )\n",
    "for outer_epoch in range(outer_epochs):\n",
    "\n",
    "    # 3: sample a batch of taskss\n",
    "    task_batch = [ taskerL[i%len(taskerL)] for i in range(task_batch_size) ]\n",
    "    P('# 3: outer_epoch:[{}] - sampled task batch: -({})-'.format(outer_epoch,task_batch))\n",
    "    \n",
    "    #P('# 4: for each task do')\n",
    "    thetaL, dbL = [], []\n",
    "    for tasker in task_batch:\n",
    "\n",
    "        #P('# 5: sample train_K data points', train_K)\n",
    "        train_db =tasker.sample(train_K)\n",
    "        batch_x = torch.tensor(np.expand_dims(train_db[ :, 0 ],axis=-1), dtype=torch.float32)\n",
    "        batch_y = torch.tensor(np.expand_dims(train_db[ :, 1 ],axis=-1), dtype=torch.float32)\n",
    "\n",
    "        #P('# 6: evaluate grad_theta(loss)')\n",
    "        #theta.zero_grad()\n",
    "        pred = theta.forward(batch_x)\n",
    "        loss =  lossM(pred, batch_y) #torch.sum((pred - batch_y) ** 2) \n",
    "        P('\\t Loss:', loss.item())\n",
    "        #loss.backward(create_graph=True)\n",
    "        grads = torch.autograd.grad(loss, theta.parameters, create_graph=True)\n",
    "\n",
    "        #P('# 7: compute adapted paramters with grad descent')\n",
    "        theta_i = [] \n",
    "        for t_param, grad in zip(theta.parameters, grads):\n",
    "            i_param = t_param - inner_lr * grad\n",
    "            theta_i.append(i_param)\n",
    "        thetaL.append(theta_i)\n",
    "        theta.zero_grad()\n",
    "\n",
    "        #P('# 8: sample test_K data points', test_K)\n",
    "        test_db =tasker.sample(test_K)\n",
    "        dbL.append(test_db)\n",
    "\n",
    "    #P('# 9: end for (tasker)')\n",
    "\n",
    "    #P('# 10: meta update')\n",
    "    # grad_theta ( sum of all looses )\n",
    "    oL = []\n",
    "    for Ti, Di in zip(thetaL, dbL):\n",
    "        dx = torch.tensor(np.expand_dims(Di[ :, 0 ],axis=-1), dtype=torch.float32)\n",
    "        dy = torch.tensor(np.expand_dims(Di[ :, 1 ],axis=-1), dtype=torch.float32)\n",
    "\n",
    "        temp = NP(Ti)\n",
    "        xpred = temp.forward(dx)\n",
    "        xloss =  lossM(xpred, dy) #torch.sum((xpred - dy) ** 2) \n",
    "        oL.append(xloss)\n",
    "        \n",
    "    oloss = torch.sum(torch.stack(oL))\n",
    "    ograds = torch.autograd.grad(oloss, theta.parameters, create_graph=False)\n",
    "    print('Outer loss:', oloss.item())\n",
    "    #print('Outer grads:', len(ograds))\n",
    "    \n",
    "    #P('... update outer params')\n",
    "    with torch.no_grad():\n",
    "        #grad_sum = torch.sum(ograds)\n",
    "        #P('Outer-Grads:', grad_sum.item())\n",
    "        for t_param, grad in zip(theta.parameters, ograds):\n",
    "            #print( 'grad-shapes', t_param , grad )\n",
    "            t_param -= outer_lr * grad\n",
    "            #print( 'After', t_param )\n",
    "\n",
    "P('# 11: end for')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31bbaf56-c52c-40d0-b23a-b383620a3343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "~ N_LAYERS:[4]\n",
      "~ D_TYPE:[torch.float32]\n",
      "~ DEV:[cpu]\n",
      "--------------------------\n",
      "--> Weights[0]:: Params[25] of Shape[torch.Size([25, 1])]\n",
      "--> Bias[0]:: Params[25] of Shape[torch.Size([25])]\n",
      "--> Weights[1]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      "--> Bias[1]:: Params[25] of Shape[torch.Size([25])]\n",
      "--> Weights[2]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      "--> Bias[2]:: Params[25] of Shape[torch.Size([25])]\n",
      "--> Weights[3]:: Params[25] of Shape[torch.Size([1, 25])]\n",
      "--> Bias[3]:: Params[1] of Shape[torch.Size([1])]\n",
      "--------------------------\n",
      "PARAMS:\t 1,376\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "theta.save_external('meta_pie')\n",
    "theta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8ffd29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "~ N_LAYERS:[4]\n",
      "~ D_TYPE:[torch.float32]\n",
      "~ DEV:[cpu]\n",
      "--------------------------\n",
      "--> Weights[0]:: Params[25] of Shape[torch.Size([25, 1])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[ 2.4490e-03],\n",
      "        [-8.0139e-02],\n",
      "        [-2.5064e-01],\n",
      "        [ 9.5909e-01],\n",
      "        [-6.5284e-05],\n",
      "        [ 4.8062e-01],\n",
      "        [-8.6809e-02],\n",
      "        [ 6.5355e-03],\n",
      "        [ 4.4192e-01],\n",
      "        [-7.2053e-01],\n",
      "        [ 8.9951e-01],\n",
      "        [-1.7595e-02],\n",
      "        [-2.0167e-02],\n",
      "        [-1.3581e-01],\n",
      "        [-2.8878e-01],\n",
      "        [-5.0440e-02],\n",
      "        [-5.0478e-02],\n",
      "        [-2.3697e-02],\n",
      "        [-6.8030e-02],\n",
      "        [-4.1850e-01],\n",
      "        [-2.6412e-02],\n",
      "        [ 5.6413e-04],\n",
      "        [-8.6678e-02],\n",
      "        [-4.2174e-03],\n",
      "        [-2.6082e-01]], requires_grad=True)\n",
      "--> Bias[0]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([-1.6234e-02, -3.8518e-02,  9.0392e-01, -2.0014e+00, -2.5732e-02,\n",
      "        -1.0545e+00, -9.3511e-02, -6.5731e-02, -9.8871e-01,  6.8273e-01,\n",
      "        -1.7839e+00, -8.1661e-02,  5.6823e-01,  7.8756e-02,  4.3464e-01,\n",
      "        -9.9774e-02, -1.8605e-04, -6.2873e-02, -9.0278e-02,  2.4809e-01,\n",
      "        -2.9483e-03, -1.6047e-02,  2.6356e-04, -2.3244e-02,  3.8612e-01],\n",
      "       requires_grad=True)\n",
      "--> Weights[1]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[-3.2441e-03, -2.7398e-02,  2.0228e-01,  5.3703e-01,  3.8126e-02,\n",
      "          3.4067e-01, -9.7550e-02,  5.1238e-01,  4.5852e-01, -3.9229e-02,\n",
      "          3.0513e-01,  4.4238e-02,  1.1977e+00, -9.7517e-02, -1.8755e-01,\n",
      "          5.0656e-02, -8.3847e-02, -6.1406e-02,  4.0292e-02, -1.5165e-01,\n",
      "         -3.2650e-02, -4.9522e-02,  3.7706e-02,  3.9754e-02,  2.1187e-02],\n",
      "        [ 1.8606e-02,  9.2919e-02,  2.3034e-01, -6.9054e-02, -1.6036e-03,\n",
      "         -6.1865e-02,  7.6423e-02,  2.6265e-01,  6.5163e-02, -5.3550e-01,\n",
      "         -1.5595e-01,  6.0927e-02,  3.7047e-01, -1.0386e-01, -1.2912e-01,\n",
      "          6.5205e-02,  5.3501e-02, -1.2080e-03, -9.5052e-02, -8.0428e-02,\n",
      "         -9.0962e-02,  1.1174e-02, -7.5764e-02, -5.0381e-02, -3.0405e-01],\n",
      "        [-5.5706e-02, -7.7229e-02,  2.2102e-01, -1.4679e-01,  5.7844e-02,\n",
      "          1.5545e-02, -2.4349e-02,  2.0313e-01,  3.0776e-03, -7.2659e-01,\n",
      "         -2.2354e-01, -7.5699e-02,  5.8315e-01, -6.2229e-02, -1.8841e-01,\n",
      "         -1.7566e-02, -1.0608e-02, -9.7016e-02, -6.3809e-02, -2.6485e-01,\n",
      "         -8.4489e-03,  5.5774e-02,  2.1198e-02,  2.2563e-03, -2.9919e-01],\n",
      "        [-7.8399e-02, -7.2435e-02,  2.4024e-02,  1.2499e-01, -2.7012e-02,\n",
      "          1.2446e-01, -5.9448e-03,  1.1937e-01,  1.0088e-01, -9.9496e-03,\n",
      "          3.1078e-02, -3.8101e-02,  3.3386e-01, -3.0796e-02, -1.0299e-01,\n",
      "         -8.6111e-02, -3.4711e-02, -5.2804e-02, -7.4622e-02, -7.9839e-02,\n",
      "         -9.7098e-02,  7.1147e-02,  7.4684e-02,  3.3985e-02,  2.4287e-02],\n",
      "        [-5.6565e-02,  5.2428e-02,  2.8124e-02,  1.3464e-02, -6.6391e-03,\n",
      "          1.0426e-01, -9.4427e-03,  1.0493e-01,  2.7870e-02, -4.3135e-02,\n",
      "          5.6682e-02, -8.0181e-02,  1.0881e-01, -1.1891e-02,  4.8651e-02,\n",
      "         -8.3377e-04,  8.2396e-02, -4.2061e-02, -4.5173e-02, -7.1457e-02,\n",
      "         -3.5258e-02, -3.2888e-02,  3.6721e-02, -5.3033e-02, -7.2604e-02],\n",
      "        [-7.6334e-02,  3.5783e-02,  1.5225e-01,  1.5484e-02,  9.9333e-02,\n",
      "         -3.8155e-02,  3.8441e-02, -2.0467e-01, -3.3935e-02,  1.5032e-01,\n",
      "          3.3382e-02, -2.6954e-02, -2.1050e-01, -3.4073e-02,  2.2713e-01,\n",
      "         -4.9366e-02, -3.8211e-02,  3.7346e-02, -5.7875e-02,  5.7625e-02,\n",
      "          6.5641e-02,  3.1216e-02, -8.6674e-02, -9.5993e-02,  9.0411e-02],\n",
      "        [-4.1554e-02, -4.7265e-02,  7.9920e-02, -1.0467e-01, -1.8815e-02,\n",
      "         -2.6154e-02, -8.6303e-02, -1.3292e-01,  1.2234e-02, -8.4057e-03,\n",
      "          3.2245e-02,  2.4130e-02, -5.0449e-02,  9.6944e-03,  1.3608e-01,\n",
      "          1.9685e-02, -8.5236e-02, -3.3794e-02, -1.7468e-02,  1.2059e-01,\n",
      "          6.1303e-03,  6.8077e-02, -7.8395e-02, -6.0201e-02,  4.7495e-02],\n",
      "        [-9.2410e-02, -2.6868e-03,  1.4205e-01, -1.7656e-01, -3.1792e-02,\n",
      "         -1.6690e-01, -2.4860e-02,  1.4624e-01, -8.5692e-02, -3.8334e-01,\n",
      "         -2.4149e-01,  9.1854e-02,  5.2238e-01, -8.0857e-03, -9.0398e-02,\n",
      "         -2.2750e-02,  3.3725e-02,  9.3468e-03,  6.4394e-02, -1.6335e-01,\n",
      "          1.8046e-03, -3.5339e-02,  5.0147e-02,  5.1674e-02, -2.0330e-01],\n",
      "        [-1.8771e-02, -5.5804e-02, -6.4812e-02, -3.3958e-02,  2.5089e-02,\n",
      "         -5.3136e-03, -5.8643e-03, -8.2165e-02, -8.1069e-02,  1.2592e-02,\n",
      "         -8.8828e-02, -5.7666e-02,  7.4702e-02, -8.7020e-02, -7.0952e-02,\n",
      "          7.4554e-02,  6.0279e-03, -9.3226e-03,  2.8583e-02, -5.3235e-02,\n",
      "         -3.9480e-02,  2.0646e-02,  9.8936e-02, -4.0145e-02, -4.4967e-02],\n",
      "        [-9.6615e-02, -1.9199e-02,  5.9957e-02,  7.6466e-02,  8.4735e-02,\n",
      "          4.1646e-02,  4.1632e-02,  2.6923e-02, -5.3247e-02,  6.3634e-02,\n",
      "          6.8825e-03, -8.5060e-02,  1.2913e-01, -4.5242e-02, -7.8962e-02,\n",
      "          8.7910e-02, -2.7140e-02, -4.3128e-02,  1.0240e-02, -9.4923e-02,\n",
      "         -1.5399e-02,  9.2093e-03, -3.5381e-03, -8.9515e-02,  7.8262e-02],\n",
      "        [ 7.4532e-02,  4.8210e-02, -3.1434e-02, -1.0838e-01,  9.6856e-02,\n",
      "         -7.7035e-02,  6.9430e-02,  1.1554e-01,  4.1196e-02, -2.4511e-01,\n",
      "         -1.6806e-01, -9.8011e-02,  2.3502e-01, -7.4104e-02, -9.4271e-02,\n",
      "          3.6167e-02,  7.2018e-02, -1.2634e-02,  6.6560e-02, -1.2567e-01,\n",
      "          4.3391e-03, -5.7993e-03,  8.5929e-02, -4.8306e-02, -7.0232e-02],\n",
      "        [ 8.4810e-02,  2.5284e-02,  1.1528e-01,  8.8354e-04,  7.6057e-02,\n",
      "         -4.4947e-02, -1.1494e-02,  1.6369e-03, -9.7296e-02, -1.2680e-01,\n",
      "          2.8454e-02, -9.3591e-02,  8.5228e-02,  7.3888e-02, -8.0722e-02,\n",
      "          9.4536e-02, -9.0288e-02,  9.7775e-02,  8.2260e-02,  1.2404e-02,\n",
      "          2.5346e-02, -2.9353e-03,  2.4367e-02,  6.2953e-02, -3.7152e-02],\n",
      "        [ 5.1539e-02, -3.0100e-02, -1.0532e-01, -1.5735e-01, -6.5274e-02,\n",
      "         -2.8376e-02, -3.8758e-02,  2.3461e-01, -4.4894e-02, -1.1831e-01,\n",
      "         -1.8241e-01,  1.3792e-02,  2.5543e-01, -7.2757e-02, -2.3017e-01,\n",
      "         -6.6551e-03, -2.8197e-02, -8.7925e-02,  7.7717e-02, -3.5581e-03,\n",
      "          7.3145e-03, -7.2894e-02,  3.2344e-02, -6.7215e-02, -1.0458e-01],\n",
      "        [-9.5529e-02, -1.0359e-02,  5.6385e-02,  6.0644e-02, -4.4321e-02,\n",
      "         -1.0690e-01,  5.3439e-02, -3.2814e-02, -9.7651e-02,  3.9569e-02,\n",
      "          3.4600e-02,  8.0584e-03,  3.5520e-02, -2.6837e-02, -1.5254e-02,\n",
      "          7.6546e-02, -4.3858e-02, -3.0213e-02,  9.1990e-02,  2.3122e-02,\n",
      "          3.0120e-02, -9.7045e-02, -1.1634e-02,  8.4084e-02,  4.1789e-02],\n",
      "        [ 1.2461e-02,  8.4201e-02,  7.9750e-02, -3.2023e-02, -6.0487e-02,\n",
      "         -1.0906e-01, -6.8668e-02, -2.7978e-01, -6.6772e-02,  2.9363e-01,\n",
      "         -2.1322e-01,  8.8434e-02, -3.9272e-01,  6.0814e-02,  1.8027e-01,\n",
      "          2.1731e-02, -8.8833e-02,  2.4384e-02,  4.5271e-03,  1.4960e-01,\n",
      "         -8.6444e-02, -9.1016e-02,  2.9062e-03, -3.2179e-02,  3.2666e-02],\n",
      "        [-4.0384e-02,  8.2520e-02,  4.5112e-03, -7.3119e-02,  1.1927e-02,\n",
      "          9.4566e-02,  7.8974e-02, -8.4549e-02, -4.1831e-02,  8.4777e-02,\n",
      "          1.5814e-02, -8.4069e-02,  6.0323e-02, -6.0845e-02, -2.3737e-02,\n",
      "         -9.2191e-02,  9.0478e-02, -5.6262e-02,  2.2819e-02,  5.1238e-02,\n",
      "         -2.6716e-03, -9.0380e-02, -9.8094e-02,  1.5185e-02, -8.8012e-02],\n",
      "        [-5.7917e-04,  2.2064e-02, -2.9789e-02,  1.5799e-02, -7.0367e-02,\n",
      "          1.2505e-02,  3.2495e-02, -1.8932e-02, -1.0928e-02, -5.2467e-02,\n",
      "         -4.8016e-02,  1.2255e-03, -2.0503e-02, -5.6218e-02,  7.4898e-02,\n",
      "         -3.9708e-02,  8.8809e-02,  4.1505e-02, -2.1289e-03, -9.9587e-02,\n",
      "          2.5652e-02, -7.1307e-02, -2.6554e-02, -6.1767e-02, -7.8872e-02],\n",
      "        [ 8.5591e-02,  1.4436e-02, -2.3803e-03,  1.0885e-02, -7.7000e-03,\n",
      "         -8.4693e-02,  9.0971e-02,  6.5432e-02, -7.0993e-02, -2.6732e-02,\n",
      "          1.0035e-02, -8.9535e-03,  5.6209e-02,  6.9487e-02,  7.8989e-02,\n",
      "          2.9797e-02, -4.8560e-03,  7.3289e-02,  6.6147e-02,  1.7131e-04,\n",
      "          2.2512e-02,  8.6048e-02,  3.3700e-02,  5.3611e-02, -8.3982e-02],\n",
      "        [ 7.5052e-02, -7.8670e-02, -6.4058e-02, -3.1819e-02,  8.2438e-02,\n",
      "         -2.3908e-02, -7.7914e-02,  3.7227e-02, -5.9076e-02,  9.4183e-02,\n",
      "         -3.2020e-02,  6.9097e-02, -7.1383e-02, -3.4692e-02, -2.8704e-03,\n",
      "         -5.7884e-02, -9.8163e-02,  5.6713e-02, -7.7431e-02,  3.9948e-02,\n",
      "          3.2014e-02, -5.9516e-02, -1.0407e-02, -9.0535e-02, -9.6042e-02],\n",
      "        [-6.3657e-02,  5.4487e-02,  6.8131e-02, -1.5374e-01, -8.4490e-02,\n",
      "          3.2927e-05,  2.1883e-02,  5.0594e-02, -7.8702e-02, -2.7147e-01,\n",
      "         -1.3533e-01, -8.5112e-02,  3.3508e-01, -1.1166e-02, -8.5772e-02,\n",
      "         -4.7636e-02,  9.8191e-02,  2.1908e-02,  4.2579e-02, -9.2032e-02,\n",
      "         -4.1823e-02,  3.4848e-02, -4.8006e-02, -6.2592e-02, -1.9213e-01],\n",
      "        [ 5.3605e-02,  6.7413e-02,  3.0090e-01, -1.3988e-01, -5.0309e-02,\n",
      "         -6.6568e-02,  8.7307e-02, -4.8663e-02,  1.2059e-02, -1.3923e-01,\n",
      "         -7.8952e-02,  9.6259e-03, -2.5890e-03, -1.7473e-02,  9.7651e-02,\n",
      "         -4.3373e-02,  4.2821e-02, -9.2288e-02, -4.2537e-02,  1.5541e-02,\n",
      "         -7.3545e-02, -8.2196e-02,  1.3318e-02,  6.2837e-03,  5.1226e-02],\n",
      "        [-5.9813e-02, -9.0109e-02, -3.2512e-02,  2.0227e-01,  3.2034e-02,\n",
      "          2.0020e-02, -2.7093e-02,  2.0659e-01,  1.0848e-01, -5.4746e-02,\n",
      "          3.1608e-02,  9.1796e-02,  2.4047e-01,  5.4625e-02, -2.9658e-02,\n",
      "         -4.1132e-02, -9.4223e-02,  5.2047e-04, -3.7662e-02, -7.4935e-02,\n",
      "         -5.0876e-02,  5.9634e-02, -2.4558e-02, -9.4866e-03,  7.8553e-02],\n",
      "        [ 2.6806e-02,  5.7516e-02, -8.0094e-02, -5.8093e-02, -1.3786e-02,\n",
      "         -7.8960e-02, -2.4646e-03, -3.0701e-02,  3.1598e-02, -7.3446e-02,\n",
      "         -1.4709e-02, -4.6247e-02,  2.8349e-02,  4.1414e-02,  4.0125e-03,\n",
      "         -5.2179e-02,  6.2004e-02, -3.2485e-02,  2.7070e-02,  2.2257e-02,\n",
      "          3.3905e-03, -1.0713e-02,  9.6776e-02,  3.5193e-02, -6.3542e-02],\n",
      "        [-2.9291e-02, -6.3834e-02,  9.5570e-02, -8.6768e-02, -8.1809e-02,\n",
      "          1.4874e-02,  1.4032e-02, -7.0339e-02, -6.6964e-02,  1.2611e-02,\n",
      "         -4.1327e-03,  9.3172e-02,  1.4367e-01, -1.6529e-02, -1.4934e-02,\n",
      "         -6.1001e-02,  6.3554e-02, -3.0618e-02, -5.6819e-02, -9.2199e-02,\n",
      "         -1.1182e-02, -6.5433e-02,  6.5418e-02, -8.3650e-02, -4.8530e-02],\n",
      "        [ 6.3898e-02,  9.0729e-02,  2.9415e-01, -8.8519e-02,  9.6876e-02,\n",
      "          4.3930e-02,  6.3055e-02,  1.1790e-01,  5.6259e-02, -2.7443e-01,\n",
      "         -2.2808e-01,  9.6945e-02,  2.2288e-01, -1.7684e-02, -1.3554e-01,\n",
      "          4.0392e-03, -1.8176e-02, -7.7560e-02,  4.9081e-02, -2.5721e-02,\n",
      "          3.6648e-02,  3.6920e-02,  7.5165e-03, -2.2634e-02, -1.1739e-01]],\n",
      "       requires_grad=True)\n",
      "--> Bias[1]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([-1.4407,  0.8679,  1.3159, -0.2185, -0.0897,  1.0488, -0.0069,  0.3703,\n",
      "         0.0021, -0.1110,  0.2108,  0.1267,  0.0184, -0.0763,  0.2048, -0.0706,\n",
      "        -0.0775, -0.0658, -0.0751,  0.0259,  1.1090, -0.3283, -0.0334,  0.0213,\n",
      "         1.1093], requires_grad=True)\n",
      "--> Weights[2]:: Params[625] of Shape[torch.Size([25, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[-5.0459e-01,  3.3349e-01,  3.4812e-01, -1.2355e-01, -6.8806e-02,\n",
      "          2.3567e-01, -1.1106e-01,  2.4948e-01, -2.7753e-02, -3.1948e-02,\n",
      "          2.3435e-01,  9.3178e-02,  1.7613e-01,  6.8743e-02, -2.0271e-01,\n",
      "         -4.6125e-02,  5.6945e-02,  3.8831e-02, -6.1837e-02,  1.8470e-01,\n",
      "          3.2486e-01, -1.5232e-01, -6.6941e-02,  1.2005e-01,  3.3598e-01],\n",
      "        [ 3.5197e-01, -2.4106e-01, -2.9477e-01,  1.4221e-01,  6.1004e-02,\n",
      "         -2.7343e-01,  9.8743e-02,  4.3516e-02, -6.5963e-02, -3.0343e-02,\n",
      "          6.1212e-02, -8.1006e-02, -1.0045e-01, -4.1095e-03,  1.5426e-02,\n",
      "         -4.1988e-02,  3.4459e-02,  6.4845e-02, -6.5458e-02,  9.3369e-03,\n",
      "         -2.1114e-01,  1.6136e-01, -3.7854e-02, -6.6938e-02, -1.4714e-01],\n",
      "        [-7.4726e-02, -7.3484e-02, -7.2044e-02, -7.1722e-02,  7.0408e-02,\n",
      "         -3.2759e-02,  4.5070e-02, -9.9326e-02, -1.5752e-02, -5.7176e-02,\n",
      "          9.8639e-02, -2.3909e-02, -2.8338e-02, -9.8931e-02, -4.4261e-02,\n",
      "         -1.4025e-02,  7.4345e-02, -2.9533e-02, -8.5300e-02,  1.9670e-02,\n",
      "         -2.8276e-02,  7.5684e-02,  3.5287e-02, -2.5301e-02,  5.4369e-02],\n",
      "        [-4.9042e-02,  1.2036e-01,  2.1938e-03, -1.7280e-02,  6.9543e-02,\n",
      "         -1.4027e-02, -7.2585e-02,  2.7376e-02,  9.4066e-02, -4.6599e-02,\n",
      "         -3.8011e-02, -7.6822e-02,  1.0332e-01,  1.5169e-02, -6.0449e-02,\n",
      "         -2.7569e-02, -2.3940e-02,  2.7682e-02,  4.3062e-02,  6.1208e-02,\n",
      "          8.3124e-02,  6.8307e-02, -9.0430e-02,  5.7083e-02, -1.2967e-03],\n",
      "        [-2.4894e-01,  5.9067e-02,  2.5485e-01, -5.8060e-02,  5.6619e-02,\n",
      "          1.7636e-01,  5.1284e-02,  2.1929e-01, -6.6549e-02, -8.6651e-02,\n",
      "          2.4581e-02, -5.2329e-02,  2.1145e-01, -7.0231e-02, -2.0800e-01,\n",
      "          7.4181e-02,  8.6789e-02,  2.8766e-02, -3.7516e-02,  2.8407e-02,\n",
      "          5.5690e-02, -1.4353e-01, -6.4807e-02,  1.1397e-01,  1.2708e-01],\n",
      "        [-6.3072e-02, -4.5073e-02,  7.5282e-02, -8.9508e-02, -8.6299e-02,\n",
      "          5.1294e-02, -5.9792e-02, -9.2680e-02, -8.6551e-02,  3.7708e-02,\n",
      "          3.5465e-02,  1.5965e-02,  2.3906e-02, -1.6923e-02,  7.4915e-02,\n",
      "         -2.7736e-02, -9.8958e-02,  1.1097e-02, -9.8328e-02, -5.3026e-02,\n",
      "         -2.1560e-03, -8.9600e-03,  8.0601e-02,  5.2412e-02, -5.5382e-02],\n",
      "        [ 2.0792e-01, -2.7575e-02, -9.3752e-02,  1.0473e-02,  9.6465e-02,\n",
      "         -1.7444e-01, -5.3089e-02,  9.2179e-02,  2.2395e-02, -2.6874e-02,\n",
      "          4.9829e-02,  2.7488e-02,  3.7837e-02, -7.8237e-02,  7.9366e-02,\n",
      "         -3.9629e-02, -7.0831e-02,  3.3138e-03,  5.2318e-02,  4.4990e-03,\n",
      "         -1.9892e-01,  1.2695e-01,  2.6302e-02,  8.1545e-02, -1.5680e-01],\n",
      "        [-7.6958e-02,  3.8645e-02, -9.0528e-02, -9.3693e-02,  2.5614e-02,\n",
      "          1.2115e-02,  5.9839e-02, -5.7479e-02,  9.9673e-02,  1.2197e-02,\n",
      "         -6.4529e-03, -2.6518e-02, -5.9029e-02, -7.4213e-02,  6.9422e-02,\n",
      "         -5.0097e-02, -6.7504e-02,  1.4231e-02,  6.0504e-02,  5.0405e-02,\n",
      "          6.5692e-03,  3.2962e-02,  2.2410e-02, -7.1360e-02,  5.2297e-02],\n",
      "        [-8.3814e-02,  1.7135e-01,  2.2980e-01, -6.3025e-02, -7.4398e-02,\n",
      "         -2.0379e-01,  1.8735e-02,  1.8275e-01,  1.0146e-01, -5.6478e-02,\n",
      "          1.5182e-01,  3.9929e-02,  9.9777e-02, -5.5516e-02, -3.9198e-02,\n",
      "          3.0481e-02, -9.9263e-02,  4.2761e-02,  1.3109e-02,  1.6619e-01,\n",
      "         -1.5339e-01, -6.4192e-02,  4.0334e-03,  9.0985e-04, -4.9384e-02],\n",
      "        [-5.5874e-01,  2.9423e-01,  4.6674e-01, -1.4625e-01, -3.9657e-02,\n",
      "          5.4028e-02,  1.9303e-02,  2.9653e-01, -9.3375e-02,  4.0708e-02,\n",
      "          1.6270e-01,  2.3030e-02,  3.0473e-01,  1.4244e-02, -2.5676e-01,\n",
      "         -1.5685e-02,  1.6471e-02,  2.6924e-02,  4.4688e-02,  2.0928e-01,\n",
      "          5.8141e-02, -1.2399e-01, -4.6016e-02,  8.1167e-02,  3.0812e-01],\n",
      "        [-1.8009e-01,  9.2264e-02,  1.9126e-01, -1.1956e-01,  5.1424e-02,\n",
      "         -1.2610e-01,  5.4834e-02,  1.7828e-01, -3.8766e-02,  8.2552e-02,\n",
      "          1.5973e-02,  1.1446e-02,  8.3925e-02,  9.1630e-02, -4.9195e-02,\n",
      "         -9.8945e-02, -3.1000e-02, -5.0575e-02,  6.6974e-02,  5.7900e-02,\n",
      "         -3.9594e-02,  5.5491e-02, -7.0036e-02, -5.0183e-02,  3.1797e-02],\n",
      "        [-7.6238e-02, -7.5717e-02, -7.5179e-03,  9.8299e-02,  7.4386e-02,\n",
      "         -4.2711e-02,  4.7045e-02, -9.2262e-02, -1.6142e-02, -4.1139e-02,\n",
      "          1.7397e-02,  5.6110e-02,  3.3996e-02, -7.1122e-02,  8.9194e-02,\n",
      "         -4.8352e-03, -8.8916e-02, -2.7892e-02, -2.2227e-02, -9.0219e-02,\n",
      "         -7.0836e-02,  1.0709e-02,  1.3661e-02,  9.4297e-02,  2.6642e-02],\n",
      "        [-2.6675e-01,  2.3219e-02,  2.2713e-01,  5.5278e-02,  2.5376e-02,\n",
      "          2.6570e-02, -2.5968e-02,  1.3293e-01,  2.1128e-02,  7.7314e-02,\n",
      "         -1.1264e-02, -2.6101e-03,  6.5712e-02, -2.3566e-02, -1.6539e-02,\n",
      "          9.7654e-02, -3.7248e-02,  9.5313e-02,  1.1844e-02,  1.2461e-01,\n",
      "          1.1188e-01, -3.3494e-02,  1.7910e-02,  1.1145e-01,  1.4145e-01],\n",
      "        [-2.0104e-01,  3.9331e-02,  1.6686e-01,  2.1208e-03, -9.7579e-03,\n",
      "          3.3279e-01,  1.2800e-01, -2.6143e-01,  3.1751e-02, -1.5929e-03,\n",
      "         -1.2516e-01, -3.7514e-02, -3.5691e-02, -4.1283e-03,  2.7981e-01,\n",
      "         -7.9928e-02,  6.6978e-02, -9.3936e-02,  7.3856e-02, -1.5287e-01,\n",
      "          1.7606e-01,  1.3410e-02,  8.5862e-02, -4.6564e-02,  1.6242e-01],\n",
      "        [-6.8482e-02, -4.5301e-02,  2.7558e-02, -3.9202e-02,  6.4171e-02,\n",
      "         -4.7520e-02, -8.2360e-02, -5.5919e-03,  1.8676e-02,  5.4259e-02,\n",
      "          9.6333e-02,  3.2363e-02,  1.0819e-02,  2.8999e-02,  9.3198e-02,\n",
      "         -3.1425e-02,  1.5465e-02,  9.2779e-02,  8.1858e-02, -4.5335e-02,\n",
      "         -8.6383e-02, -2.4954e-02,  8.6906e-02, -2.4840e-02, -6.7834e-02],\n",
      "        [-1.8068e-01,  1.2548e-01,  1.7714e-01, -1.1348e-01, -3.4084e-02,\n",
      "          6.8181e-02,  9.1823e-03,  9.1278e-02,  9.6630e-02, -8.3058e-02,\n",
      "          6.7634e-02, -1.6386e-02,  1.3919e-01,  1.0423e-02, -9.0902e-02,\n",
      "         -2.4525e-02, -3.5358e-02,  1.9686e-02,  2.1093e-03, -2.9206e-02,\n",
      "          1.6589e-01, -3.1404e-02, -3.2689e-03, -3.4628e-02,  4.5118e-02],\n",
      "        [-9.9483e-02, -3.5879e-02, -1.7794e-02, -1.8923e-02, -1.9023e-02,\n",
      "          4.0214e-02, -5.6501e-02,  3.8443e-02,  8.3853e-02, -7.9927e-02,\n",
      "          5.9584e-02,  7.1780e-02, -4.5890e-02, -7.0191e-02,  1.8279e-02,\n",
      "         -2.2641e-02, -8.4233e-02,  4.5668e-02,  4.5422e-02, -3.1258e-03,\n",
      "          5.9488e-03,  6.1369e-02,  6.3617e-03, -2.3595e-02,  1.4503e-03],\n",
      "        [-2.8541e-01,  1.6884e-01,  2.1219e-01, -4.1568e-02,  7.0498e-02,\n",
      "          1.8254e-01,  2.5075e-03,  8.4680e-02, -3.8479e-02, -4.8444e-02,\n",
      "          6.4158e-02, -6.8019e-02,  3.4107e-02, -8.2441e-02, -1.7282e-01,\n",
      "         -9.3548e-02, -7.8400e-03, -4.4115e-04,  5.0432e-02,  1.2824e-01,\n",
      "          1.7856e-01, -5.1289e-02, -6.6055e-02,  5.3728e-02,  1.8050e-01],\n",
      "        [-3.6994e-02,  7.5801e-02, -3.0249e-03,  4.0162e-02, -6.8863e-02,\n",
      "         -9.3897e-02,  2.2860e-03, -5.8869e-02, -9.6110e-02,  5.8982e-02,\n",
      "          5.1473e-02, -4.6250e-02,  4.7982e-02, -7.3799e-02,  4.8247e-02,\n",
      "          3.8849e-02, -8.0056e-02, -6.3270e-02,  2.8875e-02,  1.1339e-01,\n",
      "          5.6739e-02, -4.8509e-02, -8.6415e-02, -9.0223e-02,  9.8283e-02],\n",
      "        [-4.9851e-02,  4.5282e-04, -6.3029e-02, -8.6966e-02,  7.4817e-02,\n",
      "          4.4455e-02,  9.9550e-02,  2.7335e-02,  3.2303e-02,  5.4283e-02,\n",
      "         -4.7407e-02,  3.9955e-02, -7.9927e-02, -5.7541e-02,  3.0504e-03,\n",
      "         -1.5411e-02,  7.4795e-02, -7.5065e-02,  4.4044e-02, -2.6505e-02,\n",
      "         -2.7130e-02, -7.0500e-02,  4.0650e-02,  3.5287e-02, -1.7736e-02],\n",
      "        [-3.6647e-01,  3.4567e-01,  4.6672e-01, -2.9492e-02,  1.3278e-02,\n",
      "          5.0744e-01,  1.4959e-01, -2.0496e-01, -7.6495e-02, -6.1188e-02,\n",
      "         -1.7947e-01,  7.4439e-02, -2.8512e-01,  4.0946e-02,  3.6230e-01,\n",
      "         -6.9117e-02, -1.1313e-02,  8.2462e-02, -3.0025e-02, -2.1434e-01,\n",
      "          4.7596e-01, -5.6355e-02,  9.1410e-02,  5.1613e-02,  4.3796e-01],\n",
      "        [-4.1260e-02,  2.1743e-02, -8.4144e-02, -2.5591e-02, -6.7897e-02,\n",
      "         -1.4265e-03, -8.6168e-02, -4.1616e-02,  7.1787e-02, -7.8142e-02,\n",
      "          9.7464e-03,  3.3741e-02, -4.1570e-02,  7.5485e-02, -5.8690e-02,\n",
      "         -9.2544e-02, -6.2946e-02,  4.7687e-02, -8.3815e-02,  2.7478e-02,\n",
      "          4.1480e-02,  3.6314e-02, -5.5286e-02, -9.3994e-02,  6.0749e-02],\n",
      "        [-4.2819e-01,  2.9403e-01,  2.6094e-01, -2.5673e-02,  2.9578e-02,\n",
      "          1.1013e-01, -7.2979e-02,  2.3388e-01,  8.5143e-02, -9.7595e-02,\n",
      "          1.7602e-01,  3.9784e-02,  8.6429e-02, -5.5731e-02, -2.5616e-01,\n",
      "          6.8063e-02, -3.5059e-03,  2.2518e-02, -9.7234e-02,  1.5222e-01,\n",
      "          2.4468e-01, -1.1743e-01,  2.1191e-02,  3.0298e-02,  2.5382e-01],\n",
      "        [-5.7973e-01,  3.5781e-01,  5.1661e-01, -1.0297e-01, -1.2098e-01,\n",
      "         -2.1778e-01, -1.2334e-01,  4.2648e-01, -7.9429e-02, -8.1284e-02,\n",
      "          2.0026e-01,  1.4740e-01,  2.2797e-01,  3.1281e-02, -3.3916e-01,\n",
      "          2.5621e-02, -3.0095e-02,  7.5259e-02,  2.5883e-02,  2.2619e-01,\n",
      "          9.7644e-02, -8.7209e-02, -6.9081e-02,  5.6045e-02,  2.2221e-01],\n",
      "        [-2.2256e-01,  3.9479e-02,  2.2052e-01, -7.0144e-03, -9.4547e-02,\n",
      "          1.3472e-01,  7.4200e-02,  1.9691e-01,  5.9738e-02, -2.5375e-02,\n",
      "          5.9203e-02,  4.7908e-02,  1.2390e-01,  7.3924e-02, -1.2674e-01,\n",
      "          4.3228e-02, -2.4700e-02,  3.8824e-02, -1.9314e-02, -5.1752e-02,\n",
      "          7.6057e-02, -9.9869e-02,  7.9272e-03,  3.0495e-02,  1.9743e-01]],\n",
      "       requires_grad=True)\n",
      "--> Bias[2]:: Params[25] of Shape[torch.Size([25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([ 0.6964, -0.5780, -0.0579, -0.0637,  0.4167,  0.0239, -0.3459, -0.0715,\n",
      "         0.1221,  0.5472,  0.1845,  0.0034,  0.2289,  0.7747, -0.0048,  0.3009,\n",
      "        -0.0422,  0.3295,  0.0059, -0.0979,  1.3931, -0.0322,  0.4893,  0.5496,\n",
      "         0.2717], requires_grad=True)\n",
      "--> Weights[3]:: Params[25] of Shape[torch.Size([1, 25])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([[ 1.0595,  0.7877,  0.0201,  0.1524,  0.5836, -0.1288,  0.4589,  0.0706,\n",
      "          0.5495,  1.0891,  0.3852,  0.0492,  0.3884, -0.9221,  0.0456,  0.4087,\n",
      "         -0.0993,  0.5117,  0.0963, -0.0555, -1.6879,  0.0946,  0.8343,  1.2723,\n",
      "          0.4442]], requires_grad=True)\n",
      "--> Bias[3]:: Params[1] of Shape[torch.Size([1])]\n",
      " ~--> [PARAMETER TENSOR]: tensor([-0.4632], requires_grad=True)\n",
      "--------------------------\n",
      "PARAMS:\t 1,376\n",
      "--------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.info(show_vals=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
