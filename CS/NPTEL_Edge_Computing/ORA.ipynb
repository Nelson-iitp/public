{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Resource Allocation in Public and Private Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define System Model (MDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System: # represents the MDP\n",
    "    \n",
    "    def __init__(self, E, Pefc) -> None:\n",
    "        self.E = E # total VMs at edge\n",
    "        self.H = [] # Record List\n",
    "        self.Pe, self.Pf, self.Pc = Pefc # cost parameters\n",
    "        self.verbose=False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.et = self.E # currently avaialble VMs\n",
    "        self.H.clear() # clear record list\n",
    "        self.t = 0   # time step\n",
    "        return self\n",
    "\n",
    "    def step(self, d, l, a): \n",
    "        # represents handling one user demand\n",
    "        self.t+=1\n",
    "        verb = self.verbose\n",
    "        # d = no of vms requesterd, \n",
    "        # l = duration requested\n",
    "        # a = action (ratio of vms allocated from cloud)\n",
    "        if verb: \n",
    "            print(f'{self.t=}')\n",
    "            print(f'\\tDemand, {d, l}')\n",
    "            print(f'\\tAction, {a}')\n",
    "        \n",
    "        c = int(d*a) # vms allocated from cloud\n",
    "        e = d - c    # vms allocated from edge\n",
    "        r = self.et - e # remaining VMs after allocation\n",
    "        # check if enough vms available?\n",
    "        if r<0:\n",
    "            e = self.et # allocated all from edge\n",
    "            c += (-r) # take remaining from cloud\n",
    "        self.et -= e\n",
    "        if verb: \n",
    "            print(f'\\tAllocated from Cloud, {c=}')\n",
    "            print(f'\\tAllocated from Edge, {e=}')\n",
    "            print(f'\\tRemaining, {self.et=}')\n",
    "        # generate allocation record \n",
    "        if e > 0:\n",
    "            self.H.append([e, l]) #<-- appending a list to a list\n",
    "            if verb:\n",
    "                print(f'\\t\\tAllocation Record, {[e,l]}')\n",
    "                print(f'\\t\\tAllocation Record List {self.H=}')    \n",
    "\n",
    "        # cost at edge node\n",
    "        Ce = (self.E-self.et)*self.Pf + (self.et)*self.Pe\n",
    "\n",
    "        # cost at private cloud\n",
    "        Cpri = c*self.Pc + Ce\n",
    "        if verb:\n",
    "            print(f'\\tCost At Edge Node, {Ce=}')\n",
    "            print(f'\\tCost At Private Cloud, {Cpri=}')\n",
    "\n",
    "        #<------------------------------- round \n",
    "\n",
    "        # update allocation record\n",
    "        for el in self.H: el[-1]-=1\n",
    "        # release exsisting VMs\n",
    "        # remove completed records\n",
    "        i=0\n",
    "        n=0 # no of busy vms (waiting to be released)\n",
    "        while i < len(self.H):\n",
    "            if self.H[i][-1]==0: \n",
    "                n+=self.H[i][0] # reclaim\n",
    "                del self.H[i]   # remove\n",
    "            else: i+=1 # skip\n",
    "        \n",
    "        if verb:\n",
    "            print(f'\\tUpdated Allocation Record List {self.H=}')  \n",
    "            print(f'\\tVMs waiting to be released {n=}')\n",
    "\n",
    "        self.et += n # update available\n",
    "        if verb: print(f'\\tVMs available at next time slot {self.et=}')\n",
    "        return Cpri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Environment: # Encapsulates an MDP for agent interaction\n",
    "    def __init__(self, E, D, L, T, Pefc, seed=None ) -> None:\n",
    "        self.E, self.D, self.L, self.T = E, D, L, T\n",
    "\n",
    "        self.nS = self.T*self.D*self.L*(self.E+1)\n",
    "        self.nD = self.D*self.L\n",
    "        self.DL = np.array([ [(d,l) for l in range(1, L+1)] for d in range(1, D+1)])\n",
    "        self.DL_ = self.DL.reshape(self.DL.shape[0]*self.DL.shape[1], self.DL.shape[2])\n",
    "        self.dls = np.arange(len(self.DL_))\n",
    "        self.S = np.zeros(4, dtype=int) #(etdl)\n",
    "        self.A=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        self.nA = len(self.A)\n",
    "        self.sim = System(E, Pefc)\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.sim.reset()\n",
    "        self.S[0] = self.sim.et\n",
    "        self.S[1] = self.sim.t\n",
    "        self.S[2:] = self.DL_[self.rng.choice(self.dls)]\n",
    "        return self.S\n",
    "\n",
    "    def step(self, action):\n",
    "        #print(f'{self.S=}, {self.A=}::{action=}')\n",
    "        cost = self.sim.step(d=self.S[2], l=self.S[3], a=self.A[action])\n",
    "        done = not(self.sim.t < self.T)\n",
    "        self.S[0] = self.sim.et\n",
    "        self.S[1] = self.sim.t\n",
    "        self.S[2:] = 0 if done else self.DL_[self.rng.choice(self.dls)]\n",
    "        return self.S, float(-cost), done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(E=19, D=4, L=3, T=3, Pefc=(0.03, 0.20, 3.00), seed=13)\n",
    "env.sim.verbose=False\n",
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learning(\n",
    "        mdp,    # formulated mdp \n",
    "        πe,     # behaviour policy\n",
    "        α,      # learning rate\n",
    "        γ,      # discount factor\n",
    "        N,      # number of learning rounds\n",
    "    ):\n",
    "    Q = {}  # initialize Q-Table\n",
    "    s = tuple(mdp.reset()) # reset the mdp and obtain initial state\n",
    "    if s not in Q: \n",
    "        # add obtained state to Q-Table and initalize values as zeros\n",
    "        Q[s] = [0.0 for _ in range(mdp.nA)] \n",
    "\n",
    "    for n in range(N): # learning loop\n",
    "        a = πe(s) # select action using behaviour policy\n",
    "        s_, r,  done = mdp.step(a) # obtain reward(r) and next state(s_)\n",
    "        s_ = tuple(s_)\n",
    "        if s_ not in Q: \n",
    "            # add obtained next state to Q-Table and initalize values as zeros\n",
    "            Q[s_] = [0.0 for _ in range(mdp.nA)]\n",
    "        \n",
    "        Q[s][a] = (1-α) * Q[s][a] + (α) * (r + γ * max(Q[s_])) # update Q-values\n",
    "        if done: # final state reached?\n",
    "            s = tuple(mdp.reset()) # reset the mdp\n",
    "            if s not in Q: Q[s] = [0.0 for _ in range(mdp.nA)]\n",
    "        else:\n",
    "            s = s_  # continue to next time step\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q_Learning(env, lambda s: int(np.random.randint(0, env.nA)), 0.5, 1.0, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnt Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnt policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Policy(mdp, Q):\n",
    "    s = tuple(mdp.reset())\n",
    "    done = False\n",
    "    ret = 0.0\n",
    "    while not done:\n",
    "        q = Q[s]\n",
    "        a = np.argmax(q)\n",
    "        s_, r,  done = mdp.step(a)\n",
    "        s_ = tuple(s_)\n",
    "        ret+=r\n",
    "        print(f'{s=}, {q=}, {a=}, {r=}, {s_=}, {done=}, {ret=}')\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Learnt policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = Q_Policy(env, q)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Simulation\n",
    "\n",
    "(for worked out examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = System(E=80, Pefc=(0.03, 0.20, 3.00)).reset()\n",
    "print(S.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = S.step(d=30, l=2, a=0.4)\n",
    "print(S.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = S.step(d=10, l=1, a=0.7)\n",
    "print(S.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = S.step(d=20, l=2, a=0.8)\n",
    "print(S.__dict__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
