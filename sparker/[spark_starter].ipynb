{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d2f07a-b590-429c-8a82-2b05aa41405c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"font-size:36px\">Spark Starter</span><br> <span style=\"font-size:26px\"> *Working with Apache Spark using pyspark*</span><br><hr>\n",
    "<span style=\"font-size:18px;color:#A43562\">\n",
    "Important Links:\n",
    "* [Apache Spark Official Documentation](https://spark.apache.org/docs/latest/index.html)\n",
    "* [Spark Components - Cluster Overview](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "* [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "* [SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "* [pyspark ~ Python API](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "    * [DataFrame Creation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html#DataFrame-Creation)\n",
    "    * [Working with SQL](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html#Working-with-SQL)\n",
    "    * [SQL Core Classes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
    "* [Official Example Code](https://spark.apache.org/examples.html)\n",
    "</span><hr>\n",
    "\n",
    "<span style=\"font-size:14px;color:#4500FF\">\n",
    "Install spark on your server\n",
    "\n",
    "<code> sudo apt install spark </code>\n",
    "\n",
    "*spark can be accessed from command line using the spark-shell command (here the shell acts as a driver program)*\n",
    "\n",
    "<code> spark-shell -c spark.driver.bindAddress=127.0.0.1 </code>\n",
    "\n",
    "\n",
    "Install the pyspark package using pip\n",
    "\n",
    "<code> pip install pyspark </code>\n",
    "</span>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ab886-b6fe-4bd8-9fdc-3c46791882c8",
   "metadata": {},
   "source": [
    "# ~ pyspark\n",
    "\n",
    "<span style=\"font-size:18px;font-family:Corbel;color:#003300\">\n",
    "\n",
    "*make sure both spark & pyspark have installed correctly on the system*\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a241d170-8637-4fcd-a0ee-7548cbf74f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb948b24-97ab-40ed-8614-18fc76568540",
   "metadata": {},
   "source": [
    "# ~ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107329b9-65e8-4d41-8865-559b430ad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark, random, os, shutil, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ea60c-565b-4f26-a92f-b92849e53f35",
   "metadata": {},
   "source": [
    "# ~ Spark *session* & *context*\n",
    "\n",
    "<span style=\"font-size:18px;font-family:Corbel;color:#003300\">\n",
    "\n",
    "Spark session is a **unified entry point** of a spark application from Spark 2.0. It provides a way to interact with various spark's functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session.\n",
    "\n",
    "* SparkContext: Refer [Official Documentation](https://spark.apache.org/docs/latest/cluster-overview.html)\n",
    "* [Glossary](https://spark.apache.org/docs/latest/cluster-overview.html#glossary) of Cluster Components\n",
    "\n",
    "NOTE: *Multiple sessions may share the same context, it provides isolation for users working in the same cluster.*\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9918a10-6b3b-4a5c-9cda-0bfdff6ccd85",
   "metadata": {},
   "source": [
    "## Create a *Session*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67789b-4e03-4fc5-8dd6-a51428c5ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now() # record the start time\n",
    "\n",
    "# earlier ... \n",
    "\"\"\"\n",
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext(conf=SparkConf().setAppName('CS555').setMaster(\"local\"))\n",
    "\"\"\"\n",
    "\n",
    "# using a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('CS555').getOrCreate()  # to stop session use spark.stop()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4f29e4-67d7-4a1f-8a03-c57891eef9fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get *context* object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bf700e-036a-4010-8763-36f23e3ae9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print('sparkContext:',sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63a5a2-758c-432d-a081-6b6067c1f9c1",
   "metadata": {},
   "source": [
    "# ~ RDD - Resilient Distributed Datasets\n",
    "\n",
    "<span style=\"font-size:18px;font-family:Corbel;color:#003300\">\n",
    "\n",
    "Refer the official [RDD Programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "* **[RDDs](https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds)** are **immutable** collection of datasets that work in parallel\n",
    " \n",
    "* **[Transformations](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)** are lazy operations on RDDs - stores operations (in a DAG) rather than actual transformation\n",
    "* **[Actions](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)** on RDDs produce results (using the operations stored in DAG)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c79b8-d26c-40b3-b536-19b17c7470ac",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "<span style=\"font-size:20px;color:#A43562\">Using *sc.parallelize()*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039c015-5d83-49b3-90c3-66204f3be376",
   "metadata": {},
   "source": [
    "### Example [A]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4050b3-8037-48ac-abfa-31f47af65a05",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ create RDDs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ab2ce1-e6a3-459e-9ba0-d290327ab9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RDD_array1 = sc.parallelize(np.random.randint(0,10,size=10),numSlices=5)\n",
    "RDD_array2 = sc.parallelize(np.random.randint(0,10,size=10),numSlices=3)\n",
    "print(type(RDD_array1),RDD_array1)\n",
    "print(type(RDD_array2),RDD_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe0daa-d3bb-47a3-9782-02dd6786a239",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ collect values</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d36fa-4a32-4da3-bd3f-9e0f3bb7a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = RDD_array1.collect() # 'collect' is an action\n",
    "a2 = RDD_array2.collect()\n",
    "print(type(a1), len(a1),  a1)\n",
    "print(type(a2), len(a2), a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462284a-fc5f-4819-9b27-757e59e5eed1",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ collect glom values ( .. see [glom function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.glom.html))</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304f4de-9e61-48a6-affe-77aa0a9bf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = RDD_array1.glom().collect()\n",
    "g2 = RDD_array2.glom().collect()\n",
    "print(type(g1), len(g1),  g1)\n",
    "print(type(g2), len(g2), g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885a4ac-20dc-4cb2-8de7-72eb6ed1bca7",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ transformation **map(x)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7216ecb-befb-4cdc-93de-2b61f2f8c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_array3 = RDD_array2.map(lambda x: x*2) # 'map' is a transformation\n",
    "print(type(RDD_array3),RDD_array3)\n",
    "a3 = RDD_array3.collect()\n",
    "print(type(a3),  a3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90029c25-c3d0-4b58-8cb0-a8ae9a48fa2c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ transformation **union(x)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d48947-f53f-4ff7-8c77-a67ef9a1b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_array4 = RDD_array1.union(RDD_array2) # 'union' is a transformation\n",
    "print(type(RDD_array4),RDD_array4)\n",
    "a4 = RDD_array4.collect()\n",
    "print(type(a4),  a4, 'Sum:',sum(a4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbeb25c-c65e-43ff-8279-e149502ace98",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ action **reduce(x,y)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f54c2-8311-4602-93b0-57c8c3a94c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "a5 = RDD_array4.reduce(lambda x,y: x+y)  # 'reduce' is an action\n",
    "print(type(a5), 'Sum:', a5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462297b2-bf21-48c6-923a-2df5f7599748",
   "metadata": {},
   "source": [
    "### Example [B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2907450-e16b-4828-9733-52771ff8b348",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ create RDDs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d6986-b97d-4702-b510-7848b281329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_Names = sc.parallelize(['Alice', 'Bob', 'Charlie', 'David', 'Eli', \n",
    "                            'Felix', 'George', 'Hank', 'John', 'Isabel'])\n",
    "print(type(RDD_Names),RDD_Names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac077d-3b50-48d1-9a70-b4261ebc0990",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ transformation **filter(x)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ca3b7-ee9f-4c64-9590-7fe44257d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_Info = RDD_Names.filter(lambda x: len(x)<=4)\n",
    "print(type(RDD_Info),RDD_Info)\n",
    "info = RDD_Info.collect()\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d4a93-4ee6-4e8f-b26d-b48d5a1b14a0",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ map(x) and reduce(x,y)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e8423-e007-48ef-8624-7a155b68568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_counts = RDD_Info.map(lambda x: len(x))\n",
    "counts = RDD_counts.collect()\n",
    "print('counts',counts)\n",
    "summation = RDD_counts.reduce(lambda x,y: x+y)\n",
    "print(summation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ac965-2e6e-44ac-b6a6-212564228ed1",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ One liner</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fcf0a-0b4c-4921-b196-26a6a56b0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_liner = RDD_Names.filter(lambda x: len(x)<=4).map(lambda x: len(x)).reduce(lambda x,y: x+y)\n",
    "print(one_liner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5b417-2df6-47ef-9fcc-c91297a410a4",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "\n",
    "<span style=\"font-size:20px;color:#A43562\">from *external files/objects*</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ecaf7-fd17-4070-af2b-a79584947f16",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Obtain data from yahoo finance ( .. See [Ticker Symbols](https://finance.yahoo.com/lookup/))</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b9d62-3c51-44ae-b1f0-9e47c41c37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader\n",
    "pdr=pandas_datareader.data.DataReader('googl', 'yahoo', \n",
    "                start=datetime.datetime(2010, 8, 1), \n",
    "                end=datetime.datetime(2016, 11, 30)).to_csv('stocks.csv', sep=',') \n",
    "print('stocks.csv' in os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac0932-f6ad-4e45-ba07-bfd8850d5485",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Create RDD from CSV</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cdc620-9db6-427c-8e13-fb78a8a401d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_F = sc.textFile(\"stocks.csv\")\n",
    "\n",
    "cf = RDD_F.count() # 'count' is an action\n",
    "print('Count:',cf)\n",
    "\n",
    "data = RDD_F.collect() # 'collect' is an action\n",
    "print(type(data), len(data), '\\n[-]', data[-1])\n",
    "\n",
    "first = RDD_F.first() # 'first' is an action\n",
    "print('[H]',first) #<---- shall be used for schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1580f2ab-661e-414a-8b5d-dfb78029f4f2",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ function for parsing</span> ( .. see [Data-Types](https://spark.apache.org/docs/latest/sql-ref-datatypes.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c318a-455a-461d-b97c-1e41ea6feb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(xs):\n",
    "    \"\"\" parses each row of data and converts to appropiate data type\"\"\"\n",
    "    global first\n",
    "    if xs!=first:\n",
    "        x = xs.split(\",\")\n",
    "        return [datetime.datetime.strptime(x[0],\"%Y-%m-%d\"), \n",
    "                float(x[1]), float(x[2]), float(x[3]), float(x[4]), float(x[5]), float(x[6])]\n",
    "    else:\n",
    "        return None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53185b-e7ee-41d8-9de5-0af37f61a43c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ create a **DataFrame**</span> ( .. see [sql.DataFrame](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cda365-abd0-4d12-8279-6753171ab352",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = RDD_F.map(parse).filter(lambda x: x!=None).toDF(first.split(','))\n",
    "print(type(dframe), dframe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1376cd-ff82-4a81-a7e0-70a9f0155372",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Schema</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224f31e7-87ce-4741-9a1b-1f4c71260bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2c2be-ff4d-40be-b60c-8d5dccdd2567",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b0038-f5c2-4daa-a3e0-cd846e53585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.show() # or use .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0953ede-a800-4ebe-9ec0-7e4a2efad3fb",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Description</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbef61b-efc8-4588-88c5-68401670e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412281d-de8a-4b18-a323-452cfd322932",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Plot Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0248b153-422a-4a5b-91e0-65e854f92e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dframe.select(['Date', 'Close']).sort('Date')\n",
    "dx = d.select(['Date']).collect()\n",
    "dy = d.select(['Close']).collect()\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.plot(dx, dy, linewidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3029f04c-36a6-4bae-8296-598f03eb6ad7",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Plot Recent Data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d54091-76b7-4cc5-86c3-f2af2c52ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dframe.filter(\n",
    "            dframe.Date > datetime.datetime.strptime('2014-1-1', \"%Y-%m-%d\")\n",
    "                    ).select(['Date', 'Close']).sort('Date')\n",
    "dx = df.select(['Date']).collect()\n",
    "dy = df.select(['Close']).collect()\n",
    "print('Samples:',len(dx))\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.plot(dx, dy, linewidth=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9fd54-ccc9-46a2-83a7-f319ce83b83c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ SQL Queries ( .. see [SQL-Guide](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html))</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba4e5c-b772-4656-b7c1-9a3223f51129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"stock\")\n",
    "# Write and execute an SQL statement\n",
    "sqlDF = spark.sql(\"SELECT * FROM stock\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8496f5f-2e51-4726-b2ce-5b6240465db4",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ SQL Queries using SQLContext( .. see [SQL-Guide](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html))</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265c3ec-7f8a-4528-81fc-76296aeb504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a dictionary.\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext.sql(\"SELECT * FROM stock\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d36d2-0fb2-4f10-ad79-87374bf65c8d",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Check Catalog</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcf38b-ee70-4e0c-9917-1fcfcf845782",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = spark.catalog.listDatabases()\n",
    "tb = spark.catalog.listTables()\n",
    "print('Databases:', db)\n",
    "print('Tables:',tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdff1e4-e181-4a16-823d-4d6f3202216d",
   "metadata": {},
   "source": [
    "# ~ Price Forecasting\n",
    "\n",
    "*Not using spark.mllib*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890705d7-389f-4037-b2c7-38c167ffd1a9",
   "metadata": {},
   "source": [
    "## Using LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e78225f-b7fb-4fd8-a657-91388dd0d3bc",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Prepare</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef7bcc-690e-4237-b9d8-45b5ffa21f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, RepeatVector, TimeDistributed, Flatten, LSTM\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# convert to Pandas Data frame \n",
    "dfx = df.toPandas()\n",
    "N = len(dfx)\n",
    "tr = int(N*0.80) #<- train-test split ration\n",
    "timesteps = 20\n",
    "print('Number of rows and columns:', dfx.shape)\n",
    "print('Split-Dataset:',N,'Train:',tr,'Test:',N-tr)\n",
    "print('Time-Steps:',timesteps)\n",
    "\n",
    "# prepare data\n",
    "training_set = dfx.iloc[:tr, 1:2].values\n",
    "test_set = dfx.iloc[tr:, 1:2].values\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.plot(training_set)\n",
    "\n",
    "# Feature Scaling\n",
    "scal = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = scal.fit_transform(training_set)\n",
    "\n",
    "# Creating a data structure with time-steps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(timesteps, tr):\n",
    "    X_train.append(training_set_scaled[i-timesteps:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "print('Final shape:',X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22786a5-78f2-4ee3-b4a8-1aae876711fc",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Build LSTM</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0df6ba-b68f-4b04-a902-822a4446f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1))\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "print(model.summary())\n",
    "cb_ES_loss = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',  #monitor: Quantity to be monitored.\n",
    "        min_delta=0.001,  #min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "        patience=25,  #patience: Number of epochs with no improvement after which training will be stopped.\n",
    "        verbose=1,  #verbose: verbosity mode.\n",
    "        mode='auto',  #mode: One of {\"auto\", \"min\", \"max\"}. \n",
    "                      #In min mode, training will stop when the quantity monitored has stopped decreasing; \n",
    "                      #in \"max\" mode it will stop when the quantity monitored has stopped increasing; \n",
    "                      #in \"auto\" mode, the direction is automatically inferred from the name of the monitored quantity.\n",
    "        baseline=None,  #baseline: Baseline value for the monitored quantity. Training will stop if the model doesn't show improvement over the baseline.\n",
    "        restore_best_weights=True #restore_best_weights: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\n",
    "        )\n",
    "cb_list = [cb_ES_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15273f9-37a2-47fb-8afa-aae60fd28e24",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebfde1-2947-444b-b733-55354428413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs = 100, batch_size = 32, verbose=1, callbacks=cb_list)\n",
    "plt.plot(hist.history['loss'], color='tab:red', linewidth=0.6)\n",
    "print(hist.history['loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69af76d9-431f-4571-b988-cf68b4b80d2a",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#DA4512\">~ Plot Results</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073b56e-b5ca-419d-a3a6-93e54bc74005",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dfx.iloc[:tr, 1:2]\n",
    "dataset_test = dfx.iloc[tr:, 1:2]\n",
    "dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - timesteps:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = scal.transform(inputs)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(timesteps, N-tr):\n",
    "    X_test.append(inputs[i-timesteps:i, 0])\n",
    "    y_test.append(inputs[i, 0])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "print(X_test.shape)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_test.shape)\n",
    "# (459, 60, 1)\n",
    "model.evaluate(X_test, y_test,verbose=1)\n",
    "predicted_stock_price = model.predict(X_test)[:,0]\n",
    "predicted_stock_price1 = model.predict(X_train)[:,0]\n",
    "#predicted_stock_price = scal.inverse_transform(predicted_stock_price)\n",
    "y_g = np.hstack((y_train,y_test))\n",
    "y_p = np.hstack((predicted_stock_price1,predicted_stock_price))\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(y_g, color = 'tab:green', label = 'true', linewidth=0.8)\n",
    "plt.plot(y_p, color = 'tab:red', label = 'cast', linewidth=0.8)\n",
    "plt.vlines(tr,0,np.max(y_g), color='black', linestyles='dashed')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "MSE = np.mean((y_g - y_p)**2)\n",
    "print('MSE:', MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3862209e-0904-4d7a-a678-f08ecf39312c",
   "metadata": {},
   "source": [
    "## using PROPHET Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b669ff2d-6b54-452e-a4eb-80cb9c9c5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "sel_pd=df.toPandas()\n",
    "data = sel_pd[[\"Date\",\"Close\"]]\n",
    "data = data.rename(columns = {\"Date\":\"ds\",'Close':\"y\"})\n",
    "m = Prophet(daily_seasonality = True)\n",
    "m.fit(data) \n",
    "future = m.make_future_dataframe(periods=365*2) \n",
    "prediction = m.predict(future)\n",
    "m.plot(prediction)\n",
    "plt.ylabel(\"Close\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f6303-7baf-4cdf-a07e-3cd4abfe6e40",
   "metadata": {},
   "source": [
    "# ~ Structured API - DataFrame & Datasets\n",
    "\n",
    "<span style=\"font-size:18px;font-family:Corbel;color:#003300\">\n",
    "\n",
    "Why structed API? - DataFrames are faster than RDDs. Refer this [seminar](https://www.youtube.com/watch?v=Ofk7G3GD9jk)\n",
    "\n",
    "Creating DataFrames: Refer [Official Documentation](https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html#DataFrame-Creation)\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ab9b5-d191-44fc-a431-0ac3e4cbde30",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851b493-015f-4f48-abe0-1f676a90a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.option('header',\n",
    "                            'true' #<---- the csv has a header\n",
    "                           ).csv(\"stocks.csv\", \n",
    "                                inferSchema=True) # add infer schema to load in proper data type\n",
    "\n",
    "print(type(dataset), dataset.count())\n",
    "dataset.printSchema()\n",
    "#dataset.show() # or use .describe()\n",
    "print('\\nSELECT')\n",
    "dataset.select(['Date','Volume']).show()\n",
    "\n",
    "print('\\nFILTER & SELECT')\n",
    "dataset.filter(dataset.Volume>=4000000).select([\"Date\",\"Volume\",\"Adj Close\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727719aa-fae6-4978-8182-7d1d2cdf69d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<span style=\"font-size:22px;color:#DA4512\">See more @ \n",
    "* [Spark Core Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html)\n",
    "* [Spark SQL Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2364842-d022-43fe-bf04-e4b4b22bac17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DONE!\n",
    "\n",
    "<span style=\"font-size:16px;color:#DA4512\">~ close spark session</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02481b-b1e6-49e8-9410-932c566c4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "\n",
    "stop_time = datetime.datetime.now()\n",
    "elapsed_time = stop_time-start_time\n",
    "print('Session Duration:', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13c80b-7ce9-43ab-aa41-78b8a760fd3b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:16px;color:#F0050F\">~~ The End ~~</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
